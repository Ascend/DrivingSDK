diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..e59373f
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,18 @@
+.nfs*
+*.ipynb
+*.pyc
+.dumbo.json
+.DS_Store
+.*.swp
+*.pth
+**/__pycache__/**
+.ipynb_checkpoints/
+datasets/data/
+data/
+experiment-*
+*.tmp
+*.pkl
+**/.mypy_cache/*
+.mypy_cache/*
+not_tracked_dir/
+.vscode
\ No newline at end of file
diff --git a/LICENSE b/LICENSE
old mode 100644
new mode 100755
diff --git a/README.md b/README.md
index c9db563..f710d50 100644
--- a/README.md
+++ b/README.md
@@ -111,8 +111,8 @@ code_root/
         ├── train2017/
         ├── val2017/
         └── annotations/
-        	├── instances_train2017.json
-        	└── instances_val2017.json
+            ├── instances_train2017.json
+            └── instances_val2017.json
 ```

 ### Training
diff --git a/benchmark.py b/benchmark.py
old mode 100644
new mode 100755
diff --git a/configs/r50_deformable_detr.sh b/configs/r50_deformable_detr.sh
deleted file mode 100755
index a42953f..0000000
--- a/configs/r50_deformable_detr.sh
+++ /dev/null
@@ -1,10 +0,0 @@
-#!/usr/bin/env bash
-
-set -x
-
-EXP_DIR=exps/r50_deformable_detr
-PY_ARGS=${@:1}
-
-python -u main.py \
-    --output_dir ${EXP_DIR} \
-    ${PY_ARGS}
diff --git a/configs/r50_deformable_detr_plus_iterative_bbox_refinement.sh b/configs/r50_deformable_detr_plus_iterative_bbox_refinement.sh
deleted file mode 100755
index 8ea2000..0000000
--- a/configs/r50_deformable_detr_plus_iterative_bbox_refinement.sh
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/usr/bin/env bash
-
-set -x
-
-EXP_DIR=exps/r50_deformable_detr_plus_iterative_bbox_refinement
-PY_ARGS=${@:1}
-
-python -u main.py \
-    --output_dir ${EXP_DIR} \
-    --with_box_refine \
-    ${PY_ARGS}
diff --git a/configs/r50_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage.sh b/configs/r50_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage.sh
deleted file mode 100755
index 722c658..0000000
--- a/configs/r50_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage.sh
+++ /dev/null
@@ -1,12 +0,0 @@
-#!/usr/bin/env bash
-
-set -x
-
-EXP_DIR=exps/r50_deformable_detr_plus_iterative_bbox_refinement_plus_plus_two_stage
-PY_ARGS=${@:1}
-
-python -u main.py \
-    --output_dir ${EXP_DIR} \
-    --with_box_refine \
-    --two_stage \
-    ${PY_ARGS}
diff --git a/configs/r50_deformable_detr_single_scale.sh b/configs/r50_deformable_detr_single_scale.sh
deleted file mode 100755
index a24e547..0000000
--- a/configs/r50_deformable_detr_single_scale.sh
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/usr/bin/env bash
-
-set -x
-
-EXP_DIR=exps/r50_deformable_detr_single_scale
-PY_ARGS=${@:1}
-
-python -u main.py \
-    --num_feature_levels 1 \
-    --output_dir ${EXP_DIR} \
-    ${PY_ARGS}
diff --git a/configs/r50_deformable_detr_single_scale_dc5.sh b/configs/r50_deformable_detr_single_scale_dc5.sh
deleted file mode 100755
index 26d35d6..0000000
--- a/configs/r50_deformable_detr_single_scale_dc5.sh
+++ /dev/null
@@ -1,12 +0,0 @@
-#!/usr/bin/env bash
-
-set -x
-
-EXP_DIR=exps/r50_deformable_detr_single_scale_dc5
-PY_ARGS=${@:1}
-
-python -u main.py \
-    --num_feature_levels 1 \
-    --dilation \
-    --output_dir ${EXP_DIR} \
-    ${PY_ARGS}
diff --git a/datasets/__init__.py b/datasets/__init__.py
old mode 100644
new mode 100755
diff --git a/datasets/coco.py b/datasets/coco.py
old mode 100644
new mode 100755
index 1be8308..c23dfaf
--- a/datasets/coco.py
+++ b/datasets/coco.py
@@ -18,6 +18,7 @@ import torch
 import torch.utils.data
 from pycocotools import mask as coco_mask

+from mx_driving.dataset.utils.dynamic_transforms import BalancedRandomResize
 from .torchvision_datasets import CocoDetection as TvCocoDetection
 from util.misc import get_local_rank, get_local_size
 import datasets.transforms as T
@@ -135,19 +136,19 @@ def make_coco_transforms(image_set):
         return T.Compose([
             T.RandomHorizontalFlip(),
             T.RandomSelect(
-                T.RandomResize(scales, max_size=1333),
+                None,
                 T.Compose([
                     T.RandomResize([400, 500, 600]),
                     T.RandomSizeCrop(384, 600),
-                    T.RandomResize(scales, max_size=1333),
                 ])
             ),
+            BalancedRandomResize(scales, max_size=1333, seed=100),
             normalize,
         ])

     if image_set == 'val':
         return T.Compose([
-            T.RandomResize([800], max_size=1333),
+            T.RandomResize([640], max_size=1333),
             normalize,
         ])

diff --git a/datasets/coco_eval.py b/datasets/coco_eval.py
old mode 100644
new mode 100755
diff --git a/datasets/coco_panoptic.py b/datasets/coco_panoptic.py
old mode 100644
new mode 100755
diff --git a/datasets/data_prefetcher.py b/datasets/data_prefetcher.py
old mode 100644
new mode 100755
diff --git a/datasets/panoptic_eval.py b/datasets/panoptic_eval.py
old mode 100644
new mode 100755
diff --git a/datasets/samplers.py b/datasets/samplers.py
old mode 100644
new mode 100755
diff --git a/datasets/torchvision_datasets/__init__.py b/datasets/torchvision_datasets/__init__.py
old mode 100644
new mode 100755
diff --git a/datasets/torchvision_datasets/coco.py b/datasets/torchvision_datasets/coco.py
old mode 100644
new mode 100755
diff --git a/datasets/transforms.py b/datasets/transforms.py
old mode 100644
new mode 100755
index 8f4baeb..f99284b
--- a/datasets/transforms.py
+++ b/datasets/transforms.py
@@ -201,10 +201,20 @@ class RandomResize(object):
         assert isinstance(sizes, (list, tuple))
         self.sizes = sizes
         self.max_size = max_size
+        self.idx = 0
+        self.length = len(self.sizes)
+
+        seed = 100
+        torch.manual_seed(seed)
+        random.seed(seed)
+        self.size = self.sizes[random.randint(0, self.length - 1)]

     def __call__(self, img, target=None):
-        size = random.choice(self.sizes)
-        return resize(img, target, size, self.max_size)
+        if self.idx % 8 == 0:
+            self.size = self.sizes[random.randint(0, self.length - 1)]
+
+        self.idx += 1
+        return resize(img, target, self.size, self.max_size)


 class RandomPad(object):
@@ -229,6 +239,8 @@ class RandomSelect(object):

     def __call__(self, img, target):
         if random.random() < self.p:
+            if self.transforms1 is None:
+                return img, target
             return self.transforms1(img, target)
         return self.transforms2(img, target)

diff --git a/engine.py b/engine.py
old mode 100644
new mode 100755
index 1ae2ae9..db8641f
--- a/engine.py
+++ b/engine.py
@@ -16,6 +16,7 @@ import sys
 from typing import Iterable

 import torch
+import torch_npu
 import util.misc as utils
 from datasets.coco_eval import CocoEvaluator
 from datasets.panoptic_eval import PanopticEvaluator
@@ -37,17 +38,29 @@ def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,
     prefetcher = data_prefetcher(data_loader, device, prefetch=True)
     samples, targets = prefetcher.next()

-    # for samples, targets in metric_logger.log_every(data_loader, print_freq, header):
     for _ in metric_logger.log_every(range(len(data_loader)), print_freq, header):
+
+        num_boxes, handle, s = criterion.get_num_boxes(targets, device)
         outputs = model(samples)
-        loss_dict = criterion(outputs, targets)
+        loss_dict = criterion(outputs, targets, num_boxes, handle, s)
         weight_dict = criterion.weight_dict
         losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)

-        # reduce losses over all GPUs for logging purposes
-        loss_dict_reduced = utils.reduce_dict(loss_dict)
+        values, names, handle = utils.reduce_values(loss_dict)
+
+        optimizer.zero_grad()
+        losses.backward()
+
+        if max_norm > 0:
+            grad_total_norm = optimizer.clip_grad_norm_fused_(max_norm=max_norm, norm_type=2)
+        else:
+            grad_total_norm = utils.get_total_grad_norm(model.parameters(), max_norm)
+
+        optimizer.step()
+
+        loss_dict_reduced = utils.get_values(values, names, handle)
         loss_dict_reduced_unscaled = {f'{k}_unscaled': v
-                                      for k, v in loss_dict_reduced.items()}
+                                    for k, v in loss_dict_reduced.items()}
         loss_dict_reduced_scaled = {k: v * weight_dict[k]
                                     for k, v in loss_dict_reduced.items() if k in weight_dict}
         losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())
@@ -59,21 +72,13 @@ def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,
             print(loss_dict_reduced)
             sys.exit(1)

-        optimizer.zero_grad()
-        losses.backward()
-        if max_norm > 0:
-            grad_total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
-        else:
-            grad_total_norm = utils.get_total_grad_norm(model.parameters(), max_norm)
-        optimizer.step()
-
         metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)
         metric_logger.update(class_error=loss_dict_reduced['class_error'])
         metric_logger.update(lr=optimizer.param_groups[0]["lr"])
         metric_logger.update(grad_norm=grad_total_norm)

         samples, targets = prefetcher.next()
-    # gather the stats from all processes
+
     metric_logger.synchronize_between_processes()
     print("Averaged stats:", metric_logger)
     return {k: meter.global_avg for k, meter in metric_logger.meters.items()}
@@ -104,8 +109,9 @@ def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, out
         samples = samples.to(device)
         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

+        num_boxes, handle, s = criterion.get_num_boxes(targets, device)
         outputs = model(samples)
-        loss_dict = criterion(outputs, targets)
+        loss_dict = criterion(outputs, targets, num_boxes, handle, s)
         weight_dict = criterion.weight_dict

         # reduce losses over all GPUs for logging purposes
@@ -150,6 +156,7 @@ def evaluate(model, criterion, postprocessors, data_loader, base_ds, device, out
     if coco_evaluator is not None:
         coco_evaluator.accumulate()
         coco_evaluator.summarize()
+
     panoptic_res = None
     if panoptic_evaluator is not None:
         panoptic_res = panoptic_evaluator.summarize()
diff --git a/figs/convergence.png b/figs/convergence.png
old mode 100644
new mode 100755
diff --git a/figs/illustration.png b/figs/illustration.png
old mode 100644
new mode 100755
diff --git a/main.py b/main.py
old mode 100644
new mode 100755
index fc6ccfa..6d0186b
--- a/main.py
+++ b/main.py
@@ -18,6 +18,8 @@ from pathlib import Path
 import numpy as np
 import torch
 from torch.utils.data import DataLoader
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import datasets
 import util.misc as utils
 import datasets.samplers as samplers
@@ -25,6 +27,9 @@ from datasets import build_dataset, get_coco_api_from_dataset
 from engine import evaluate, train_one_epoch
 from models import build_model

+# 加上下面的代码防止走进不支持的图模式
+torch.npu.config.allow_internal_format = False
+torch.npu.set_compile_mode(jit_compile=False)

 def get_args_parser():
     parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)
@@ -33,7 +38,7 @@ def get_args_parser():
     parser.add_argument('--lr_backbone', default=2e-5, type=float)
     parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')
     parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)
-    parser.add_argument('--batch_size', default=2, type=int)
+    parser.add_argument('--batch_size', default=8, type=int)
     parser.add_argument('--weight_decay', default=1e-4, type=float)
     parser.add_argument('--epochs', default=50, type=int)
     parser.add_argument('--lr_drop', default=40, type=int)
@@ -84,7 +89,7 @@ def get_args_parser():
     # * Segmentation
     parser.add_argument('--masks', action='store_true',
                         help="Train segmentation head if the flag is provided")
-
+
     # Loss
     parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',
                         help="Disables auxiliary decoding losses (loss at each layer)")
@@ -107,11 +112,11 @@ def get_args_parser():

     # dataset parameters
     parser.add_argument('--dataset_file', default='coco')
-    parser.add_argument('--coco_path', default='./data/coco', type=str)
+    parser.add_argument('--coco_path', default='', type=str)
     parser.add_argument('--coco_panoptic_path', type=str)
     parser.add_argument('--remove_difficult', action='store_true')

-    parser.add_argument('--output_dir', default='',
+    parser.add_argument('--output_dir', default='./checkpoints',
                         help='path where to save, empty for no saving')
     parser.add_argument('--device', default='cuda',
                         help='device to use for training / testing')
@@ -201,12 +206,15 @@ def main(args):
             "lr": args.lr * args.lr_linear_proj_mult,
         }
     ]
+
+
     if args.sgd:
-        optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,
+        optimizer = torch_npu.optim.NpuFusedSGD(param_dicts, lr=args.lr, momentum=0.9,
                                     weight_decay=args.weight_decay)
     else:
-        optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,
+        optimizer = torch_npu.optim.NpuFusedAdamW(param_dicts, lr=args.lr,
                                       weight_decay=args.weight_decay)
+
     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)

     if args.distributed:
diff --git a/models/__init__.py b/models/__init__.py
old mode 100644
new mode 100755
diff --git a/models/backbone.py b/models/backbone.py
old mode 100644
new mode 100755
index 4bfe705..94f5e2d
--- a/models/backbone.py
+++ b/models/backbone.py
@@ -18,13 +18,12 @@ import torchvision
 from torch import nn
 from torchvision.models._utils import IntermediateLayerGetter
 from typing import Dict, List
+from mx_driving.fused import npu_max_pool2d

 from util.misc import NestedTensor, is_main_process
-
 from .position_encoding import build_position_encoding

-
-class FrozenBatchNorm2d(torch.nn.Module):
+class FrozenBatchNorm2d(nn.Module):
     """
     BatchNorm2d where the batch statistics and the affine parameters are fixed.

@@ -52,17 +51,17 @@ class FrozenBatchNorm2d(torch.nn.Module):
             missing_keys, unexpected_keys, error_msgs)

     def forward(self, x):
-        # move reshapes to the beginning
-        # to make it fuser-friendly
-        w = self.weight.reshape(1, -1, 1, 1)
-        b = self.bias.reshape(1, -1, 1, 1)
-        rv = self.running_var.reshape(1, -1, 1, 1)
-        rm = self.running_mean.reshape(1, -1, 1, 1)
-        eps = self.eps
-        scale = w * (rv + eps).rsqrt()
-        bias = b - rm * scale
-        return x * scale + bias
-
+        return torch.nn.functional.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias, False, 0, self.eps)
+
+class MaxPool2d(nn.Module):
+    def __init__(self, kernel_size, stride, padding):
+        super(MaxPool2d, self).__init__()
+        self.kernel_size = kernel_size
+        self.stride = stride
+        self.padding = padding
+
+    def forward(self, x):
+        return npu_max_pool2d(x, self.kernel_size, self.stride, self.padding)

 class BackboneBase(nn.Module):

@@ -80,7 +79,10 @@ class BackboneBase(nn.Module):
             return_layers = {'layer4': "0"}
             self.strides = [32]
             self.num_channels = [2048]
+
         self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)
+        self.body.maxpool = MaxPool2d(self.body.maxpool.kernel_size, self.body.maxpool.stride,
+            self.body.maxpool.padding)

     def forward(self, tensor_list: NestedTensor):
         xs = self.body(tensor_list.tensors)
diff --git a/models/deformable_detr.py b/models/deformable_detr.py
old mode 100644
new mode 100755
index f1415e8..1465672
--- a/models/deformable_detr.py
+++ b/models/deformable_detr.py
@@ -14,6 +14,7 @@ import torch
 import torch.nn.functional as F
 from torch import nn
 import math
+import torch_npu

 from util import box_ops
 from util.misc import (NestedTensor, nested_tensor_from_tensor_list,
@@ -275,6 +276,7 @@ class SetCriterion(nn.Module):
         loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(
             box_ops.box_cxcywh_to_xyxy(src_boxes),
             box_ops.box_cxcywh_to_xyxy(target_boxes)))
+
         losses['loss_giou'] = loss_giou.sum() / num_boxes
         return losses

@@ -329,25 +331,22 @@ class SetCriterion(nn.Module):
         assert loss in loss_map, f'do you really want to compute {loss} loss?'
         return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)

-    def forward(self, outputs, targets):
+    def forward(self, outputs, targets, num_boxes, handle, s):
         """ This performs the loss computation.
         Parameters:
              outputs: dict of tensors, see the output specification of the model for the format
              targets: list of dicts, such that len(targets) == batch_size.
                       The expected keys in each dict depends on the losses applied, see each loss' doc
         """
+        handle.wait()
+        with torch.cuda.stream(s):
+            s.wait_stream(torch.cuda.default_stream())
+            num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()
         outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs' and k != 'enc_outputs'}

         # Retrieve the matching between the outputs of the last layer and the targets
         indices = self.matcher(outputs_without_aux, targets)

-        # Compute the average number of target boxes accross all nodes, for normalization purposes
-        num_boxes = sum(len(t["labels"]) for t in targets)
-        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)
-        if is_dist_avail_and_initialized():
-            torch.distributed.all_reduce(num_boxes)
-        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()
-
         # Compute all the requested losses
         losses = {}
         for loss in self.losses:
@@ -387,9 +386,16 @@ class SetCriterion(nn.Module):
                 l_dict = self.get_loss(loss, enc_outputs, bin_targets, indices, num_boxes, **kwargs)
                 l_dict = {k + f'_enc': v for k, v in l_dict.items()}
                 losses.update(l_dict)
-
         return losses

+    def get_num_boxes(self, targets, device):
+        num_boxes = sum(len(t["labels"]) for t in targets)
+        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=device)
+        if is_dist_avail_and_initialized():
+            s = torch.cuda.Stream()
+            handle = torch.distributed.all_reduce(num_boxes, async_op=True)
+        return num_boxes, handle, s
+

 class PostProcess(nn.Module):
     """ This module converts the model's output into the format expected by the coco api"""
diff --git a/models/deformable_transformer.py b/models/deformable_transformer.py
old mode 100644
new mode 100755
index 08ca377..81e9516
--- a/models/deformable_transformer.py
+++ b/models/deformable_transformer.py
@@ -17,7 +17,7 @@ from torch import nn, Tensor
 from torch.nn.init import xavier_uniform_, constant_, uniform_, normal_

 from util.misc import inverse_sigmoid
-from models.ops.modules import MSDeformAttn
+from .ms_deform_attn import MSDeformAttn


 class DeformableTransformer(nn.Module):
@@ -148,6 +148,7 @@ class DeformableTransformer(nn.Module):
         spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)
         level_start_index = torch.cat((spatial_shapes.new_zeros((1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
         valid_ratios = torch.stack([self.get_valid_ratio(m) for m in masks], 1)
+

         # encoder
         memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)
@@ -238,7 +239,6 @@ class DeformableTransformerEncoder(nn.Module):
     def get_reference_points(spatial_shapes, valid_ratios, device):
         reference_points_list = []
         for lvl, (H_, W_) in enumerate(spatial_shapes):
-
             ref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),
                                           torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))
             ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H_)
@@ -333,7 +333,8 @@ class DeformableTransformerDecoder(nn.Module):
                 reference_points_input = reference_points[:, :, None] \
                                          * torch.cat([src_valid_ratios, src_valid_ratios], -1)[:, None]
             else:
-                assert reference_points.shape[-1] == 2
+                ## reomve assert
+                # assert reference_points.shape[-1] == 2
                 reference_points_input = reference_points[:, :, None] * src_valid_ratios[:, None]
             output = layer(output, query_pos, reference_points_input, src, src_spatial_shapes, src_level_start_index, src_padding_mask)

@@ -344,7 +345,7 @@ class DeformableTransformerDecoder(nn.Module):
                     new_reference_points = tmp + inverse_sigmoid(reference_points)
                     new_reference_points = new_reference_points.sigmoid()
                 else:
-                    assert reference_points.shape[-1] == 2
+                    # assert reference_points.shape[-1] == 2
                     new_reference_points = tmp
                     new_reference_points[..., :2] = tmp[..., :2] + inverse_sigmoid(reference_points)
                     new_reference_points = new_reference_points.sigmoid()
diff --git a/models/matcher.py b/models/matcher.py
old mode 100644
new mode 100755
index 63ef029..6924824
--- a/models/matcher.py
+++ b/models/matcher.py
@@ -13,7 +13,7 @@ Modules to compute the matching cost and solve the corresponding LSAP.
 import torch
 from scipy.optimize import linear_sum_assignment
 from torch import nn
-
+import torch_npu
 from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou


@@ -83,9 +83,8 @@ class HungarianMatcher(nn.Module):
             # Compute the L1 cost between boxes
             cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)

-            # Compute the giou cost betwen boxes
-            cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox),
-                                             box_cxcywh_to_xyxy(tgt_bbox))
+            # # Compute the giou cost betwen boxes
+            cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))

             # Final cost matrix
             C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou
diff --git a/models/ops/modules/ms_deform_attn.py b/models/ms_deform_attn.py
similarity index 90%
rename from models/ops/modules/ms_deform_attn.py
rename to models/ms_deform_attn.py
index 663d64a..353ea66 100644
--- a/models/ops/modules/ms_deform_attn.py
+++ b/models/ms_deform_attn.py
@@ -13,13 +13,14 @@
 import warnings
 import math

 import torch
 from torch import nn
 import torch.nn.functional as F
+from torch.autograd import Function
+from torch.autograd.function import once_differentiable
 from torch.nn.init import xavier_uniform_, constant_

-from ..functions import MSDeformAttnFunction
-
+import mx_driving

 def _is_power_of_2(n):
     if (not isinstance(n, int)) or (n < 0):
@@ -89,8 +86,6 @@ class MSDeformAttn(nn.Module):
         """
         N, Len_q, _ = query.shape
         N, Len_in, _ = input_flatten.shape
-        assert (input_spatial_shapes[:, 0] * input_spatial_shapes[:, 1]).sum() == Len_in
-
         value = self.value_proj(input_flatten)
         if input_padding_mask is not None:
             value = value.masked_fill(input_padding_mask[..., None], float(0))
@@ -102,14 +97,14 @@ class MSDeformAttn(nn.Module):
         if reference_points.shape[-1] == 2:
             offset_normalizer = torch.stack([input_spatial_shapes[..., 1], input_spatial_shapes[..., 0]], -1)
             sampling_locations = reference_points[:, :, None, :, None, :] \
-                                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+                                 + sampling_offsets / (offset_normalizer[None, None, None, :, None, :])
         elif reference_points.shape[-1] == 4:
             sampling_locations = reference_points[:, :, None, :, None, :2] \
-                                 + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5
+                                 + sampling_offsets / (self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5)
         else:
             raise ValueError(
                 'Last dim of reference_points must be 2 or 4, but get {} instead.'.format(reference_points.shape[-1]))
-        output = MSDeformAttnFunction.apply(
-            value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)
+        output = mx_driving.multi_scale_deformable_attn(
+            value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights)
         output = self.output_proj(output)
         return output
diff --git a/models/position_encoding.py b/models/position_encoding.py
old mode 100644
new mode 100755
index a92f0d3..d39965a
--- a/models/position_encoding.py
+++ b/models/position_encoding.py
@@ -34,19 +34,17 @@ class PositionEmbeddingSine(nn.Module):
         self.scale = scale

     def forward(self, tensor_list: NestedTensor):
-        x = tensor_list.tensors
-        mask = tensor_list.mask
-        assert mask is not None
-        not_mask = ~mask
+        not_mask = ~tensor_list.mask
         y_embed = not_mask.cumsum(1, dtype=torch.float32)
         x_embed = not_mask.cumsum(2, dtype=torch.float32)
+
         if self.normalize:
             eps = 1e-6
             y_embed = (y_embed - 0.5) / (y_embed[:, -1:, :] + eps) * self.scale
             x_embed = (x_embed - 0.5) / (x_embed[:, :, -1:] + eps) * self.scale

-        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)
-        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
+        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=tensor_list.tensors.device)
+        dim_t = self.temperature ** ((dim_t // 2) * (2 / self.num_pos_feats))

         pos_x = x_embed[:, :, :, None] / dim_t
         pos_y = y_embed[:, :, :, None] / dim_t
diff --git a/models/segmentation.py b/models/segmentation.py
old mode 100644
new mode 100755
index c801c0e..ae7fd5c
--- a/models/segmentation.py
+++ b/models/segmentation.py
@@ -211,11 +211,10 @@ def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: f
     """
     prob = inputs.sigmoid()
     ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")
-    p_t = prob * targets + (1 - prob) * (1 - targets)
-    loss = ce_loss * ((1 - p_t) ** gamma)
+    loss = ce_loss * ((prob + targets - 2 * prob * targets) ** gamma)

     if alpha >= 0:
-        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
+        alpha_t = 2 * alpha * targets - alpha - targets + 1
         loss = alpha_t * loss

     return loss.mean(1).sum() / num_boxes
diff --git a/requirements.txt b/requirements.txt
old mode 100644
new mode 100755
diff --git a/tools/run_dist_launch.sh b/tools/run_dist_launch.sh
deleted file mode 100755
index f6f6c4f..0000000
--- a/tools/run_dist_launch.sh
+++ /dev/null
@@ -1,29 +0,0 @@
-#!/usr/bin/env bash
-# ------------------------------------------------------------------------
-# Deformable DETR
-# Copyright (c) 2020 SenseTime. All Rights Reserved.
-# Licensed under the Apache License, Version 2.0 [see LICENSE for details]
-# ------------------------------------------------------------------------
-
-set -x
-
-GPUS=$1
-RUN_COMMAND=${@:2}
-if [ $GPUS -lt 8 ]; then
-    GPUS_PER_NODE=${GPUS_PER_NODE:-$GPUS}
-else
-    GPUS_PER_NODE=${GPUS_PER_NODE:-8}
-fi
-MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
-MASTER_PORT=${MASTER_PORT:-"29500"}
-NODE_RANK=${NODE_RANK:-0}
-
-let "NNODES=GPUS/GPUS_PER_NODE"
-
-python ./tools/launch.py \
-    --nnodes ${NNODES} \
-    --node_rank ${NODE_RANK} \
-    --master_addr ${MASTER_ADDR} \
-    --master_port ${MASTER_PORT} \
-    --nproc_per_node ${GPUS_PER_NODE} \
-    ${RUN_COMMAND}
\ No newline at end of file
diff --git a/util/__init__.py b/util/__init__.py
old mode 100644
new mode 100755
diff --git a/util/box_ops.py b/util/box_ops.py
old mode 100644
new mode 100755
index ca29592..79aeafb
--- a/util/box_ops.py
+++ b/util/box_ops.py
@@ -16,8 +16,10 @@ from torchvision.ops.boxes import box_area

 def box_cxcywh_to_xyxy(x):
     x_c, y_c, w, h = x.unbind(-1)
-    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
-         (x_c + 0.5 * w), (y_c + 0.5 * h)]
+    w = 0.5 * w
+    h = 0.5 * h
+    b = [(x_c - w), (y_c - h),
+         (x_c + w), (y_c + h)]
     return torch.stack(b, dim=-1)


@@ -54,10 +56,6 @@ def generalized_box_iou(boxes1, boxes2):
     Returns a [N, M] pairwise matrix, where N = len(boxes1)
     and M = len(boxes2)
     """
-    # degenerate boxes gives inf / nan results
-    # so do an early check
-    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
-    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()
     iou, union = box_iou(boxes1, boxes2)

     lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
@@ -68,7 +66,6 @@ def generalized_box_iou(boxes1, boxes2):

     return iou - (area - union) / area

-
 def masks_to_boxes(masks):
     """Compute the bounding boxes around the provided masks

diff --git a/util/misc.py b/util/misc.py
old mode 100644
new mode 100755
index 6d4d076..56c26c3
--- a/util/misc.py
+++ b/util/misc.py
@@ -27,37 +27,6 @@ from torch import Tensor

 # needed due to empty tensor bug in pytorch and torchvision 0.5
 import torchvision
-if float(torchvision.__version__[:3]) < 0.5:
-    import math
-    from torchvision.ops.misc import _NewEmptyTensorOp
-    def _check_size_scale_factor(dim, size, scale_factor):
-        # type: (int, Optional[List[int]], Optional[float]) -> None
-        if size is None and scale_factor is None:
-            raise ValueError("either size or scale_factor should be defined")
-        if size is not None and scale_factor is not None:
-            raise ValueError("only one of size or scale_factor should be defined")
-        if not (scale_factor is not None and len(scale_factor) != dim):
-            raise ValueError(
-                "scale_factor shape must match input shape. "
-                "Input is {}D, scale_factor size is {}".format(dim, len(scale_factor))
-            )
-    def _output_size(dim, input, size, scale_factor):
-        # type: (int, Tensor, Optional[List[int]], Optional[float]) -> List[int]
-        assert dim == 2
-        _check_size_scale_factor(dim, size, scale_factor)
-        if size is not None:
-            return size
-        # if dim is not 2 or scale_factor is iterable use _ntuple instead of concat
-        assert scale_factor is not None and isinstance(scale_factor, (int, float))
-        scale_factors = [scale_factor, scale_factor]
-        # math.floor might return float in py2.7
-        return [
-            int(math.floor(input.size(i + 2) * scale_factors[i])) for i in range(dim)
-        ]
-elif float(torchvision.__version__[:3]) < 0.7:
-    from torchvision.ops import _new_empty_tensor
-    from torchvision.ops.misc import _output_size
-

 class SmoothedValue(object):
     """Track a series of values and provide access to smoothed values over a
@@ -83,7 +52,7 @@ class SmoothedValue(object):
         """
         if not is_dist_avail_and_initialized():
             return
-        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')
+        t = torch.tensor([self.count, self.total], dtype=torch.float32, device='cuda')
         dist.barrier()
         dist.all_reduce(t)
         t = t.tolist()
@@ -190,6 +159,26 @@ def reduce_dict(input_dict, average=True):
         reduced_dict = {k: v for k, v in zip(names, values)}
     return reduced_dict

+def reduce_values(input_dict, average=True):
+    world_size = get_world_size()
+    if world_size < 2:
+        return input_dict
+    with torch.no_grad():
+        names = []
+        values = []
+        # sort the keys so that they are consistent across processes
+        for k in sorted(input_dict.keys()):
+            names.append(k)
+            values.append(input_dict[k])
+        values = torch.stack(values, dim=0)
+        handle = dist.all_reduce(values, async_op=True)
+    return values, names, handle
+
+def get_values(values, names, handle):
+    handle.wait()
+    values /= get_world_size()
+    reduced_dict = {k: v for k, v in zip(names, values)}
+    return reduced_dict

 class MetricLogger(object):
     def __init__(self, delimiter="\t"):
@@ -434,7 +423,7 @@ def init_distributed_mode(args):
         num_gpus = torch.cuda.device_count()
         addr = subprocess.getoutput(
             'scontrol show hostname {} | head -n1'.format(node_list))
-        os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT', '29500')
+        os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT', '29501')
         os.environ['MASTER_ADDR'] = addr
         os.environ['WORLD_SIZE'] = str(ntasks)
         os.environ['RANK'] = str(proc_id)
@@ -487,20 +476,6 @@ def interpolate(input, size=None, scale_factor=None, mode="nearest", align_corne
     This will eventually be supported natively by PyTorch, and this
     class can go away.
     """
-    if float(torchvision.__version__[:3]) < 0.7:
-        if input.numel() > 0:
-            return torch.nn.functional.interpolate(
-                input, size, scale_factor, mode, align_corners
-            )
-
-        output_shape = _output_size(2, input, size, scale_factor)
-        output_shape = list(input.shape[:-2]) + list(output_shape)
-        if float(torchvision.__version__[:3]) < 0.5:
-            return _NewEmptyTensorOp.apply(input, output_shape)
-        return _new_empty_tensor(input, output_shape)
-    else:
-        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)
-

 def get_total_grad_norm(parameters, norm_type=2):
     parameters = list(filter(lambda p: p.grad is not None, parameters))
diff --git a/util/plot_utils.py b/util/plot_utils.py
old mode 100644
new mode 100755
