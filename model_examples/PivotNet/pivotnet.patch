diff --git a/configs/pivotnet_nuscenes_swint.py b/configs/pivotnet_nuscenes_swint.py
index 00b6f43..5065c87 100644
--- a/configs/pivotnet_nuscenes_swint.py
+++ b/configs/pivotnet_nuscenes_swint.py
@@ -3,6 +3,7 @@ import torch
 import numpy as np
 import torch.nn as nn
 from torch.optim import AdamW
+from torch_npu.optim import NpuFusedAdamW
 from torchvision.transforms import Compose
 from torch.optim.lr_scheduler import MultiStepLR
 from torch.utils.data.distributed import DistributedSampler
@@ -12,17 +13,23 @@ from mapmaster.engine.experiment import BaseExp
 from mapmaster.dataset.nuscenes_pivotnet import NuScenesMapDataset
 from mapmaster.dataset.transform import Resize, Normalize, ToTensor_Pivot
 from mapmaster.utils.misc import get_param_groups, is_distributed
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
+torch.npu.config.allow_internal_format = False
+torch.npu.set_compile_mode(jit_compile=False)
+


 class EXPConfig:
-
+
     DATA_ROOT = "/data/dataset/public/nuScenes/"
     IMAGE_SHAPE = (900, 1600)

     map_conf = dict(
         dataset_name="nuscenes",
-        nusc_root="/data/dataset/public/nuScenes",
-        anno_root="/data/dataset/public/nuScenes/customer/pivot-bezier",
+        nusc_root="data/nuscenes",
+        anno_root="data/nuscenes/customer/pivot-bezier",
         split_dir="assets/splits/nuscenes",
         num_classes=3,
         ego_size=(60, 30),
@@ -48,9 +55,9 @@ class EXPConfig:
         # image-branch
         im_backbone=dict(
             arch_name="swin_transformer",
-            bkb_kwargs=dict(arch="tiny",
-                            shift_mode=0,
-                            out_indices=(2, 3),
+            bkb_kwargs=dict(arch="tiny",
+                            shift_mode=0,
+                            out_indices=(2, 3),
                             use_checkpoint=True,
                             pretrained='assets/weights/upernet_swin_tiny_patch4_window7_512x512.pth'),
             ret_layers=2,
@@ -99,7 +106,7 @@ class EXPConfig:
                 mask_dim=256,
                 enforce_input_project=False,
                 query_split=(20, 25, 15),
-                max_pieces=pivot_conf["max_pieces"],
+                max_pieces=pivot_conf["max_pieces"],
             ),
         ),
         output_head=dict(
@@ -121,7 +128,7 @@ class EXPConfig:
                     weight_dict=dict(
                         sem_msk_loss=3,
                         ins_obj_loss=2, ins_msk_loss=5,
-                        pts_loss=30, collinear_pts_loss=10,
+                        pts_loss=30, collinear_pts_loss=10,
                         pt_logits_loss=2,
                     ),
                     decoder_weights=[0.4, 0.4, 0.4, 0.8, 1.2, 1.6]
@@ -167,9 +174,9 @@ class EXPConfig:
         iou_thicknesses=(1,),
         cd_thresholds=(0.2, 0.5, 1.0, 1.5, 5.0)
     )
-
+
     VAL_TXT = [
-        "assets/splits/nuscenes/val.txt",
+        "assets/splits/nuscenes/val.txt",
     ]


@@ -279,7 +286,7 @@ class Exp(BaseExp):

     def _configure_optimizer(self):
         optimizer_setup = self.exp_config.optimizer_setup
-        optimizer = AdamW(get_param_groups(self.model, optimizer_setup))
+        optimizer = NpuFusedAdamW(get_param_groups(self.model, optimizer_setup))
         return optimizer

     def _configure_lr_scheduler(self):
@@ -315,4 +322,7 @@ class Exp(BaseExp):


 if __name__ == "__main__":
-    MapMasterCli(Exp).run()
+    if os.environ.get("PIVOTNET_PERFORMANCE_FLAG"):
+        MapMasterCli(Exp, stop_step=1500).run()
+    else:
+        MapMasterCli(Exp).run()
diff --git a/diff.patch b/diff.patch
new file mode 100644
index 0000000..e69de29
diff --git a/mapmaster/engine/core.py b/mapmaster/engine/core.py
index 4ecc69f..c968aa8 100644
--- a/mapmaster/engine/core.py
+++ b/mapmaster/engine/core.py
@@ -10,15 +10,21 @@ from mapmaster.engine.callbacks import CheckPointLoader, CheckPointSaver, ClearM
 from mapmaster.engine.callbacks import TensorBoardMonitor, TextMonitor, ClipGrad
 from mapmaster.utils.env import collect_env_info, get_root_dir
 from mapmaster.utils.misc import setup_logger, sanitize_filename, PyDecorator, all_gather_object
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu


+torch.npu.config.allow_internal_format = False
+torch.npu.set_compile_mode(jit_compile=False)
+
 __all__ = ["BaseCli", "BeMapNetCli"]


 class BaseCli:
     """Command line tools for any exp."""

-    def __init__(self, Exp):
+    def __init__(self, Exp, stop_step=None):
         """Make sure the order of initialization is: build_args --> build_env --> build_exp,
         since experiments depend on the environment and the environment depends on args.

@@ -28,6 +34,7 @@ class BaseCli:
         self.ExpCls = Exp
         self.args = self._get_parser(Exp).parse_args()
         self.env = RlaunchReplicaEnv(self.args.sync_bn, self.args.devices, self.args.find_unused_parameters)
+        self.stop_step = stop_step

     @property
     def exp(self):
@@ -140,7 +147,7 @@ class BaseCli:
             callbacks.append(CheckPointLoader(args.pretrained_model, weight_only=True))
         callbacks.extend(exp.callbacks)

-        trainer = Trainer(exp=exp, callbacks=callbacks, logger=logger, evaluator=evaluator)
+        trainer = Trainer(exp=exp, callbacks=callbacks, logger=logger, evaluator=evaluator, stop_step=self.stop_step)
         return trainer

     def executor(self):
diff --git a/mapmaster/engine/executor.py b/mapmaster/engine/executor.py
index a7bcf4c..44ee5d1 100644
--- a/mapmaster/engine/executor.py
+++ b/mapmaster/engine/executor.py
@@ -4,8 +4,13 @@ from tqdm import tqdm
 from typing import Sequence
 from mapmaster.engine.experiment import BaseExp
 from mapmaster.utils.misc import get_rank, synchronize
+import torch_npu
+from torch_npu.contrib import transfer_to_npu


+torch.npu.config.allow_internal_format = False
+torch.npu.set_compile_mode(jit_compile=False)
+
 __all__ = ["Callback", "BaseExecutor", "Trainer", "BeMapNetEvaluator"]


@@ -99,7 +104,7 @@ class BaseExecutor:

 class Trainer(BaseExecutor):
     def __init__(
-        self, exp: BaseExp, callbacks: Sequence["Callback"], logger=None, use_amp=False, evaluator=None
+        self, exp: BaseExp, callbacks: Sequence["Callback"], logger=None, use_amp=False, evaluator=None, stop_step=None
     ) -> None:
         super(Trainer, self).__init__(exp, callbacks, logger)
         self.use_amp = use_amp
@@ -107,6 +112,8 @@ class Trainer(BaseExecutor):
         if self.use_amp:
             self.grad_scaler = torch.cuda.amp.GradScaler()

+        self.stop_step=stop_step
+
     def train(self):
         self.train_iter = iter(self.train_dataloader)
         self._invoke_callback("before_train")
@@ -125,13 +132,25 @@ class Trainer(BaseExecutor):
         sampler = self.train_dataloader.sampler
         if hasattr(sampler, "set_epoch"):
             sampler.set_epoch(epoch)
-        for step in range(len(self.train_dataloader)):
-            try:
-                data = next(self.train_iter)
-            except StopIteration:
-                self.train_iter = iter(self.train_dataloader)
-                data = next(self.train_iter)
-            self.train_step(data, step)
+        if not self.stop_step:
+            for step in range(len(self.train_dataloader)):
+                try:
+                    data = next(self.train_iter)
+                except StopIteration:
+                    self.train_iter = iter(self.train_dataloader)
+                    data = next(self.train_iter)
+                self.train_step(data, step)
+        else:
+            for step in range(len(self.train_dataloader)):
+                try:
+                    data = next(self.train_iter)
+                except StopIteration:
+                    self.train_iter = iter(self.train_dataloader)
+                    data = next(self.train_iter)
+                self.train_step(data, step)
+                if step == self.stop_step:
+                    break
+
         if self.evaluator is not None:
             self.evaluator.eval()
         self._invoke_callback("after_epoch", epoch, update_best_ckpt=False)
diff --git a/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py b/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py
index 28f9ea1..87dbcf7 100644
--- a/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py
+++ b/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py
@@ -155,7 +155,8 @@ class DeformTransformer(nn.Module):

         cams = []
         for cam in extrinsic:
-            cam_coords = torch.linalg.inv(cam) @ coords_flatten.T  # (4, N)
+            # cam_coords = torch.linalg.inv(cam) @ coords_flatten.T  # (4, N)
+            cam_coords = torch.matmul(torch.inverse(cam), coords_flatten.T)
             cam_coords = cam_coords[:3, :]  # (3, N) -- x, y, z
             cams.append(cam_coords)
         cams = torch.stack(cams, dim=0)  # (6, 3, N) Coordinates in Camera Frame
diff --git a/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py b/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py
index c091ed6..ad31f6c 100644
--- a/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py
+++ b/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py
@@ -9,26 +9,26 @@
 from __future__ import absolute_import
 from __future__ import print_function
 from __future__ import division
-
-import warnings
-import math
-
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.nn.init import xavier_uniform_, constant_
-
-from ..functions import MSDeformAttnFunction
-
-
-def _is_power_of_2(n):
-    if (not isinstance(n, int)) or (n < 0):
-        raise ValueError("invalid input for _is_power_of_2: {} (type: {})".format(n, type(n)))
-    return (n & (n - 1) == 0) and n != 0
-
-
-class MSDeformAttn(nn.Module):
-    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):
+
+import warnings
+import math
+
+import torch
+from torch import nn
+import torch.nn.functional as F
+from torch.nn.init import xavier_uniform_, constant_
+
+import torch, torch_npu
+from mx_driving import npu_multi_scale_deformable_attn_function
+
+def _is_power_of_2(n):
+    if (not isinstance(n, int)) or (n < 0):
+        raise ValueError("invalid input for _is_power_of_2: {} (type: {})".format(n, type(n)))
+    return (n & (n - 1) == 0) and n != 0
+
+
+class MSDeformAttn(nn.Module):
+    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):
         """
         Multi-Scale Deformable Attention Module
         :param d_model      hidden dimension
@@ -119,22 +119,22 @@ class MSDeformAttn(nn.Module):
                 reference_points[:, :, None, :, None, :]
                 + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
             )
-        elif reference_points.shape[-1] == 4:
-            sampling_locations = (
-                reference_points[:, :, None, :, None, :2]
-                + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5
-            )
-        else:
-            raise ValueError(
-                "Last dim of reference_points must be 2 or 4, but get {} instead.".format(reference_points.shape[-1])
-            )
-        output = MSDeformAttnFunction.apply(
-            value,
-            input_spatial_shapes,
-            input_level_start_index,
-            sampling_locations,
-            attention_weights,
-            self.im2col_step,
-        )
-        output = self.output_proj(output)
-        return output
+        elif reference_points.shape[-1] == 4:
+            sampling_locations = (
+                reference_points[:, :, None, :, None, :2]
+                + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5
+            )
+        else:
+            raise ValueError(
+                "Last dim of reference_points must be 2 or 4, but get {} instead.".format(reference_points.shape[-1])
+            )
+
+        output = npu_multi_scale_deformable_attn_function(
+            value,
+            input_spatial_shapes,
+            input_level_start_index,
+            sampling_locations,
+            attention_weights,
+        )
+        output = self.output_proj(output)
+        return output
diff --git a/mapmaster/models/output_head/line_matching.py b/mapmaster/models/output_head/line_matching.py
index c690ffb..8c47f85 100644
--- a/mapmaster/models/output_head/line_matching.py
+++ b/mapmaster/models/output_head/line_matching.py
@@ -1,33 +1,67 @@
 import numpy as np
+import torch
+
+# def seq_matching_dist_parallel(cost, gt_lens, coe_endpts=0):
+#     # Time complexity: O(m*n)
+#     bs, m, n = cost.shape
+#     assert m <= n
+#     min_cost = np.ones((bs, m, n)) * np.inf
+#     mem_sort_value = np.ones((bs, m, n)) * np.inf  # v[i][j] = np.min(min_cost[i][:j+1])
+
+#     # initialization
+#     for j in range(0, n):
+#         if j == 0:
+#             min_cost[:, 0, j] = cost[:, 0, j]
+#         mem_sort_value[:, 0, j] = min_cost[:, 0, 0]
+
+#     for i in range(1, m):
+#         for j in range(i, n):
+#             min_cost[:, i, j] = mem_sort_value[:, i-1, j-1] + cost[:, i, j]
+#             indexes = (min_cost[:, i, j] < mem_sort_value[:, i, j-1])
+#             indexes_inv = np.array(1-indexes, dtype=np.bool)
+#             mem_sort_value[indexes, i, j] = min_cost[indexes, i, j]
+#             mem_sort_value[indexes_inv, i, j] = mem_sort_value[indexes_inv, i, j-1]
+
+#     indexes = []
+#     for i, ll in enumerate(gt_lens):
+#         indexes.append([i, ll-1, n-1])
+#     indexes = np.array(indexes)
+#     xs, ys, zs = indexes[:, 0], indexes[:, 1], indexes[:, 2]
+#     res_cost = min_cost[xs, ys, zs] + (cost[xs, 0, 0] + cost[xs, ys, zs]) * coe_endpts
+#     return  res_cost / (indexes[:, 1]+1+coe_endpts*2)

 def seq_matching_dist_parallel(cost, gt_lens, coe_endpts=0):
+    if cost.device.type != 'npu':
+        cost = cost.to('npu')
+    if gt_lens.device.type != 'npu':
+        gt_lens = gt_lens.to('npu')
+
     # Time complexity: O(m*n)
     bs, m, n = cost.shape
     assert m <= n
-    min_cost = np.ones((bs, m, n)) * np.inf
-    mem_sort_value = np.ones((bs, m, n)) * np.inf  # v[i][j] = np.min(min_cost[i][:j+1])

-    # initialization
-    for j in range(0, n):
-        if j == 0:
-            min_cost[:, 0, j] = cost[:, 0, j]
-        mem_sort_value[:, 0, j] = min_cost[:, 0, 0]
-
+    min_cost = torch.full((bs, m, n), float('inf'), device=cost.device)
+    mem_sort_value = torch.full((bs, m, n), float('inf'), device=cost.device)
+
+    # Initialization
+    min_cost[:, 0, 0] = cost[:, 0, 0]
+    mem_sort_value[:, 0, :] = min_cost[:, 0, 0].unsqueeze(-1)
+
+    # Vectorized computation
     for i in range(1, m):
-        for j in range(i, n):
-            min_cost[:, i, j] = mem_sort_value[:, i-1, j-1] + cost[:, i, j]
-            indexes = (min_cost[:, i, j] < mem_sort_value[:, i, j-1])
-            indexes_inv = np.array(1-indexes, dtype=np.bool)
-            mem_sort_value[indexes, i, j] = min_cost[indexes, i, j]
-            mem_sort_value[indexes_inv, i, j] = mem_sort_value[indexes_inv, i, j-1]
-
-    indexes = []
-    for i, ll in enumerate(gt_lens):
-        indexes.append([i, ll-1, n-1])
-    indexes = np.array(indexes)
-    xs, ys, zs = indexes[:, 0], indexes[:, 1], indexes[:, 2]
-    res_cost = min_cost[xs, ys, zs] + (cost[xs, 0, 0] + cost[xs, ys, zs]) * coe_endpts
-    return  res_cost / (indexes[:, 1]+1+coe_endpts*2)
+        # 计算 min_cost[:, i, i:]
+        min_cost[:, i, i:] = mem_sort_value[:, i-1, i-1:-1] + cost[:, i, i:]
+        mem_sort_value[:, i, i:], _ = torch.cummin(min_cost[:, i, i:], dim=-1)
+
+    batch_idx = torch.arange(bs, device=cost.device)
+    ys = gt_lens.to(torch.long) - 1
+    zs = torch.full((bs,), n-1, device=cost.device, dtype=torch.long)
+
+    res_min_cost = min_cost[batch_idx, ys, zs]
+    endpoint_cost = (cost[batch_idx, 0, 0] + cost[batch_idx, ys, zs]) * coe_endpts
+    denominator = gt_lens.to(res_min_cost.dtype) + coe_endpts * 2
+
+    return (res_min_cost + endpoint_cost) / denominator

 def pivot_dynamic_matching(cost: np.array):
     # Time complexity: O(m*n)
diff --git a/mapmaster/models/output_head/pivot_post_processor.py b/mapmaster/models/output_head/pivot_post_processor.py
index cc49dcc..42e8646 100644
--- a/mapmaster/models/output_head/pivot_post_processor.py
+++ b/mapmaster/models/output_head/pivot_post_processor.py
@@ -56,15 +56,24 @@ class HungarianMatcher(nn.Module):
                 gt_pts_mask = torch.zeros(gt_num, n_pt, dtype=torch.double, device=gt_pts.device)
                 gt_lens = torch.tensor([ll for ll in targets[0]["valid_len"][cid]]) # n_gt
                 gt_lens = gt_lens.unsqueeze(-1).repeat(1, dt_num).flatten()
-                for i, ll in enumerate(targets[0]["valid_len"][cid]):
-                    gt_pts_mask[i][:ll] = 1
+                # for i, ll in enumerate(targets[0]["valid_len"][cid]):
+                #     gt_pts_mask[i][:ll] = 1
+
+                valid_lens = torch.tensor(targets[0]["valid_len"][cid], device=gt_pts.device, dtype=torch.long)
+                row_indices = torch.arange(n_pt, device=gt_pts.device).expand(len(valid_lens), -1)
+                gt_pts_mask = (row_indices < valid_lens.unsqueeze(1)).to(dtype=torch.float32)
+
                 gt_pts_mask = gt_pts_mask.unsqueeze(1).unsqueeze(-1).repeat(1, dt_num, 1, n_pt).flatten(0, 1)
                 cost_mat_seqmatching = torch.cdist(gt_pts, dt_pts, p=1) * gt_pts_mask                # [n_gt*n_dt, n_pts, n_pts]
+                # cost_mat_seqmatching = seq_matching_dist_parallel(
+                #     cost_mat_seqmatching.detach().cpu().numpy(),
+                #     gt_lens,
+                #     self.coe_endpts).reshape(gt_num, dt_num).transpose(1, 0)  #[n_gt, n_dt]
                 cost_mat_seqmatching = seq_matching_dist_parallel(
-                    cost_mat_seqmatching.detach().cpu().numpy(),
+                    cost_mat_seqmatching,
                     gt_lens,
                     self.coe_endpts).reshape(gt_num, dt_num).transpose(1, 0)  #[n_gt, n_dt]
-                cost_mat_seqmatching = torch.from_numpy(cost_mat_seqmatching).to(cost_mat_mask.device)
+                # cost_mat_seqmatching = torch.from_numpy(cost_mat_seqmatching).to(cost_mat_mask.device)

                 # 4. sum mat
                 sizes = [len(tgt["obj_labels"][cid]) for tgt in targets]
@@ -154,6 +163,8 @@ class SetCriterion(nn.Module):
                     _, matched_pt_idx = pivot_dynamic_matching(cost_mat.detach().cpu().numpy())
                     matched_pt_idx = torch.tensor(matched_pt_idx)
                     # match pts loss
+                    # npu adapt, no damage to precision
+                    tgt_pts = torch.tensor(tgt_pts, dtype=torch.float32)
                     loss_match = w * F.l1_loss(src_pts[matched_pt_idx], tgt_pts, reduction="none").sum(dim=-1)   # [n_gt_pt, 2] -> [n_gt_dt]
                     loss_match = (loss_match * weight_pt).sum() / weight_pt.sum()
                     loss_pts += loss_match / num_instances
@@ -191,14 +202,20 @@ class SetCriterion(nn.Module):
             inter_loss = F.l1_loss(collinear_src_pts, inter_tgt, reduction="sum")
         return inter_loss

+    # @staticmethod
+    # def interpolate(start_pt, end_pt, inter_num):
+    #     res = torch.zeros((inter_num, 2), dtype=start_pt.dtype, device=start_pt.device)
+    #     num_len = inter_num + 1  # segment num.
+    #     for i in range(1, num_len):
+    #         ratio = i / num_len
+    #         res[i-1] = (1 - ratio) * start_pt + ratio * end_pt
+    #     return res
+
     @staticmethod
     def interpolate(start_pt, end_pt, inter_num):
-        res = torch.zeros((inter_num, 2), dtype=start_pt.dtype, device=start_pt.device)
-        num_len = inter_num + 1  # segment num.
-        for i in range(1, num_len):
-            ratio = i / num_len
-            res[i-1] = (1 - ratio) * start_pt + ratio * end_pt
-        return res
+        ratios = torch.arange(1, inter_num + 1, dtype=start_pt.dtype, device=start_pt.device) / (inter_num + 1)
+        ratios = ratios.view(-1, 1)
+        return (1 - ratios) * start_pt + ratios * end_pt

     def criterion_instance_labels(self, outputs, targets, matching_indices):
         loss_labels = 0
diff --git a/requirement.txt b/requirement.txt
index 7120c16..8e55b13 100644
--- a/requirement.txt
+++ b/requirement.txt
@@ -1,12 +1,14 @@
-clearml
-loguru
-Ninja
-numba
-opencv-contrib-python
-pandas
-scikit-image
-tabulate
-tensorboardX
-Pillow==9.4.0
-numpy==1.23.5
-visvalingamwyatt=0.2.0
\ No newline at end of file
+torchvision==0.16.0
+decorator
+clearml
+loguru
+Ninja
+numba
+opencv-contrib-python
+pandas
+scikit-image
+tabulate
+tensorboardX
+Pillow==9.4.0
+numpy==1.23.5
+visvalingamwyatt==0.2.0
\ No newline at end of file
diff --git a/run.sh b/run.sh
index ee8c865..bab12de 100644
--- a/run.sh
+++ b/run.sh
@@ -1,6 +1,9 @@
+
 #!/usr/bin/env bash

-export PYTHONPATH=$(pwd)
+export PYTHONPATH=$PYTHONPATH:$(pwd)
+export CPU_AFFINITY_CONF=1
+export TASK_QUEUE_ENABLE=2

 case "$1" in
     "train")
diff --git a/tools/evaluation/cd.py b/tools/evaluation/cd.py
index 9edf001..35cd9c4 100644
--- a/tools/evaluation/cd.py
+++ b/tools/evaluation/cd.py
@@ -1,20 +1,21 @@
-import torch
-
-def chamfer_distance(source_pc, target_pc, threshold, cum=False, bidirectional=True):
-    torch.backends.cuda.matmul.allow_tf32 = False
-    torch.backends.cudnn.allow_tf32 = False
-    # dist = torch.cdist(source_pc.float(), target_pc.float())
-    # dist = torch.cdist(source_pc.float(), target_pc.float(), compute_mode='donot_use_mm_for_euclid_dist')
-    dist = torch.cdist(source_pc.type(torch.float64), target_pc.type(torch.float64))
-    dist1, _ = torch.min(dist, 2)
-    dist2, _ = torch.min(dist, 1)
-    if cum:
-        len1 = dist1.shape[-1]
-        len2 = dist2.shape[-1]
-        dist1 = dist1.sum(-1)
-        dist2 = dist2.sum(-1)
-        return dist1, dist2, len1, len2
-    dist1 = dist1.mean(-1)
+import torch
+
+def chamfer_distance(source_pc, target_pc, threshold, cum=False, bidirectional=True):
+    torch.backends.cuda.matmul.allow_tf32 = False
+    torch.backends.cudnn.allow_tf32 = False
+    # dist = torch.cdist(source_pc.float(), target_pc.float())
+    # dist = torch.cdist(source_pc.float(), target_pc.float(), compute_mode='donot_use_mm_for_euclid_dist')
+    # npu adapt, no damage to precision
+    dist = torch.cdist(source_pc.type(torch.float32), target_pc.type(torch.float32))
+    dist1, _ = torch.min(dist, 2)
+    dist2, _ = torch.min(dist, 1)
+    if cum:
+        len1 = dist1.shape[-1]
+        len2 = dist2.shape[-1]
+        dist1 = dist1.sum(-1)
+        dist2 = dist2.sum(-1)
+        return dist1, dist2, len1, len2
+    dist1 = dist1.mean(-1)
     dist2 = dist2.mean(-1)
     if bidirectional:
         return min((dist1 + dist2) / 2, threshold)
diff --git a/tools/evaluation/eval.py b/tools/evaluation/eval.py
index d85a976..df2b940 100644
--- a/tools/evaluation/eval.py
+++ b/tools/evaluation/eval.py
@@ -1,22 +1,23 @@
-import os
-import sys
-import torch
-import numpy as np
-import pickle as pkl
-from tqdm import tqdm
-from tabulate import tabulate
-from torch.utils.data import Dataset, DataLoader
-from ap import instance_mask_ap as get_batch_ap
-
-
-class BeMapNetResultForNuScenes(Dataset):
-    def __init__(self, gt_dir, dt_dir, val_txt):
-        self.gt_dir, self.dt_dir = gt_dir, dt_dir
-        self.tokens = [fname.strip().split('.')[0] for fname in open(val_txt).readlines()]
-        self.max_line_count = 100
-
-    def __getitem__(self, idx):
-        token = self.tokens[idx]
+import os
+import sys
+import torch
+import numpy as np
+import pickle as pkl
+from tqdm import tqdm
+from tabulate import tabulate
+from torch.utils.data import Dataset, DataLoader
+from ap import instance_mask_ap as get_batch_ap
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
+class BeMapNetResultForNuScenes(Dataset):
+    def __init__(self, gt_dir, dt_dir, val_txt):
+        self.gt_dir, self.dt_dir = gt_dir, dt_dir
+        self.tokens = [fname.strip().split('.')[0] for fname in open(val_txt).readlines()]
+        self.max_line_count = 100
+
+    def __getitem__(self, idx):
+        token = self.tokens[idx]
         gt_path = os.path.join(self.gt_dir, f"{token}.npz")
         gt_masks = np.load(open(gt_path, "rb"), allow_pickle=True)["instance_mask"]
         dt_item = np.load(os.path.join(self.dt_dir, f"{token}.npz"), allow_pickle=True)
