diff --git a/torch_geometric/nn/conv/message_passing.py b/torch_geometric/nn/conv/message_passing.py
index 2db5c9893..cb99ec80f 100644
--- a/torch_geometric/nn/conv/message_passing.py
+++ b/torch_geometric/nn/conv/message_passing.py
@@ -20,7 +20,6 @@ from uuid import uuid1
 import torch
 from torch import Tensor
 from torch.utils.hooks import RemovableHandle
-
 from torch_geometric.nn.aggr import Aggregation, MultiAggregation
 from torch_geometric.nn.conv.utils.inspector import (
     Inspector,
@@ -43,6 +42,18 @@ from torch_geometric.utils import (
 )
 from torch_geometric.utils.sparse import ptr2index
 
+import torch_npu
+import ctypes
+try:
+    from mx_driving import npu_index_select
+    npu_index_select_available = True
+except ImportError:
+    npu_index_select_available = False
+
+lib_path = os.path.join(os.environ.get('ASCEND_HOME_PATH', '/usr/local/Ascend'), 'lib64', 'libopapi.so')
+lib = ctypes.CDLL(lib_path)
+aclnnIndexAddV2_available = hasattr(lib, "aclnnIndexAddV2")
+index_select_op = None
 FUSE_AGGRS = {'add', 'sum', 'mean', 'min', 'max'}
 
 
@@ -190,6 +201,16 @@ class MessagePassing(torch.nn.Module):
         self._edge_update_forward_pre_hooks = OrderedDict()
         self._edge_update_forward_hooks = OrderedDict()
 
+        global index_select_op
+    
+        # 检查是否已初始化
+        if index_select_op is not None:
+            return
+        if npu_index_select_available and aclnnIndexAddV2_available:
+            index_select_op = npu_index_select
+        else:
+            index_select_op = torch_npu.index_select
+
     def reset_parameters(self):
         r"""Resets all learnable parameters of the module."""
         if self.aggr_module is not None:
@@ -264,12 +285,24 @@ class MessagePassing(torch.nn.Module):
             else:
                 raise ValueError(f"Unsupported sparse tensor layout "
                                  f"(got '{edge_index.layout}')")
-            return src.index_select(self.node_dim, index)
+            tmp = src.reshape(len(src), -1)
+            x = index_select_op(tmp, self.node_dim, index)
+            if len(x) == 0:
+                x_shape = x.shape[1]
+                return x.reshape(len(x), 8, x_shape//8)
+            else:
+                return x.reshape(len(x), 8, -1)
 
         elif isinstance(edge_index, Tensor):
             try:
                 index = edge_index[dim]
-                return src.index_select(self.node_dim, index)
+                tmp = src.reshape(len(src), -1)
+                x = index_select_op(tmp, self.node_dim, index)
+                if len(x) == 0:
+                    x_shape = x.shape[1]
+                    return x.reshape(len(x), 8, x_shape//8)
+                else:
+                    return x.reshape(len(x), 8, -1)
             except (IndexError, RuntimeError) as e:
                 if index.min() < 0 or index.max() >= src.size(self.node_dim):
                     raise IndexError(
@@ -304,10 +337,22 @@ class MessagePassing(torch.nn.Module):
         elif isinstance(edge_index, SparseTensor):
             if dim == 0:
                 col = edge_index.storage.col()
-                return src.index_select(self.node_dim, col)
+                tmp = src.reshape(len(src), -1)
+                x = index_select_op(tmp, self.node_dim, col)
+                if len(x) == 0:
+                    x_shape = x.shape[1]
+                    return x.reshape(len(x), 8, x_shape//8)
+                else:
+                    return x.reshape(len(x), 8, -1)
             elif dim == 1:
                 row = edge_index.storage.row()
-                return src.index_select(self.node_dim, row)
+                tmp = src.reshape(len(src), -1)
+                x = index_select_op(tmp, self.node_dim, row)
+                if len(x) == 0:
+                    x_shape = x.shape[1]
+                    return x.reshape(len(x), 8, x_shape//8)
+                else:
+                    return x.reshape(len(x), 8, -1)
 
         raise ValueError(
             ('`MessagePassing.propagate` only supports integer tensors of '
diff --git a/torch_geometric/utils/scatter.py b/torch_geometric/utils/scatter.py
index a75f1fe36..0aa8fe916 100644
--- a/torch_geometric/utils/scatter.py
+++ b/torch_geometric/utils/scatter.py
@@ -7,6 +7,9 @@ from torch import Tensor
 import torch_geometric.typing
 from torch_geometric.typing import torch_scatter
 
+import torch_npu
+import mx_driving
+
 major, minor, _ = torch.__version__.split('.', maxsplit=2)
 major, minor = int(major), int(minor)
 has_pytorch112 = major > 1 or (major == 1 and minor >= 12)
@@ -42,87 +45,19 @@ if has_pytorch112:  # pragma: no cover
                 :obj:`"mean"`, :obj:`"mul"`, :obj:`"min"` or :obj:`"max"`).
                 (default: :obj:`"sum"`)
         """
-        if index.dim() != 1:
-            raise ValueError(f"The `index` argument must be one-dimensional "
-                             f"(got {index.dim()} dimensions)")
-
-        dim = src.dim() + dim if dim < 0 else dim
-
-        if dim < 0 or dim >= src.dim():
-            raise ValueError(f"The `dim` argument must lay between 0 and "
-                             f"{src.dim() - 1} (got {dim})")
-
-        if dim_size is None:
-            dim_size = int(index.max()) + 1 if index.numel() > 0 else 0
-
-        # For now, we maintain various different code paths, based on whether
-        # the input requires gradients and whether it lays on the CPU/GPU.
-        # For example, `torch_scatter` is usually faster than
-        # `torch.scatter_reduce` on GPU, while `torch.scatter_reduce` is faster
-        # on CPU.
-        # `torch.scatter_reduce` has a faster forward implementation for
-        # "min"/"max" reductions since it does not compute additional arg
-        # indices, but is therefore way slower in its backward implementation.
-        # More insights can be found in `test/utils/test_scatter.py`.
-
-        size = list(src.size())
-        size[dim] = dim_size
-
-        # For "sum" and "mean" reduction, we make use of `scatter_add_`:
+        if not torch_geometric.typing.WITH_TORCH_SCATTER:
+            raise ImportError("'scatter' requires the 'torch-scatter' package")
         if reduce == 'sum' or reduce == 'add':
-            index = broadcast(index, src, dim)
-            return src.new_zeros(size).scatter_add_(dim, index, src)
-
-        if reduce == 'mean':
-            count = src.new_zeros(dim_size)
-            count.scatter_add_(0, index, src.new_ones(src.size(dim)))
-            count = count.clamp(min=1)
-
-            index = broadcast(index, src, dim)
-            out = src.new_zeros(size).scatter_add_(dim, index, src)
-
-            return out / broadcast(count, out, dim)
-
-        # For "min" and "max" reduction, we prefer `scatter_reduce_` on CPU or
-        # in case the input does not require gradients:
-        if reduce == 'min' or reduce == 'max':
-            if (not torch_geometric.typing.WITH_TORCH_SCATTER
-                    or not src.is_cuda or not src.requires_grad):
-
-                if src.is_cuda and src.requires_grad:
-                    warnings.warn(f"The usage of `scatter(reduce='{reduce}')` "
-                                  f"can be accelerated via the 'torch-scatter'"
-                                  f" package, but it was not found")
-
-                index = broadcast(index, src, dim)
-                return src.new_zeros(size).scatter_reduce_(
-                    dim, index, src, reduce=f'a{reduce}', include_self=False)
-
-            return torch_scatter.scatter(src, index, dim, dim_size=dim_size,
-                                         reduce=reduce)
-
-        # For "mul" reduction, we prefer `scatter_reduce_` on CPU:
-        if reduce == 'mul':
-            if (not torch_geometric.typing.WITH_TORCH_SCATTER
-                    or not src.is_cuda):
-
-                if src.is_cuda:
-                    warnings.warn(f"The usage of `scatter(reduce='{reduce}')` "
-                                  f"can be accelerated via the 'torch-scatter'"
-                                  f" package, but it was not found")
-
-                index = broadcast(index, src, dim)
-                # We initialize with `one` here to match `scatter_mul` output:
-                return src.new_ones(size).scatter_reduce_(
-                    dim, index, src, reduce='prod', include_self=True)
-
+            return mx_driving.scatter_add(src.float(), index.to(torch.int32), None, dim, dim_size)
+        elif reduce == 'mean':
+            return mx_driving.scatter_mean(src.float(), index.to(torch.int32), None, dim, dim_size)
+        elif reduce == 'max':
+            return mx_driving.scatter_max(src.float(), index.to(torch.int32), None)[0]
+        else:
             return torch_scatter.scatter(src, index, dim, dim_size=dim_size,
-                                         reduce='mul')
-
-        raise ValueError(f"Encountered invalid `reduce` argument '{reduce}'")
+                                        reduce=reduce)
 
 else:
-
     def scatter(src: Tensor, index: Tensor, dim: int = 0,
                 dim_size: Optional[int] = None, reduce: str = 'sum') -> Tensor:
         r"""Reduces all values from the :obj:`src` tensor at the indices
@@ -147,5 +82,12 @@ else:
         """
         if not torch_geometric.typing.WITH_TORCH_SCATTER:
             raise ImportError("'scatter' requires the 'torch-scatter' package")
-        return torch_scatter.scatter(src, index, dim, dim_size=dim_size,
-                                     reduce=reduce)
+        if reduce == 'sum' or reduce == 'add':
+            return mx_driving.scatter_add(src.float(), index.to(torch.int32), None, dim, dim_size)
+        elif reduce == 'mean':
+            return mx_driving.scatter_mean(src.float(), index.to(torch.int32), None, dim, dim_size)
+        elif reduce == 'max':
+            return mx_driving.scatter_max(src.float(), index.to(torch.int32), None)[0]
+        else:
+            return torch_scatter.scatter(src, index, dim, dim_size=dim_size,
+                                        reduce=reduce)
diff --git a/torch_geometric/utils/softmax.py b/torch_geometric/utils/softmax.py
index 3ea36a0fa..73caee6dd 100644
--- a/torch_geometric/utils/softmax.py
+++ b/torch_geometric/utils/softmax.py
@@ -2,9 +2,8 @@ from typing import Optional
 
 from torch import Tensor
 
-from torch_geometric.utils import scatter, segment
-from torch_geometric.utils.num_nodes import maybe_num_nodes
-
+from torch_geometric.utils import segment
+import mx_driving
 
 def softmax(
     src: Tensor,
@@ -61,12 +60,7 @@ def softmax(
         out_sum = segment(out, ptr, reduce='sum') + 1e-16
         out_sum = out_sum.repeat_interleave(count, dim=dim)
     elif index is not None:
-        N = maybe_num_nodes(index, num_nodes)
-        src_max = scatter(src.detach(), index, dim, dim_size=N, reduce='max')
-        out = src - src_max.index_select(dim, index)
-        out = out.exp()
-        out_sum = scatter(out, index, dim, dim_size=N, reduce='sum') + 1e-16
-        out_sum = out_sum.index_select(dim, index)
+        return mx_driving.graph_softmax(src, index)
     else:
         raise NotImplementedError
 
