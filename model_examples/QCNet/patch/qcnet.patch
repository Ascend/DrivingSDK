diff --git a/datamodules/argoverse_v2_dataloader.py b/datamodules/argoverse_v2_dataloader.py
new file mode 100644
index 0000000..4e1e50f
--- /dev/null
+++ b/datamodules/argoverse_v2_dataloader.py
@@ -0,0 +1,70 @@
+# Copyright (c) Huawei Technologies Co., Ltd. 2025. All rights reserved.
+# ------------------------------------------------------------------------
+# Copyright (c) 2023, Zikang Zhou. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ------------------------------------------------------------------------
+# Copyright (c) 2023 PyG Team. All rights reserved.
+# ------------------------------------------------------------------------
+# Copyright (c) 2018 Matthias Fey. All rights reserved.
+# ------------------------------------------------------------------------
+# Licensed under The MIT License [see LICENSE for details]
+
+from collections.abc import Mapping
+from typing import TypeVar, Optional, Iterator, List, Optional, Sequence, Union
+from functools import partial
+import math
+import numpy as np
+import torch
+import torch.utils.data
+from torch.utils.data import DataLoader
+
+from torch_geometric.data import Batch, Dataset
+from torch_geometric.data.data import BaseData
+from torch_geometric.data.datapipes import DatasetAdapter
+
+from mx_driving.dataset.agent_dataset import AgentDynamicBatchSampler, DynamicBatchSampler, Collater
+
+
+class QCNetDynamicBatchSampler(AgentDynamicBatchSampler):
+    def bucket_arange(self):
+        g = torch.Generator()
+        if self.epoch >= 44:
+            self.seed=1
+        g.manual_seed(self.epoch + self.seed)
+        # Shuffling buckets order.
+        bucket_index = torch.randperm(len(self.dataset.buckets), generator=g).tolist()
+        indices = []
+        for bct_idx in bucket_index:
+            bucket = self.dataset.buckets[bct_idx]
+            # Shuffling samples in a bucket.
+            indices.extend([bucket[i] for i in torch.randperm(len(bucket), generator=g).tolist()])
+
+        return indices
+
+
+class QCNetDynamicBatchDataLoader(DataLoader):
+    def __init__(self, 
+                 dataset: Union[Dataset, Sequence[BaseData], DatasetAdapter],
+                 batch_size: int,
+                 train_batch_size: int,
+                 shuffle: bool = True,
+                 follow_batch: Optional[List[str]] = None,
+                 exclude_keys: Optional[List[str]] = None,
+                 **kwargs) -> None:
+        kwargs.pop('collate_fn', None)
+        kwargs.pop('batch_sampler', None)
+
+        self.follow_batch = follow_batch
+        self.exclude_keys = exclude_keys
+        sampler = QCNetDynamicBatchSampler(dataset, shuffle=True)
+
+        super().__init__(
+            dataset, collate_fn=Collater(follow_batch, exclude_keys),
+            batch_sampler=DynamicBatchSampler(dataset, sampler, train_batch_size), **kwargs)
\ No newline at end of file
diff --git a/datamodules/argoverse_v2_datamodule.py b/datamodules/argoverse_v2_datamodule.py
index 1b55133..1efee1c 100644
--- a/datamodules/argoverse_v2_datamodule.py
+++ b/datamodules/argoverse_v2_datamodule.py
@@ -13,8 +13,9 @@
 # limitations under the License.
 from typing import Callable, Optional
 
-import pytorch_lightning as pl
+import lightning.pytorch as pl
 from torch_geometric.loader import DataLoader
+from .argoverse_v2_dataloader import QCNetDynamicBatchDataLoader
 
 from datasets import ArgoverseV2Dataset
 from transforms import TargetBuilder
@@ -40,6 +41,7 @@ class ArgoverseV2DataModule(pl.LightningDataModule):
                  train_transform: Optional[Callable] = TargetBuilder(50, 60),
                  val_transform: Optional[Callable] = TargetBuilder(50, 60),
                  test_transform: Optional[Callable] = None,
+                 dynamic_sort: bool = False,
                  **kwargs) -> None:
         super(ArgoverseV2DataModule, self).__init__()
         self.root = root
@@ -59,6 +61,7 @@ class ArgoverseV2DataModule(pl.LightningDataModule):
         self.train_transform = train_transform
         self.val_transform = val_transform
         self.test_transform = test_transform
+        self.dynamic_sort = dynamic_sort
 
     def prepare_data(self) -> None:
         ArgoverseV2Dataset(self.root, 'train', self.train_raw_dir, self.train_processed_dir, self.train_transform)
@@ -66,17 +69,26 @@ class ArgoverseV2DataModule(pl.LightningDataModule):
         ArgoverseV2Dataset(self.root, 'test', self.test_raw_dir, self.test_processed_dir, self.test_transform)
 
     def setup(self, stage: Optional[str] = None) -> None:
-        self.train_dataset = ArgoverseV2Dataset(self.root, 'train', self.train_raw_dir, self.train_processed_dir,
-                                                self.train_transform)
+        if self.dynamic_sort:
+            self.train_dataset = ArgoverseV2Dataset(self.root, 'train', self.train_raw_dir, self.train_processed_dir,
+                                                    self.train_transform, use_dynamic_sort=True)
+        else:
+            self.train_dataset = ArgoverseV2Dataset(self.root, 'train', self.train_raw_dir, self.train_processed_dir,
+                                                    self.train_transform)
         self.val_dataset = ArgoverseV2Dataset(self.root, 'val', self.val_raw_dir, self.val_processed_dir,
                                               self.val_transform)
         self.test_dataset = ArgoverseV2Dataset(self.root, 'test', self.test_raw_dir, self.test_processed_dir,
                                                self.test_transform)
 
     def train_dataloader(self):
-        return DataLoader(self.train_dataset, batch_size=self.train_batch_size, shuffle=self.shuffle,
-                          num_workers=self.num_workers, pin_memory=self.pin_memory,
-                          persistent_workers=self.persistent_workers)
+        if self.dynamic_sort:
+            return QCNetDynamicBatchDataLoader(self.train_dataset, batch_size=self.train_batch_size, train_batch_size=self.train_batch_size, shuffle=self.shuffle,
+                                          num_workers=self.num_workers, pin_memory=self.pin_memory,
+                                          persistent_workers=self.persistent_workers)
+        else:
+            return DataLoader(self.train_dataset, batch_size=self.train_batch_size, shuffle=self.shuffle,
+                              num_workers=self.num_workers, pin_memory=self.pin_memory,
+                              persistent_workers=self.persistent_workers)
 
     def val_dataloader(self):
         return DataLoader(self.val_dataset, batch_size=self.val_batch_size, shuffle=False,
diff --git a/datasets/argoverse_v2_dataset.py b/datasets/argoverse_v2_dataset.py
index f78b02f..a32a9be 100644
--- a/datasets/argoverse_v2_dataset.py
+++ b/datasets/argoverse_v2_dataset.py
@@ -16,6 +16,7 @@ import os
 import pickle
 import shutil
 import sys
+import json
 from pathlib import Path
 from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple, Union
 from urllib import request
@@ -27,6 +28,7 @@ from torch_geometric.data import Dataset
 from torch_geometric.data import HeteroData
 from torch_geometric.data import extract_tar
 from tqdm import tqdm
+from mx_driving.dataset import AgentDynamicDataset
 
 from utils import safe_list_index
 from utils import side_to_directed_lineseg
@@ -43,7 +45,7 @@ except ImportError:
     read_json_file = object
 
 
-class ArgoverseV2Dataset(Dataset):
+class ArgoverseV2Dataset(Dataset, AgentDynamicDataset):
     """Dataset class for Argoverse 2 Motion Forecasting Dataset.
 
     See https://www.argoverse.org/av2.html for more information about the dataset.
@@ -80,7 +82,8 @@ class ArgoverseV2Dataset(Dataset):
                  num_historical_steps: int = 50,
                  num_future_steps: int = 60,
                  predict_unseen_agents: bool = False,
-                 vector_repr: bool = True) -> None:
+                 vector_repr: bool = True,
+                 use_dynamic_sort: bool = False) -> None:
         root = os.path.expanduser(os.path.normpath(root))
         if not os.path.isdir(root):
             os.makedirs(root)
@@ -105,6 +108,9 @@ class ArgoverseV2Dataset(Dataset):
             else:
                 self._raw_file_names = []
 
+        self._raw_file_names = np.array(self._raw_file_names)
+        self._raw_file_names = sorted(self._raw_file_names)
+
         if processed_dir is None:
             processed_dir = os.path.join(root, split, 'processed')
             self._processed_dir = processed_dir
@@ -123,8 +129,8 @@ class ArgoverseV2Dataset(Dataset):
                                               name.endswith(('pkl', 'pickle'))]
             else:
                 self._processed_file_names = []
+                self.dim = dim
 
-        self.dim = dim
         self.num_historical_steps = num_historical_steps
         self.num_future_steps = num_future_steps
         self.num_steps = num_historical_steps + num_future_steps
@@ -136,6 +142,11 @@ class ArgoverseV2Dataset(Dataset):
             'val': 24988,
             'test': 24984,
         }[split]
+
+        self.agents_num = [0 for _ in range(self._num_samples)]
+        self._processed_file_names = np.array(self._processed_file_names)
+        self._processed_file_names = sorted(self._processed_file_names)
+
         self._agent_types = ['vehicle', 'pedestrian', 'motorcyclist', 'cyclist', 'bus', 'static', 'background',
                              'construction', 'riderless_bicycle', 'unknown']
         self._agent_categories = ['TRACK_FRAGMENT', 'UNSCORED_TRACK', 'SCORED_TRACK', 'FOCAL_TRACK']
@@ -147,7 +158,11 @@ class ArgoverseV2Dataset(Dataset):
                              'NONE', 'UNKNOWN', 'CROSSWALK', 'CENTERLINE']
         self._point_sides = ['LEFT', 'RIGHT', 'CENTER']
         self._polygon_to_polygon_types = ['NONE', 'PRED', 'SUCC', 'LEFT', 'RIGHT']
-        super(ArgoverseV2Dataset, self).__init__(root=root, transform=transform, pre_transform=None, pre_filter=None)
+        Dataset.__init__(self, root=root, transform=transform, pre_transform=None, pre_filter=None)
+        self.get_agents_num()
+        if use_dynamic_sort:
+            self.sorting()
+            self.bucketing()
 
     @property
     def raw_dir(self) -> str:
@@ -182,7 +197,9 @@ class ArgoverseV2Dataset(Dataset):
         os.rmdir(os.path.join(self.raw_dir, self.split))
 
     def process(self) -> None:
-        for raw_file_name in tqdm(self.raw_file_names):
+        agents_num_file_name = f"{self.split}_agents_num.json"
+        agents_num_file_path = os.path.join(self.processed_dir, agents_num_file_name)
+        for idx, raw_file_name in tqdm(enumerate(self.raw_file_names)):
             df = pd.read_parquet(os.path.join(self.raw_dir, raw_file_name, f'scenario_{raw_file_name}.parquet'))
             map_dir = Path(self.raw_dir) / raw_file_name
             map_path = map_dir / sorted(map_dir.glob('log_map_archive_*.json'))[0]
@@ -194,10 +211,14 @@ class ArgoverseV2Dataset(Dataset):
             data['scenario_id'] = self.get_scenario_id(df)
             data['city'] = self.get_city(df)
             data['agent'] = self.get_agent_features(df)
+            self.agents_num[idx] = data["agent"]['num_nodes']
             data.update(self.get_map_features(map_api, centerlines))
             with open(os.path.join(self.processed_dir, f'{raw_file_name}.pkl'), 'wb') as handle:
                 pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
 
+        with open(agents_num_file_path, 'w') as handle:
+            json.dump(self.agents_num, handle)
+
     @staticmethod
     def get_scenario_id(df: pd.DataFrame) -> str:
         return df['scenario_id'].values[0]
diff --git a/layers/__init__.py b/layers/__init__.py
index 6633b5a..2c8782b 100644
--- a/layers/__init__.py
+++ b/layers/__init__.py
@@ -14,3 +14,4 @@
 from layers.attention_layer import AttentionLayer
 from layers.fourier_embedding import FourierEmbedding
 from layers.mlp_layer import MLPLayer
+from layers.radius import radius, radius_graph
diff --git a/layers/attention_layer.py b/layers/attention_layer.py
index 3b62e71..137cd23 100644
--- a/layers/attention_layer.py
+++ b/layers/attention_layer.py
@@ -38,11 +38,9 @@ class AttentionLayer(MessagePassing):
         self.scale = head_dim ** -0.5
 
         self.to_q = nn.Linear(hidden_dim, head_dim * num_heads)
-        self.to_k = nn.Linear(hidden_dim, head_dim * num_heads, bias=False)
-        self.to_v = nn.Linear(hidden_dim, head_dim * num_heads)
+        self.to_kv = nn.Linear(hidden_dim, 2 * head_dim * num_heads, bias=False)
         if has_pos_emb:
-            self.to_k_r = nn.Linear(hidden_dim, head_dim * num_heads, bias=False)
-            self.to_v_r = nn.Linear(hidden_dim, head_dim * num_heads)
+            self.to_kv_r = nn.Linear(hidden_dim, 2 * head_dim * num_heads, bias=False)
         self.to_s = nn.Linear(hidden_dim, head_dim * num_heads)
         self.to_g = nn.Linear(head_dim * num_heads + hidden_dim, head_dim * num_heads)
         self.to_out = nn.Linear(head_dim * num_heads, hidden_dim)
@@ -85,14 +83,13 @@ class AttentionLayer(MessagePassing):
 
     def message(self,
                 q_i: torch.Tensor,
-                k_j: torch.Tensor,
-                v_j: torch.Tensor,
+                kv_j: torch.Tensor,
                 r: Optional[torch.Tensor],
                 index: torch.Tensor,
                 ptr: Optional[torch.Tensor]) -> torch.Tensor:
         if self.has_pos_emb and r is not None:
-            k_j = k_j + self.to_k_r(r).view(-1, self.num_heads, self.head_dim)
-            v_j = v_j + self.to_v_r(r).view(-1, self.num_heads, self.head_dim)
+            kv_j = kv_j + self.to_kv_r(r).view(-1, self.num_heads, 2 * self.head_dim)
+        k_j, v_j = torch.split(kv_j, self.head_dim, dim=2)
         sim = (q_i * k_j).sum(dim=-1) * self.scale
         attn = softmax(sim, index, ptr)
         attn = self.attn_drop(attn)
@@ -111,9 +108,8 @@ class AttentionLayer(MessagePassing):
                     r: Optional[torch.Tensor],
                     edge_index: torch.Tensor) -> torch.Tensor:
         q = self.to_q(x_dst).view(-1, self.num_heads, self.head_dim)
-        k = self.to_k(x_src).view(-1, self.num_heads, self.head_dim)
-        v = self.to_v(x_src).view(-1, self.num_heads, self.head_dim)
-        agg = self.propagate(edge_index=edge_index, x_dst=x_dst, q=q, k=k, v=v, r=r)
+        kv = self.to_kv(x_src).view(-1, self.num_heads, 2 * self.head_dim)
+        agg = self.propagate(edge_index=edge_index, x_dst=x_dst, q=q, kv=kv, r=r)
         return self.to_out(agg)
 
     def _ff_block(self, x: torch.Tensor) -> torch.Tensor:
diff --git a/layers/fourier_embedding.py b/layers/fourier_embedding.py
index 092643b..be2a28c 100644
--- a/layers/fourier_embedding.py
+++ b/layers/fourier_embedding.py
@@ -29,6 +29,7 @@ class FourierEmbedding(nn.Module):
         super(FourierEmbedding, self).__init__()
         self.input_dim = input_dim
         self.hidden_dim = hidden_dim
+        self.pi2 = 2 * math.pi
 
         self.freqs = nn.Embedding(input_dim, num_freq_bands) if input_dim != 0 else None
         self.mlps = nn.ModuleList(
@@ -55,7 +56,7 @@ class FourierEmbedding(nn.Module):
             else:
                 raise ValueError('Both continuous_inputs and categorical_embs are None')
         else:
-            x = continuous_inputs.unsqueeze(-1) * self.freqs.weight * 2 * math.pi
+            x = self.pi2 * continuous_inputs.unsqueeze(-1) * self.freqs.weight
             # Warning: if your data are noisy, don't use learnable sinusoidal embedding
             x = torch.cat([x.cos(), x.sin(), continuous_inputs.unsqueeze(-1)], dim=-1)
             continuous_embs: List[Optional[torch.Tensor]] = [None] * self.input_dim
diff --git a/layers/radius.py b/layers/radius.py
new file mode 100644
index 0000000..677ba95
--- /dev/null
+++ b/layers/radius.py
@@ -0,0 +1,134 @@
+from typing import Optional
+
+import torch
+import torch_npu
+import mx_driving
+
+
+def radius(x: torch.Tensor, y: torch.Tensor, r: float,
+           batch_x: Optional[torch.Tensor] = None,
+           batch_y: Optional[torch.Tensor] = None, max_num_neighbors: int = 32,
+           num_workers: int = 1) -> torch.Tensor:
+    r"""Finds for each element in :obj:`y` all points in :obj:`x` within
+    distance :obj:`r`.
+
+    Args:
+        x (Tensor): Node feature matrix
+            :math:`\mathbf{X} \in \mathbb{R}^{N \times F}`.
+        y (Tensor): Node feature matrix
+            :math:`\mathbf{Y} \in \mathbb{R}^{M \times F}`.
+        r (float): The radius.
+        batch_x (LongTensor, optional): Batch vector
+            :math:`\mathbf{b} \in {\{ 0, \ldots, B-1\}}^N`, which assigns each
+            node to a specific example. :obj:`batch_x` needs to be sorted.
+            (default: :obj:`None`)
+        batch_y (LongTensor, optional): Batch vector
+            :math:`\mathbf{b} \in {\{ 0, \ldots, B-1\}}^M`, which assigns each
+            node to a specific example. :obj:`batch_y` needs to be sorted.
+            (default: :obj:`None`)
+        max_num_neighbors (int, optional): The maximum number of neighbors to
+            return for each element in :obj:`y`.
+            If the number of actual neighbors is greater than
+            :obj:`max_num_neighbors`, returned neighbors are picked randomly.
+            (default: :obj:`32`)
+        num_workers (int): Number of workers to use for computation. Has no
+            effect in case :obj:`batch_x` or :obj:`batch_y` is not
+            :obj:`None`, or the input lies on the GPU. (default: :obj:`1`)
+
+    .. code-block:: python
+
+        import torch
+        from torch_cluster import radius
+
+        x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])
+        batch_x = torch.tensor([0, 0, 0, 0])
+        y = torch.Tensor([[-1, 0], [1, 0]])
+        batch_y = torch.tensor([0, 0])
+        assign_index = radius(x, y, 1.5, batch_x, batch_y)
+    """
+    if x.numel() == 0 or y.numel() == 0:
+        return torch.empty(2, 0, dtype=torch.long, device=x.device)
+
+    x = x.view(-1, 1) if x.dim() == 1 else x
+    y = y.view(-1, 1) if y.dim() == 1 else y
+    x, y = x.contiguous(), y.contiguous()
+
+    batch_size = 1
+    if batch_x is not None:
+        assert x.size(0) == batch_x.numel()
+        batch_size = int(batch_x.max()) + 1
+    if batch_y is not None:
+        assert y.size(0) == batch_y.numel()
+        batch_size = max(batch_size, int(batch_y.max()) + 1)
+
+    ptr_x: Optional[torch.Tensor] = None
+    ptr_y: Optional[torch.Tensor] = None
+    device = x.device
+    if batch_size > 1:
+        assert batch_x is not None
+        assert batch_y is not None
+        arange = torch.arange(batch_size + 1, device=x.device)
+        ptr_x = torch.bucketize(arange, batch_x).cpu()
+        ptr_y = torch.bucketize(arange, batch_y).cpu()
+    else:
+        ptr_x = torch.tensor([0, x.shape[0]]).to(device)
+        ptr_y = torch.tensor([0, y.shape[0]]).to(device)
+
+    return mx_driving.radius(x.npu(), y.npu(), ptr_x.int().to(device), ptr_y.int().to(device), r,
+                             max_num_neighbors)
+
+
+def radius_graph(x: torch.Tensor, r: float,
+                 batch: Optional[torch.Tensor] = None, loop: bool = False,
+                 max_num_neighbors: int = 32, flow: str = 'source_to_target',
+                 num_workers: int = 1) -> torch.Tensor:
+    r"""Computes graph edges to all points within a given distance.
+
+    Args:
+        x (Tensor): Node feature matrix
+            :math:`\mathbf{X} \in \mathbb{R}^{N \times F}`.
+        r (float): The radius.
+        batch (LongTensor, optional): Batch vector
+            :math:`\mathbf{b} \in {\{ 0, \ldots, B-1\}}^N`, which assigns each
+            node to a specific example. :obj:`batch` needs to be sorted.
+            (default: :obj:`None`)
+        loop (bool, optional): If :obj:`True`, the graph will contain
+            self-loops. (default: :obj:`False`)
+        max_num_neighbors (int, optional): The maximum number of neighbors to
+            return for each element.
+            If the number of actual neighbors is greater than
+            :obj:`max_num_neighbors`, returned neighbors are picked randomly.
+            (default: :obj:`32`)
+        flow (string, optional): The flow direction when used in combination
+            with message passing (:obj:`"source_to_target"` or
+            :obj:`"target_to_source"`). (default: :obj:`"source_to_target"`)
+        num_workers (int): Number of workers to use for computation. Has no
+            effect in case :obj:`batch` is not :obj:`None`, or the input lies
+            on the GPU. (default: :obj:`1`)
+
+    :rtype: :class:`LongTensor`
+
+    .. code-block:: python
+
+        import torch
+        from torch_cluster import radius_graph
+
+        x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])
+        batch = torch.tensor([0, 0, 0, 0])
+        edge_index = radius_graph(x, r=1.5, batch=batch, loop=False)
+    """
+
+    assert flow in ['source_to_target', 'target_to_source']
+    edge_index = radius(x, x, r, batch, batch,
+                        max_num_neighbors if loop else max_num_neighbors + 1,
+                        num_workers)
+    if flow == 'source_to_target':
+        row, col = edge_index[1], edge_index[0]
+    else:
+        row, col = edge_index[0], edge_index[1]
+
+    if not loop:
+        mask = row != col
+        row, col = row[mask], col[mask]
+
+    return torch.stack([row, col], dim=0)
diff --git a/modules/qcnet_agent_encoder.py b/modules/qcnet_agent_encoder.py
index 99d19a6..615a6e2 100644
--- a/modules/qcnet_agent_encoder.py
+++ b/modules/qcnet_agent_encoder.py
@@ -15,8 +15,6 @@ from typing import Dict, Mapping, Optional
 
 import torch
 import torch.nn as nn
-from torch_cluster import radius
-from torch_cluster import radius_graph
 from torch_geometric.data import Batch
 from torch_geometric.data import HeteroData
 from torch_geometric.utils import dense_to_sparse
@@ -24,6 +22,7 @@ from torch_geometric.utils import subgraph
 
 from layers.attention_layer import AttentionLayer
 from layers.fourier_embedding import FourierEmbedding
+from layers import radius, radius_graph
 from utils import angle_between_2d_vectors
 from utils import weight_init
 from utils import wrap_angle
@@ -101,6 +100,7 @@ class QCNetAgentEncoder(nn.Module):
         head_vector_a = torch.stack([head_a.cos(), head_a.sin()], dim=-1)
         pos_pl = data['map_polygon']['position'][:, :self.input_dim].contiguous()
         orient_pl = data['map_polygon']['orientation'].contiguous()
+
         if self.dataset == 'argoverse_v2':
             vel = data['agent']['velocity'][:, :self.num_historical_steps, :self.input_dim].contiguous()
             length = width = height = None
@@ -145,10 +145,17 @@ class QCNetAgentEncoder(nn.Module):
         pos_pl = pos_pl.repeat(self.num_historical_steps, 1)
         orient_pl = orient_pl.repeat(self.num_historical_steps)
         if isinstance(data, Batch):
-            batch_s = torch.cat([data['agent']['batch'] + data.num_graphs * t
-                                 for t in range(self.num_historical_steps)], dim=0)
-            batch_pl = torch.cat([data['map_polygon']['batch'] + data.num_graphs * t
-                                  for t in range(self.num_historical_steps)], dim=0)
+            agent_data = data['agent']['batch'].unsqueeze(dim = 0).repeat(self.num_historical_steps, 1)
+            agent_num = agent_data.shape[1]
+            time_range = data.num_graphs * torch.arange(self.num_historical_steps).to(device=pos_a.device)
+            time_range = time_range.unsqueeze(dim = 1).repeat(1, agent_num)
+            batch_s = (agent_data + time_range).reshape([-1])
+
+            map_data = data['map_polygon']['batch'].unsqueeze(dim = 0).repeat(self.num_historical_steps, 1)
+            map_num = map_data.shape[1]
+            time_range_map = data.num_graphs * torch.arange(self.num_historical_steps).to(device=pos_a.device)
+            time_range_map = time_range_map.unsqueeze(dim = 1).repeat(1, map_num)
+            batch_pl = (map_data + time_range_map).reshape([-1])
         else:
             batch_s = torch.arange(self.num_historical_steps,
                                    device=pos_a.device).repeat_interleave(data['agent']['num_nodes'])
diff --git a/modules/qcnet_decoder.py b/modules/qcnet_decoder.py
index 32066a5..59e69e6 100644
--- a/modules/qcnet_decoder.py
+++ b/modules/qcnet_decoder.py
@@ -17,8 +17,6 @@ from typing import Dict, List, Mapping, Optional
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from torch_cluster import radius
-from torch_cluster import radius_graph
 from torch_geometric.data import Batch
 from torch_geometric.data import HeteroData
 from torch_geometric.utils import dense_to_sparse
@@ -26,6 +24,7 @@ from torch_geometric.utils import dense_to_sparse
 from layers import AttentionLayer
 from layers import FourierEmbedding
 from layers import MLPLayer
+from layers import radius, radius_graph
 from utils import angle_between_2d_vectors
 from utils import bipartite_dense_to_sparse
 from utils import weight_init
@@ -83,9 +82,10 @@ class QCNetDecoder(nn.Module):
                                           num_freq_bands=num_freq_bands)
         self.y_emb = FourierEmbedding(input_dim=output_dim + output_head, hidden_dim=hidden_dim,
                                       num_freq_bands=num_freq_bands)
-        self.traj_emb = nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=1, bias=True,
-                               batch_first=False, dropout=0.0, bidirectional=False)
-        self.traj_emb_h0 = nn.Parameter(torch.zeros(1, hidden_dim))
+        self.traj_emb_lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=1, bias=True,
+                        batch_first=False, dropout=0.0, bidirectional=False)
+        self.lstm_h0 = nn.Parameter(torch.zeros([1, hidden_dim]))
+        self.lstm_c0 = nn.Parameter(torch.zeros([1, hidden_dim]))
         self.t2m_propose_attn_layers = nn.ModuleList(
             [AttentionLayer(hidden_dim=hidden_dim, num_heads=num_heads, head_dim=head_dim, dropout=dropout,
                             bipartite=True, has_pos_emb=True) for _ in range(num_layers)]
@@ -221,7 +221,8 @@ class QCNetDecoder(nn.Module):
                 m = m.reshape(-1, self.num_modes, self.hidden_dim).transpose(0, 1).reshape(-1, self.hidden_dim)
                 m = self.pl2m_propose_attn_layers[i]((x_pl, m), r_pl2m, edge_index_pl2m)
                 m = self.a2m_propose_attn_layers[i]((x_a, m), r_a2m, edge_index_a2m)
-                m = m.reshape(self.num_modes, -1, self.hidden_dim).transpose(0, 1).reshape(-1, self.hidden_dim)
+                m = m.reshape(self.num_modes, -1, self.hidden_dim).transpose(0, 1)
+            m = m.reshape(-1, self.hidden_dim)
             m = self.m2m_propose_attn_layer(m, None, edge_index_m2m)
             m = m.reshape(-1, self.num_modes, self.hidden_dim)
             locs_propose_pos[t] = self.to_loc_propose_pos(m)
@@ -252,7 +253,9 @@ class QCNetDecoder(nn.Module):
                                                              self.num_future_steps, 1))
             m = self.y_emb(loc_propose_pos.detach().view(-1, self.output_dim))
         m = m.reshape(-1, self.num_future_steps, self.hidden_dim).transpose(0, 1)
-        m = self.traj_emb(m, self.traj_emb_h0.unsqueeze(1).repeat(1, m.size(1), 1))[1].squeeze(0)
+        m_ = self.traj_emb_lstm(m.float(), (self.lstm_h0.unsqueeze(1).repeat(1, m.size(1), 1),
+                                            self.lstm_c0.unsqueeze(1).repeat(1, m.size(1), 1)))[1]
+        m = m_[0].squeeze(0)
         for i in range(self.num_layers):
             m = self.t2m_refine_attn_layers[i]((x_t, m), r_t2m, edge_index_t2m)
             m = m.reshape(-1, self.num_modes, self.hidden_dim).transpose(0, 1).reshape(-1, self.hidden_dim)
diff --git a/modules/qcnet_map_encoder.py b/modules/qcnet_map_encoder.py
index 19e3817..c9d8d59 100644
--- a/modules/qcnet_map_encoder.py
+++ b/modules/qcnet_map_encoder.py
@@ -15,12 +15,12 @@ from typing import Dict
 
 import torch
 import torch.nn as nn
-from torch_cluster import radius_graph
 from torch_geometric.data import Batch
 from torch_geometric.data import HeteroData
 
 from layers.attention_layer import AttentionLayer
 from layers.fourier_embedding import FourierEmbedding
+from layers import radius, radius_graph
 from utils import angle_between_2d_vectors
 from utils import merge_edges
 from utils import weight_init
diff --git a/predictors/qcnet.py b/predictors/qcnet.py
index 35ee89e..a0b6804 100644
--- a/predictors/qcnet.py
+++ b/predictors/qcnet.py
@@ -11,17 +11,19 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import time
 from itertools import chain
 from itertools import compress
 from pathlib import Path
 from typing import Optional
 
-import pytorch_lightning as pl
+import lightning.pytorch as pl
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch_geometric.data import Batch
 from torch_geometric.data import HeteroData
+import torch.distributed as dist
 
 from losses import MixtureNLLLoss
 from losses import NLLLoss
@@ -71,6 +73,7 @@ class QCNet(pl.LightningModule):
                  T_max: int,
                  submission_dir: str,
                  submission_file_name: str,
+                 profiling_step: int,
                  **kwargs) -> None:
         super(QCNet, self).__init__()
         self.save_hyperparameters()
@@ -152,6 +155,10 @@ class QCNet(pl.LightningModule):
         self.MR = MR(max_guesses=6)
 
         self.test_predictions = dict()
+        # for evaluating training speed
+        self.init_time = time.time()
+        self.profiling_step = profiling_step
+        self.avg_train_time = 0.0
 
     def forward(self, data: HeteroData):
         scene_enc = self.encoder(data)
@@ -200,6 +207,13 @@ class QCNet(pl.LightningModule):
                                  prob=pi,
                                  mask=reg_mask[:, -1:]) * cls_mask
         cls_loss = cls_loss.sum() / cls_mask.sum().clamp_(min=1)
+
+        if dist.get_rank() == 0 and batch_idx >= 1 and (batch_idx-1) % self.profiling_step == 0:
+            self.avg_train_time = (time.time() - self.init_time) / self.profiling_step
+            self.init_time = time.time()
+            if batch_idx != 1:
+                print(f"Average Training Time (step {batch_idx - self.profiling_step}-{batch_idx}): {self.avg_train_time:.3f}s")
+
         self.log('train_reg_loss_propose', reg_loss_propose, prog_bar=False, on_step=True, on_epoch=True, batch_size=1)
         self.log('train_reg_loss_refine', reg_loss_refine, prog_bar=False, on_step=True, on_epoch=True, batch_size=1)
         self.log('train_cls_loss', cls_loss, prog_bar=False, on_step=True, on_epoch=True, batch_size=1)
@@ -402,4 +416,6 @@ class QCNet(pl.LightningModule):
         parser.add_argument('--T_max', type=int, default=64)
         parser.add_argument('--submission_dir', type=str, default='./')
         parser.add_argument('--submission_file_name', type=str, default='submission')
+        parser.add_argument('--profiling_step', type=int, default=6246)
+        parser.add_argument('--dynamic_sort', action="store_true", default=True)
         return parent_parser
diff --git a/train_qcnet.py b/train_qcnet.py
index 092b41c..0fcc46c 100644
--- a/train_qcnet.py
+++ b/train_qcnet.py
@@ -13,16 +13,22 @@
 # limitations under the License.
 from argparse import ArgumentParser
 
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import LearningRateMonitor
-from pytorch_lightning.callbacks import ModelCheckpoint
-from pytorch_lightning.strategies import DDPStrategy
+import lightning as L
+import lightning.pytorch as pl
+from lightning.pytorch.callbacks import LearningRateMonitor
+from lightning.pytorch.callbacks import ModelCheckpoint
+from lightning.pytorch.strategies import DDPStrategy
+from lightning.pytorch.trainer import Trainer
 
 from datamodules import ArgoverseV2DataModule
 from predictors import QCNet
 
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
 if __name__ == '__main__':
-    pl.seed_everything(2023, workers=True)
+    L.seed_everything(2023, workers=True)
 
     parser = ArgumentParser()
     parser.add_argument('--root', type=str, required=True)
@@ -51,7 +57,7 @@ if __name__ == '__main__':
     }[args.dataset](**vars(args))
     model_checkpoint = ModelCheckpoint(monitor='val_minFDE', save_top_k=5, mode='min')
     lr_monitor = LearningRateMonitor(logging_interval='epoch')
-    trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices,
+    trainer = Trainer(accelerator=args.accelerator, devices=args.devices,
                          strategy=DDPStrategy(find_unused_parameters=False, gradient_as_bucket_view=True),
                          callbacks=[model_checkpoint, lr_monitor], max_epochs=args.max_epochs)
     trainer.fit(model, datamodule)
diff --git a/val.py b/val.py
index 7491b8d..216e089 100644
--- a/val.py
+++ b/val.py
@@ -13,7 +13,8 @@
 # limitations under the License.
 from argparse import ArgumentParser
 
-import pytorch_lightning as pl
+import lightning as L
+from lightning.pytorch.trainer import Trainer
 from torch_geometric.loader import DataLoader
 
 from datasets import ArgoverseV2Dataset
@@ -21,7 +22,7 @@ from predictors import QCNet
 from transforms import TargetBuilder
 
 if __name__ == '__main__':
-    pl.seed_everything(2023, workers=True)
+    L.seed_everything(2023, workers=True)
 
     parser = ArgumentParser()
     parser.add_argument('--model', type=str, required=True)
@@ -44,5 +45,5 @@ if __name__ == '__main__':
                      transform=TargetBuilder(model.num_historical_steps, model.num_future_steps))
     dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers,
                             pin_memory=args.pin_memory, persistent_workers=args.persistent_workers)
-    trainer = pl.Trainer(accelerator=args.accelerator, devices=args.devices, strategy='ddp')
+    trainer = Trainer(accelerator=args.accelerator, devices=args.devices, strategy='ddp')
     trainer.validate(model, dataloader)
