diff --git a/src/lerobot/configs/train.py b/src/lerobot/configs/train.py
index 2f3a65db..8c20f399 100644
--- a/src/lerobot/configs/train.py
+++ b/src/lerobot/configs/train.py
@@ -51,7 +51,7 @@ class TrainPipelineConfig(HubMixin):
     # AND for the evaluation environments.
     seed: int | None = 1000
     # Number of workers for the dataloader.
-    num_workers: int = 4
+    num_workers: int = 12
     batch_size: int = 8
     steps: int = 100_000
     eval_freq: int = 20_000
diff --git a/src/lerobot/optim/optimizers.py b/src/lerobot/optim/optimizers.py
index f2bd0df4..418bf4ca 100644
--- a/src/lerobot/optim/optimizers.py
+++ b/src/lerobot/optim/optimizers.py
@@ -21,6 +21,7 @@ from typing import Any
 import draccus
 import torch
 from safetensors.torch import load_file, save_file
+from mindspeed.optimizer.adamw import AdamW
 
 from lerobot.datasets.utils import flatten_dict, unflatten_dict, write_json
 from lerobot.utils.constants import (
@@ -85,7 +86,7 @@ class AdamWConfig(OptimizerConfig):
     def build(self, params: dict) -> torch.optim.Optimizer:
         kwargs = asdict(self)
         kwargs.pop("grad_clip_norm")
-        return torch.optim.AdamW(params, **kwargs)
+        return AdamW(params, **kwargs)
 
 
 @OptimizerConfig.register_subclass("sgd")
@@ -172,12 +173,25 @@ def save_optimizer_state(
         _save_single_optimizer_state(optimizer, save_dir)
 
 
+def convert_tensor_to_python(obj):
+    if isinstance(obj, torch.Tensor):
+        return obj.item() if obj.numel() == 1 else obj.tolist()
+    elif isinstance(obj, (list, tuple)):
+        return [convert_tensor_to_python(x) for x in obj]
+    elif isinstance(obj, dict):
+        return {k: convert_tensor_to_python(v) for k, v in obj.items()}
+    elif isinstance(obj, (int, float, str, bool, type(None))):
+        return obj
+    else:
+        return str(obj)
+
 def _save_single_optimizer_state(optimizer: torch.optim.Optimizer, save_dir: Path) -> None:
     """Save a single optimizer's state to disk."""
     state = optimizer.state_dict()
     param_groups = state.pop("param_groups")
     flat_state = flatten_dict(state)
     save_file(flat_state, save_dir / OPTIMIZER_STATE)
+    param_groups = convert_tensor_to_python(param_groups)
     write_json(param_groups, save_dir / OPTIMIZER_PARAM_GROUPS)
 
 
diff --git a/src/lerobot/policies/act/modeling_act.py b/src/lerobot/policies/act/modeling_act.py
index b7cbcd06..6591cd9a 100644
--- a/src/lerobot/policies/act/modeling_act.py
+++ b/src/lerobot/policies/act/modeling_act.py
@@ -551,8 +551,7 @@ class ACTEncoderLayer(nn.Module):
         if self.pre_norm:
             x = self.norm1(x)
         q = k = x if pos_embed is None else x + pos_embed
-        x = self.self_attn(q, k, value=x, key_padding_mask=key_padding_mask)
-        x = x[0]  # note: [0] to select just the output, not the attention weights
+        x = self.self_attn(q, k, value=x, key_padding_mask=key_padding_mask, need_weights=False)[0]
         x = skip + self.dropout1(x)
         if self.pre_norm:
             skip = x
@@ -635,7 +634,7 @@ class ACTDecoderLayer(nn.Module):
         if self.pre_norm:
             x = self.norm1(x)
         q = k = self.maybe_add_pos_embed(x, decoder_pos_embed)
-        x = self.self_attn(q, k, value=x)[0]  # select just the output, not the attention weights
+        x = self.self_attn(q, k, value=x, need_weights=False)[0]
         x = skip + self.dropout1(x)
         if self.pre_norm:
             skip = x
@@ -647,6 +646,7 @@ class ACTDecoderLayer(nn.Module):
             query=self.maybe_add_pos_embed(x, decoder_pos_embed),
             key=self.maybe_add_pos_embed(encoder_out, encoder_pos_embed),
             value=encoder_out,
+            need_weights=False
         )[0]  # select just the output, not the attention weights
         x = skip + self.dropout2(x)
         if self.pre_norm:
diff --git a/src/lerobot/scripts/lerobot_train.py b/src/lerobot/scripts/lerobot_train.py
index 84eb81ad..b746354c 100644
--- a/src/lerobot/scripts/lerobot_train.py
+++ b/src/lerobot/scripts/lerobot_train.py
@@ -20,6 +20,8 @@ from pprint import pformat
 from typing import Any
 
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 from accelerate import Accelerator
 from termcolor import colored
 from torch.optim import Optimizer
@@ -280,12 +282,13 @@ def train(cfg: TrainPipelineConfig, accelerator: Accelerator | None = None):
     dataloader = torch.utils.data.DataLoader(
         dataset,
         num_workers=cfg.num_workers,
+        persistent_workers=True,
         batch_size=cfg.batch_size,
         shuffle=shuffle and not cfg.dataset.streaming,
         sampler=sampler,
-        pin_memory=device.type == "cuda",
+        pin_memory=True,
         drop_last=False,
-        prefetch_factor=2 if cfg.num_workers > 0 else None,
+        prefetch_factor=4 if cfg.num_workers > 0 else None,
     )
 
     # Prepare everything with accelerator
