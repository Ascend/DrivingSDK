diff --git a/.DS_Store b/.DS_Store
deleted file mode 100644
index 1e4cefd..0000000
Binary files a/.DS_Store and /dev/null differ
diff --git a/README.md b/README.md
deleted file mode 100644
index 11276c9..0000000
--- a/README.md
+++ /dev/null
@@ -1,117 +0,0 @@
-# SurroundOcc
-### [Project Page](https://weiyithu.github.io/SurroundOcc/) | [Paper](https://arxiv.org/abs/2303.09551) | [Video](https://cloud.tsinghua.edu.cn/d/97b74c039b8d4fd48830/) | [Data](https://cloud.tsinghua.edu.cn/d/8dcb547238144d08a0bb/)
-<br/>
-
-> SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving  
-> [Yi Wei*](https://weiyithu.github.io/), [Linqing Zhao*](https://github.com/lqzhao), [Wenzhao Zheng](https://scholar.google.com/citations?user=LdK9scgAAAAJ&hl=en), [Zheng Zhu](http://www.zhengzhu.net/), [Jiwen Lu](http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/), [Jie Zhou](https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1)  
-
-## News
-- [2023/7/14]: SurroundOcc is accepted to ICCV 2023! 
-- [2023/5/19]: We upload generated mesh vertices. You can downsample it and generate occupancy labels with arbitrary resolution.  
-- [2023/3/21]: Support for private data. You can try both occupancy prediction method and ground truth generation pipeline on your own data. 
-- [2023/3/17]: Initial code and paper release. 
-- [2023/2/27]: Demo release.
-
-## Demo  
-Demos are a little bit large; please wait a moment to load them. If you cannot load them or feel them blurry, you can click the hyperlink of each demo for the full-resolution raw video. Welcome to the home page for more demos and detailed introductions. 
-
-### [Occupancy prediction:](https://cloud.tsinghua.edu.cn/f/f7768f1f110c414297cc/)
-
-<p align='center'>
-<img src="./assets/demo1.gif" width="720px">
-<img src="./assets/bar.jpg" width="720px">
-</p>
-
-### [Generated dense occupancy labels:](https://cloud.tsinghua.edu.cn/f/65d91a4c891f447da731/)
-<p align='center'>
-<img src="./assets/demo2.gif" width="720px">
-</p>
-
-
-## Introduction
-Towards a more comprehensive and consistent scene reconstruction, in this paper, we propose a SurroundOcc method to predict the volumetric occupancy with multi-camera images. We first extract multi-scale features for each image and adopt spatial cross attention to lift them to the 3D volume space. Then we apply 3D convolutions to progressively upsample the volume features and impose supervision on multiple levels. To train the multi-camera 3D scene reconstruction model, we design a pipeline to generate dense occupancy ground truth with sparse LiDAR points. The generation pipeline only needs existed 3D detection and 3D semantic segmentation labels without extra human annotations. Specifically, we fuse multi-frame LiDAR points of dynamic objects and static scenes separately. Then we adopt Poisson Reconstruction to fill the holes and voxelize the mesh to get dense volumetric occupancy.
-
-## Method 
-
-Method Pipeline:
-
-<p align='center'>
-<img src="./assets/pipeline.jpg" width="720px">
-</p>
-
-Occupancy Ground Truth Generation Pipeline:
-
-<p align='center'>
-<img src="./assets/groundtruth_pipeline.jpg" width="800px">
-</p>
-
-## Getting Started
-- [Installation](docs/install.md) 
-- [Prepare Dataset](docs/data.md)
-- [Train, Eval and Visualize](docs/run.md)
-
-You can download our pretrained model for [3D semantic occupancy prediction](https://cloud.tsinghua.edu.cn/f/7b2887a8fe3f472c8566/?dl=1) and [3D scene reconstruction tasks](https://cloud.tsinghua.edu.cn/f/ca595f31c8bd4ec49cf7/?dl=1). The difference is whether use semantic labels to train the model. The models are trained on 8 RTX 3090s with about 2.5 days.  
-
-## Try your own data
-### Occupancy prediction
-You can try our nuScenes [pretrained model](https://cloud.tsinghua.edu.cn/f/7b2887a8fe3f472c8566/?dl=1) on your own data!  Here we give a template in-the-wild [data](https://cloud.tsinghua.edu.cn/f/48bd4b3e88f64ed7b76b/?dl=1) and [pickle file](https://cloud.tsinghua.edu.cn/f/5c710efd78854c529705/?dl=1). You should place it in ./data and change the corresponding infos. Specifically, you need to change the 'lidar2img', 'intrinsic' and 'data_path' as the extrinsic matrix, intrinsic matrix and path of your multi-camera images. Note that the order of frames should be same to their timestamps. 'occ_path' in this pickle file indicates the save path and you will get raw results (.npy) and point coulds (.ply) in './visual_dir' for further visualization. You can use meshlab to directly visualize .ply files. Or you can run tools/visual.py to visualize .npy files. 
-```
-./tools/dist_inference.sh ./projects/configs/surroundocc/surroundocc_inference.py ./path/to/ckpts.pth 8
-```
-
-### Ground truth generation
-You can also generate dense occupancy labels with your own data! We provide a highly extensible code to achieve [this](https://github.com/weiyithu/SurroundOcc/blob/main/tools/generate_occupancy_with_own_data/process_your_own_data.py). We provide an example [sequence](https://cloud.tsinghua.edu.cn/f/94fea6c8be4448168667/?dl=1) and you need to prepare your data like this:
-
-```
-your_own_data_folder/
-├── pc/
-│   ├── pc0.npy
-│   ├── pc1.npy
-│   ├── ...
-├── bbox/
-│   ├── bbox0.npy (bounding box of the object)
-│   ├── bbox1.npy
-│   ├── ...
-│   ├── object_category0.npy (semantic category of the object)
-│   ├── object_category1.npy
-│   ├── ...
-│   ├── boxes_token0.npy (Unique bbox codes used to combine the same object in different frames)
-│   ├── boxes_token1.npy
-│   ├── ...
-├── calib/
-│   ├── lidar_calibrated_sensor0.npy
-│   ├── lidar_calibrated_sensor1.npy
-│   ├── ...
-├── pose/
-│   ├── lidar_ego_pose0.npy
-│   ├── lidar_ego_pose1.npy
-│   ├── ...
-```
-You can generate occupancy labels with or without semantics (via acitivating --with semantic). If your LiDAR is high-resolution, e.g. RS128, LiVOX and M1, you can skip Poisson reconstruction step and the generation processe will be very fast! You can change the point cloud range and voxel size in config.yaml. You can use multithreading to boost the generation process.
-```
-cd $Home/tools/generate_occupancy_nuscenes
-python process_your_own_data.py --to_mesh --with_semantic --data_path $your_own_data_folder$ --len_sequence $frame number$
-```
-You can use --whole_scene_to_mesh to generate a complete static scene with all frames at one time, then add the moving object point cloud, and finally divide it into small scenes. In this way, we can accelerate the generation process and get denser but more uneven occupancy labels. 
-
-## Acknowledgement
-Many thanks to these excellent projects:
-- [BEVFormer](https://github.com/fundamentalvision/BEVFormer)
-- [MonoScene](https://github.com/astra-vision/MonoScene)
-
-Related Projects:
-- [TPVFormer](https://github.com/wzzheng/TPVFormer)
-- [OpenOccupancy](https://github.com/JeffWang987/OpenOccupancy)
-
-## Bibtex
-If this work is helpful for your research, please consider citing the following BibTeX entry.
-
-```
-@article{wei2023surroundocc, 
-      title={SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving}, 
-      author={Yi Wei and Linqing Zhao and Wenzhao Zheng and Zheng Zhu and Jie Zhou and Jiwen Lu},
-      journal={arXiv preprint arXiv:2303.09551},
-      year={2023}
-}
-```
-
diff --git a/assets/bar.jpg b/assets/bar.jpg
deleted file mode 100644
index 6b503b3..0000000
Binary files a/assets/bar.jpg and /dev/null differ
diff --git a/assets/demo1.gif b/assets/demo1.gif
deleted file mode 100644
index e960166..0000000
Binary files a/assets/demo1.gif and /dev/null differ
diff --git a/assets/demo2.gif b/assets/demo2.gif
deleted file mode 100644
index de40a01..0000000
Binary files a/assets/demo2.gif and /dev/null differ
diff --git a/assets/demo3.gif b/assets/demo3.gif
deleted file mode 100644
index e1d45b8..0000000
Binary files a/assets/demo3.gif and /dev/null differ
diff --git a/assets/demo4.gif b/assets/demo4.gif
deleted file mode 100644
index d58d29d..0000000
Binary files a/assets/demo4.gif and /dev/null differ
diff --git a/assets/groundtruth_pipeline.jpg b/assets/groundtruth_pipeline.jpg
deleted file mode 100644
index ba6b8db..0000000
Binary files a/assets/groundtruth_pipeline.jpg and /dev/null differ
diff --git a/assets/pipeline.jpg b/assets/pipeline.jpg
deleted file mode 100644
index 032221e..0000000
Binary files a/assets/pipeline.jpg and /dev/null differ
diff --git a/extensions/chamfer_dist/__init__.py b/extensions/chamfer_dist/__init__.py
index 8b4f53c..87b8fef 100644
--- a/extensions/chamfer_dist/__init__.py
+++ b/extensions/chamfer_dist/__init__.py
@@ -28,6 +28,7 @@ class ChamferFunction(torch.autograd.Function):
 class ChamferDistanceL2(torch.nn.Module):
     f''' Chamder Distance L2
     '''
+
     def __init__(self, ignore_zeros=False):
         super().__init__()
         self.ignore_zeros = ignore_zeros
@@ -43,9 +44,11 @@ class ChamferDistanceL2(torch.nn.Module):
         dist1, dist2 = ChamferFunction.apply(xyz1, xyz2)
         return torch.mean(dist1) + torch.mean(dist2)
 
+
 class ChamferDistanceL2_split(torch.nn.Module):
     f''' Chamder Distance L2
     '''
+
     def __init__(self, ignore_zeros=False):
         super().__init__()
         self.ignore_zeros = ignore_zeros
@@ -61,9 +64,11 @@ class ChamferDistanceL2_split(torch.nn.Module):
         dist1, dist2 = ChamferFunction.apply(xyz1, xyz2)
         return torch.mean(dist1), torch.mean(dist2)
 
+
 class ChamferDistanceL1(torch.nn.Module):
     f''' Chamder Distance L1
     '''
+
     def __init__(self, ignore_zeros=False):
         super().__init__()
         self.ignore_zeros = ignore_zeros
@@ -77,9 +82,7 @@ class ChamferDistanceL1(torch.nn.Module):
             xyz2 = xyz2[non_zeros2].unsqueeze(dim=0)
 
         dist1, dist2 = ChamferFunction.apply(xyz1, xyz2)
-        # import pdb
-        # pdb.set_trace()
         dist1 = torch.sqrt(dist1)
         dist2 = torch.sqrt(dist2)
-        return (torch.mean(dist1) + torch.mean(dist2))/2
+        return (torch.mean(dist1) + torch.mean(dist2)) / 2
 
diff --git a/extensions/chamfer_dist/chamfer_cuda.cpp b/extensions/chamfer_dist/chamfer_cuda.cpp
index 9fca161..b3bb729 100644
--- a/extensions/chamfer_dist/chamfer_cuda.cpp
+++ b/extensions/chamfer_dist/chamfer_cuda.cpp
@@ -21,7 +21,7 @@ std::vector<torch::Tensor> chamfer_cuda_backward(torch::Tensor xyz1,
 
 std::vector<torch::Tensor> chamfer_forward(torch::Tensor xyz1,
                                            torch::Tensor xyz2) {
-  return chamfer_cuda_forward(xyz1, xyz2);
+    return chamfer_cuda_forward(xyz1, xyz2);
 }
 
 std::vector<torch::Tensor> chamfer_backward(torch::Tensor xyz1,
@@ -30,10 +30,10 @@ std::vector<torch::Tensor> chamfer_backward(torch::Tensor xyz1,
                                             torch::Tensor idx2,
                                             torch::Tensor grad_dist1,
                                             torch::Tensor grad_dist2) {
-  return chamfer_cuda_backward(xyz1, xyz2, idx1, idx2, grad_dist1, grad_dist2);
+    return chamfer_cuda_backward(xyz1, xyz2, idx1, idx2, grad_dist1, grad_dist2);
 }
 
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("forward", &chamfer_forward, "Chamfer forward (CUDA)");
-  m.def("backward", &chamfer_backward, "Chamfer backward (CUDA)");
+    m.def("forward", &chamfer_forward, "Chamfer forward (CUDA)");
+    m.def("backward", &chamfer_backward, "Chamfer backward (CUDA)");
 }
diff --git a/extensions/chamfer_dist/test.py b/extensions/chamfer_dist/test.py
index 0ece5d2..0ef2499 100644
--- a/extensions/chamfer_dist/test.py
+++ b/extensions/chamfer_dist/test.py
@@ -31,8 +31,5 @@ class ChamferDistanceTestCase(unittest.TestCase):
 
 
 if __name__ == '__main__':
-    # unittest.main()
-    import pdb
-    x = torch.rand(32,128,3)
-    y = torch.rand(32,128,3)
-    pdb.set_trace()
+    x = torch.rand(32, 128, 3)
+    y = torch.rand(32, 128, 3)
diff --git a/patch/mmcv/distributed.py b/patch/mmcv/distributed.py
new file mode 100644
index 0000000..20a44be
--- /dev/null
+++ b/patch/mmcv/distributed.py
@@ -0,0 +1,165 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+from typing import Any, List, Tuple
+
+import torch
+from torch.nn.parallel.distributed import (DistributedDataParallel,
+                                           _find_tensors)
+
+from mmcv import print_log
+from mmcv.utils import TORCH_VERSION, digit_version
+from .scatter_gather import ScatterInputs, scatter_kwargs
+
+
+class MMDistributedDataParallel(DistributedDataParallel):
+    """The DDP module that supports DataContainer.
+
+    MMDDP has two main differences with PyTorch DDP:
+
+    - It supports a custom type :class:`DataContainer` which allows more
+      flexible control of input data.
+    - It implement two APIs ``train_step()`` and ``val_step()``.
+    """
+
+    def to_kwargs(self, inputs: ScatterInputs, kwargs: ScatterInputs,
+                  device_id: int) -> Tuple[tuple, tuple]:
+        # Use `self.to_kwargs` instead of `self.scatter` in pytorch1.8
+        # to move all tensors to device_id
+        return scatter_kwargs(inputs, kwargs, [device_id], dim=self.dim)
+
+    def scatter(self, inputs: ScatterInputs, kwargs: ScatterInputs,
+                device_ids: List[int]) -> Tuple[tuple, tuple]:
+        return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)
+
+    def train_step(self, *inputs, **kwargs):
+        """train_step() API for module wrapped by DistributedDataParallel.
+
+        This method is basically the same as
+        ``DistributedDataParallel.forward()``, while replacing
+        ``self.module.forward()`` with ``self.module.train_step()``.
+        It is compatible with PyTorch 1.1 - 1.5.
+        """
+
+        # In PyTorch >= 1.7, ``reducer._rebuild_buckets()`` is moved from the
+        # end of backward to the beginning of forward.
+        if ('parrots' not in TORCH_VERSION
+                and digit_version(TORCH_VERSION) >= digit_version('1.7')
+                and self.reducer._rebuild_buckets()):
+            print_log(
+                'Reducer buckets have been rebuilt in this iteration.',
+                logger='mmcv')
+
+        if ('parrots' not in TORCH_VERSION
+                and digit_version(TORCH_VERSION) >= digit_version('1.11.0a0')):
+            if self._check_sync_bufs_pre_fwd():
+                self._sync_buffers()
+        else:
+            if (getattr(self, 'require_forward_param_sync', False)
+                    and self.require_forward_param_sync):
+                self._sync_params()
+
+        if self.device_ids:
+            inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)
+            if len(self.device_ids) == 1:
+                output = self.module.train_step(*inputs[0], **kwargs[0])
+            else:
+                outputs = self.parallel_apply(
+                    self._module_copies[:len(inputs)], inputs, kwargs)
+                output = self.gather(outputs, self.output_device)
+        else:
+            output = self.module.train_step(*inputs, **kwargs)
+
+        if ('parrots' not in TORCH_VERSION
+                and digit_version(TORCH_VERSION) >= digit_version('1.11.0a0')):
+            if self._check_sync_bufs_post_fwd():
+                self._sync_buffers()
+
+        if (torch.is_grad_enabled()
+                and getattr(self, 'require_backward_grad_sync', False)
+                and self.require_backward_grad_sync):
+            if self.find_unused_parameters:
+                self.reducer.prepare_for_backward(list(_find_tensors(output)))
+            else:
+                self.reducer.prepare_for_backward([])
+        else:
+            if ('parrots' not in TORCH_VERSION
+                    and digit_version(TORCH_VERSION) > digit_version('1.2')):
+                self.require_forward_param_sync = False
+        return output
+
+    def val_step(self, *inputs, **kwargs):
+        """val_step() API for module wrapped by DistributedDataParallel.
+
+        This method is basically the same as
+        ``DistributedDataParallel.forward()``, while replacing
+        ``self.module.forward()`` with ``self.module.val_step()``.
+        It is compatible with PyTorch 1.1 - 1.5.
+        """
+        # In PyTorch >= 1.7, ``reducer._rebuild_buckets()`` is moved from the
+        # end of backward to the beginning of forward.
+        if ('parrots' not in TORCH_VERSION
+                and digit_version(TORCH_VERSION) >= digit_version('1.7')
+                and self.reducer._rebuild_buckets()):
+            print_log(
+                'Reducer buckets have been rebuilt in this iteration.',
+                logger='mmcv')
+
+        if ('parrots' not in TORCH_VERSION
+                and digit_version(TORCH_VERSION) >= digit_version('1.11.0a0')):
+            if self._check_sync_bufs_pre_fwd():
+                self._sync_buffers()
+        else:
+            if (getattr(self, 'require_forward_param_sync', False)
+                    and self.require_forward_param_sync):
+                self._sync_params()
+
+        if self.device_ids:
+            inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)
+            if len(self.device_ids) == 1:
+                output = self.module.val_step(*inputs[0], **kwargs[0])
+            else:
+                outputs = self.parallel_apply(
+                    self._module_copies[:len(inputs)], inputs, kwargs)
+                output = self.gather(outputs, self.output_device)
+        else:
+            output = self.module.val_step(*inputs, **kwargs)
+
+        if ('parrots' not in TORCH_VERSION
+                and digit_version(TORCH_VERSION) >= digit_version('1.11.0a0')):
+            if self._check_sync_bufs_post_fwd():
+                self._sync_buffers()
+
+        if (torch.is_grad_enabled()
+                and getattr(self, 'require_backward_grad_sync', False)
+                and self.require_backward_grad_sync):
+            if self.find_unused_parameters:
+                self.reducer.prepare_for_backward(list(_find_tensors(output)))
+            else:
+                self.reducer.prepare_for_backward([])
+        else:
+            if ('parrots' not in TORCH_VERSION
+                    and digit_version(TORCH_VERSION) > digit_version('1.2')):
+                self.require_forward_param_sync = False
+        return output
+
+    def _run_ddp_forward(self, *inputs, **kwargs) -> Any:
+        """Processes inputs and runs ``self.module.forward``.
+
+        Pytorch 1.12.0 performs ``self.module.forward`` in ``_run_ddp_forward``
+        and deprecates using ``DistributedDataParallel.to_kwargs`` to
+        process inputs, which leads to inputs cannot be processed by
+        :meth:`MMDistributedDataParallel.to_kwargs` anymore. Therefore,
+        ``MMDistributedDataParallel`` overrides this method to call
+        :meth:`to_kwargs` explicitly.
+        
+        Returns:
+            Any: Forward result of :attr:`module`.
+        """
+        module_to_run = self.module
+
+        if self.device_ids:
+            inputs, kwargs = self.to_kwargs(  # type: ignore
+                inputs, kwargs, self.device_ids[0])
+            return module_to_run(*inputs[0], **kwargs[0])  # type: ignore
+        else:
+            return module_to_run(*inputs, **kwargs)
diff --git a/patch/mmcv/epoch_based_runner.py b/patch/mmcv/epoch_based_runner.py
new file mode 100644
index 0000000..0610ba3
--- /dev/null
+++ b/patch/mmcv/epoch_based_runner.py
@@ -0,0 +1,235 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Copyright (c) OpenMMLab. All rights reserved.
+import os
+import os.path as osp
+import platform
+import shutil
+import time
+import warnings
+from typing import Any, Dict, List, Optional, Tuple
+
+import torch
+from torch.utils.data import DataLoader
+
+import mmcv
+from .base_runner import BaseRunner
+from .builder import RUNNERS
+from .checkpoint import save_checkpoint
+from .utils import get_host_info
+
+try:
+    from torch_npu.utils.profiler import Profile
+except ImportError:
+    print("Profile not in torch_npu.utils.profiler now.. Auto Profile disabled.", flush=True)
+
+    class Profile:
+        def __init__(self, *args, **kwargs):
+            pass
+
+        def start(self):
+            pass
+
+        def end(self):
+            pass
+
+
+@RUNNERS.register_module()
+class EpochBasedRunner(BaseRunner):
+    """Epoch-based Runner.
+
+    This runner train models epoch by epoch.
+    """
+
+    def run_iter(self, data_batch: Any, train_mode: bool, **kwargs) -> None:
+        if self.batch_processor is not None:
+            outputs = self.batch_processor(
+                self.model, data_batch, train_mode=train_mode, **kwargs)
+        elif train_mode:
+            outputs = self.model.train_step(data_batch, self.optimizer,
+                                            **kwargs)
+        else:
+            outputs = self.model.val_step(data_batch, self.optimizer, **kwargs)
+        if not isinstance(outputs, dict):
+            raise TypeError('"batch_processor()" or "model.train_step()"'
+                            'and "model.val_step()" must return a dict')
+        if 'log_vars' in outputs:
+            self.log_buffer.update(outputs['log_vars'], outputs['num_samples'])
+        self.outputs = outputs
+
+    def train(self, data_loader, **kwargs):
+        self.model.train()
+        self.mode = 'train'
+        self.data_loader = data_loader
+        self._max_iters = self._max_epochs * len(self.data_loader)
+        self.call_hook('before_train_epoch')
+        time.sleep(2)  # Prevent possible deadlock during epoch transition
+        profile = Profile(start_step=int(os.getenv('PROFILE_START_STEP', 10)),
+                          profile_type=os.getenv('PROFILE_TYPE'))
+        for i, data_batch in enumerate(self.data_loader):
+            profile.start()
+            self.data_batch = data_batch
+            self._inner_iter = i
+            self.call_hook('before_train_iter')
+            self.run_iter(data_batch, train_mode=True, **kwargs)
+            self.call_hook('after_train_iter')
+            del self.data_batch
+            self._iter += 1
+            profile.end()
+
+        self.call_hook('after_train_epoch')
+        self._epoch += 1
+
+    @torch.no_grad()
+    def val(self, data_loader, **kwargs):
+        self.model.eval()
+        self.mode = 'val'
+        self.data_loader = data_loader
+        self.call_hook('before_val_epoch')
+        time.sleep(2)  # Prevent possible deadlock during epoch transition
+        for i, data_batch in enumerate(self.data_loader):
+            self.data_batch = data_batch
+            self._inner_iter = i
+            self.call_hook('before_val_iter')
+            self.run_iter(data_batch, train_mode=False)
+            self.call_hook('after_val_iter')
+            del self.data_batch
+        self.call_hook('after_val_epoch')
+
+    def run(self,
+            data_loaders: List[DataLoader],
+            workflow: List[Tuple[str, int]],
+            max_epochs: Optional[int] = None,
+            **kwargs) -> None:
+        """Start running.
+
+        Args:
+            data_loaders (list[:obj:`DataLoader`]): Dataloaders for training
+                and validation.
+            workflow (list[tuple]): A list of (phase, epochs) to specify the
+                running order and epochs. E.g, [('train', 2), ('val', 1)] means
+                running 2 epochs for training and 1 epoch for validation,
+                iteratively.
+        """
+        assert isinstance(data_loaders, list)
+        assert mmcv.is_list_of(workflow, tuple)
+        assert len(data_loaders) == len(workflow)
+        if max_epochs is not None:
+            warnings.warn(
+                'setting max_epochs in run is deprecated, '
+                'please set max_epochs in runner_config', DeprecationWarning)
+            self._max_epochs = max_epochs
+
+        assert self._max_epochs is not None, (
+            'max_epochs must be specified during instantiation')
+
+        for i, flow in enumerate(workflow):
+            mode, epochs = flow
+            if mode == 'train':
+                self._max_iters = self._max_epochs * len(data_loaders[i])
+                break
+
+        work_dir = self.work_dir if self.work_dir is not None else 'NONE'
+        self.logger.info('Start running, host: %s, work_dir: %s',
+                         get_host_info(), work_dir)
+        self.logger.info('Hooks will be executed in the following order:\n%s',
+                         self.get_hook_info())
+        self.logger.info('workflow: %s, max: %d epochs', workflow,
+                         self._max_epochs)
+        self.call_hook('before_run')
+
+        while self.epoch < self._max_epochs:
+            for i, flow in enumerate(workflow):
+                mode, epochs = flow
+                if isinstance(mode, str):  # self.train()
+                    if not hasattr(self, mode):
+                        raise ValueError(
+                            f'runner has no method named "{mode}" to run an '
+                            'epoch')
+                    epoch_runner = getattr(self, mode)
+                else:
+                    raise TypeError(
+                        'mode in workflow must be a str, but got {}'.format(
+                            type(mode)))
+
+                for _ in range(epochs):
+                    if mode == 'train' and self.epoch >= self._max_epochs:
+                        break
+                    epoch_runner(data_loaders[i], **kwargs)
+
+        time.sleep(1)  # wait for some hooks like loggers to finish
+        self.call_hook('after_run')
+
+    def save_checkpoint(self,
+                        out_dir: str,
+                        filename_tmpl: str = 'epoch_{}.pth',
+                        save_optimizer: bool = True,
+                        meta: Optional[Dict] = None,
+                        create_symlink: bool = True) -> None:
+        """Save the checkpoint.
+
+        Args:
+            out_dir (str): The directory that checkpoints are saved.
+            filename_tmpl (str, optional): The checkpoint filename template,
+                which contains a placeholder for the epoch number.
+                Defaults to 'epoch_{}.pth'.
+            save_optimizer (bool, optional): Whether to save the optimizer to
+                the checkpoint. Defaults to True.
+            meta (dict, optional): The meta information to be saved in the
+                checkpoint. Defaults to None.
+            create_symlink (bool, optional): Whether to create a symlink
+                "latest.pth" to point to the latest checkpoint.
+                Defaults to True.
+        """
+        if meta is None:
+            meta = {}
+        elif not isinstance(meta, dict):
+            raise TypeError(
+                f'meta should be a dict or None, but got {type(meta)}')
+        if self.meta is not None:
+            meta.update(self.meta)
+            # Note: meta.update(self.meta) should be done before
+            # meta.update(epoch=self.epoch + 1, iter=self.iter) otherwise
+            # there will be problems with resumed checkpoints.
+            # More details in https://github.com/open-mmlab/mmcv/pull/1108
+        meta.update(epoch=self.epoch + 1, iter=self.iter)
+
+        filename = filename_tmpl.format(self.epoch + 1)
+        filepath = osp.join(out_dir, filename)
+        optimizer = self.optimizer if save_optimizer else None
+        save_checkpoint(self.model, filepath, optimizer=optimizer, meta=meta)
+        # in some environments, `os.symlink` is not supported, you may need to
+        # set `create_symlink` to False
+        if create_symlink:
+            dst_file = osp.join(out_dir, 'latest.pth')
+            if not osp.exists(dist_file):
+                raise FileNotFoundError(f"{dist_file} not exists!")
+            if not osp.exists(filepath):
+                raise FileNotFoundError(f"{filepath} not exists!")
+            if platform.system() != 'Windows':
+                mmcv.symlink(filename, dst_file)
+            else:
+                shutil.copy(filepath, dst_file)
+
+
+@RUNNERS.register_module()
+class Runner(EpochBasedRunner):
+    """Deprecated name of EpochBasedRunner."""
+
+    def __init__(self, *args, **kwargs):
+        warnings.warn(
+            'Runner was deprecated, please use EpochBasedRunner instead',
+            DeprecationWarning)
+        super().__init__(*args, **kwargs)
\ No newline at end of file
diff --git a/patch/mmcv/modulated_deform_conv.py b/patch/mmcv/modulated_deform_conv.py
new file mode 100644
index 0000000..6667f55
--- /dev/null
+++ b/patch/mmcv/modulated_deform_conv.py
@@ -0,0 +1,154 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+import math
+from typing import Optional, Tuple, Union
+
+import torch
+import torch_npu
+import torch.nn as nn
+from torch.nn.modules.utils import _pair, _single
+from mmcv.utils import deprecated_api_warning
+from mx_driving.fused import modulated_deform_conv2d, ModulatedDeformConv2dFunction
+
+from ..cnn import CONV_LAYERS
+from ..utils import print_log
+
+
+class ModulatedDeformConv2d(nn.Module):
+
+    @deprecated_api_warning({"deformable_groups": "deform_groups"}, cls_name="ModulatedDeformConv2d")
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Union[int, Tuple[int]],
+        stride: int = 1,
+        padding: int = 0,
+        dilation: int = 1,
+        groups: int = 1,
+        deform_groups: int = 1,
+        bias: Union[bool, str] = True,
+    ):
+        super().__init__()
+        self.in_channels = in_channels
+        self.out_channels = out_channels
+        self.kernel_size = _pair(kernel_size)
+        self.stride = _pair(stride)
+        self.padding = _pair(padding)
+        self.dilation = _pair(dilation)
+        self.groups = groups
+        self.deform_groups = deform_groups
+        # enable compatibility with nn.Conv2d
+        self.transposed = False
+        self.output_padding = _single(0)
+
+        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size))
+        if bias:
+            self.bias = nn.Parameter(torch.Tensor(out_channels))
+        else:
+            self.register_parameter("bias", None)
+        self.init_weights()
+
+    def init_weights(self):
+        n = self.in_channels
+        for k in self.kernel_size:
+            n *= k
+        stdv = 1.0 / math.sqrt(n)
+        self.weight.data.uniform_(-stdv, stdv)
+        if self.bias is not None:
+            self.bias.data.zero_()
+
+    def forward(self, x: torch.Tensor, offset: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
+        return modulated_deform_conv2d(
+            x,
+            offset,
+            mask,
+            self.weight,
+            self.bias,
+            self.stride,
+            self.padding,
+            self.dilation,
+            self.groups,
+            self.deform_groups,
+        )
+
+
+@CONV_LAYERS.register_module("DCNv2")
+class ModulatedDeformConv2dPack(ModulatedDeformConv2d):
+    """A ModulatedDeformable Conv Encapsulation that acts as normal Conv
+    layers.
+
+    Args:
+        in_channels (int): Same as nn.Conv2d.
+        out_channels (int): Same as nn.Conv2d.
+        kernel_size (int or tuple[int]): Same as nn.Conv2d.
+        stride (int): Same as nn.Conv2d, while tuple is not supported.
+        padding (int): Same as nn.Conv2d, while tuple is not supported.
+        dilation (int): Same as nn.Conv2d, while tuple is not supported.
+        groups (int): Same as nn.Conv2d.
+        bias (bool or str): If specified as `auto`, it will be decided by the
+            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise
+            False.
+    """
+
+    _version = 2
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.conv_offset = nn.Conv2d(
+            self.in_channels,
+            self.deform_groups * 3 * self.kernel_size[0] * self.kernel_size[1],
+            kernel_size=self.kernel_size,
+            stride=self.stride,
+            padding=self.padding,
+            dilation=self.dilation,
+            bias=True,
+        )
+        self.init_weights()
+
+    def init_weights(self) -> None:
+        super().init_weights()
+        if hasattr(self, "conv_offset"):
+            self.conv_offset.weight.data.zero_()
+            self.conv_offset.bias.data.zero_()
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore
+        out = self.conv_offset(x)
+        len1 = ((out.shape[1] + 2) // 3) * 2
+        len2 = out.shape[1] - len1
+        offset, mask = torch.split(out, [len1, len2], dim=1)
+        mask = torch.sigmoid(mask)
+        return modulated_deform_conv2d(
+            x,
+            offset,
+            mask,
+            self.weight,
+            self.bias,
+            self.stride,
+            self.padding,
+            self.dilation,
+            self.groups,
+            self.deform_groups,
+        )
+
+    # pylint: disable=huawei-too-many-arguments
+    def _load_from_state_dict(
+        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs
+    ):
+        version = local_metadata.get("version", None)
+
+        if version is None or version < 2:
+            # the key is different in early versions
+            # In version < 2, ModulatedDeformConvPack
+            # loads previous benchmark models.
+            if prefix + "conv_offset.weight" not in state_dict and prefix[:-1] + "_offset.weight" in state_dict:
+                state_dict[prefix + "conv_offset.weight"] = state_dict.pop(prefix[:-1] + "_offset.weight")
+            if prefix + "conv_offset.bias" not in state_dict and prefix[:-1] + "_offset.bias" in state_dict:
+                state_dict[prefix + "conv_offset.bias"] = state_dict.pop(prefix[:-1] + "_offset.bias")
+
+        if version is not None and version > 1:
+            print_log(f'ModulatedDeformConvPack {prefix.rstrip(".")} is upgraded to ' "version 2.", logger="root")
+
+        super()._load_from_state_dict(
+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs
+        )
diff --git a/patch/mmcv/optimizer.py b/patch/mmcv/optimizer.py
new file mode 100644
index 0000000..e3bb975
--- /dev/null
+++ b/patch/mmcv/optimizer.py
@@ -0,0 +1,574 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Copyright (c) OpenMMLab. All rights reserved.
+import copy
+import logging
+from collections import defaultdict
+from itertools import chain
+from typing import Optional, Union
+
+import torch.nn as nn
+from torch import Tensor
+from torch.nn.utils import clip_grad
+
+from mmcv.utils import (IS_NPU_AVAILABLE, TORCH_VERSION, _BatchNorm,
+                        digit_version)
+from ..dist_utils import allreduce_grads
+from ..fp16_utils import LossScaler, wrap_fp16_model
+from .hook import HOOKS, Hook
+
+try:
+    # If PyTorch version >= 1.6.0, torch.cuda.amp.GradScaler would be imported
+    # and used; otherwise, auto fp16 will adopt mmcv's implementation.
+    if IS_NPU_AVAILABLE:
+        from torch.npu.amp import GradScaler
+    else:
+        from torch.cuda.amp import GradScaler
+except ImportError:
+    pass
+
+
+@HOOKS.register_module()
+class OptimizerHook(Hook):
+    """A hook contains custom operations for the optimizer.
+
+    Args:
+        grad_clip (dict, optional): A config dict to control the clip_grad.
+            Default: None.
+        detect_anomalous_params (bool): This option is only used for
+            debugging which will slow down the training speed.
+            Detect anomalous parameters that are not included in
+            the computational graph with `loss` as the root.
+            There are two cases
+
+                - Parameters were not used during
+                  forward pass.
+                - Parameters were not used to produce
+                  loss.
+            Default: False.
+    """
+
+    def __init__(self,
+                 grad_clip: Optional[dict] = None,
+                 detect_anomalous_params: bool = False):
+        self.grad_clip = grad_clip
+        self.detect_anomalous_params = detect_anomalous_params
+
+    def clip_grads(self, params, runner):
+        params = list(
+            filter(lambda p: p.requires_grad and p.grad is not None, params))
+        if len(params) > 0:
+            return runner.optimizer.clip_grad_norm_fused_(**self.grad_clip)
+
+    def after_train_iter(self, runner):
+        runner.optimizer.zero_grad()
+        if self.detect_anomalous_params:
+            self.detect_anomalous_parameters(runner.outputs['loss'], runner)
+        runner.outputs['loss'].backward()
+
+        if self.grad_clip is not None:
+            grad_norm = self.clip_grads(runner.model.parameters(), runner)
+            if grad_norm is not None:
+                # Add grad norm to the logger
+                runner.log_buffer.update({'grad_norm': float(grad_norm)},
+                                         runner.outputs['num_samples'])
+        runner.optimizer.step()
+
+    def detect_anomalous_parameters(self, loss: Tensor, runner) -> None:
+        logger = runner.logger
+        parameters_in_graph = set()
+        visited = set()
+
+        def traverse(grad_fn):
+            if grad_fn is None:
+                return
+            if grad_fn not in visited:
+                visited.add(grad_fn)
+                if hasattr(grad_fn, 'variable'):
+                    parameters_in_graph.add(grad_fn.variable)
+                parents = grad_fn.next_functions
+                if parents is not None:
+                    for parent in parents:
+                        grad_fn = parent[0]
+                        traverse(grad_fn)
+
+        traverse(loss.grad_fn)
+        for n, p in runner.model.named_parameters():
+            if p not in parameters_in_graph and p.requires_grad:
+                logger.log(
+                    level=logging.ERROR,
+                    msg=f'{n} with shape {p.size()} is not '
+                    f'in the computational graph \n')
+
+
+@HOOKS.register_module()
+class GradientCumulativeOptimizerHook(OptimizerHook):
+    """Optimizer Hook implements multi-iters gradient cumulating.
+
+    Args:
+        cumulative_iters (int, optional): Num of gradient cumulative iters.
+            The optimizer will step every `cumulative_iters` iters.
+            Defaults to 1.
+
+    Examples:
+        >>> # Use cumulative_iters to simulate a large batch size
+        >>> # It is helpful when the hardware cannot handle a large batch size.
+        >>> loader = DataLoader(data, batch_size=64)
+        >>> optim_hook = GradientCumulativeOptimizerHook(cumulative_iters=4)
+        >>> # almost equals to
+        >>> loader = DataLoader(data, batch_size=256)
+        >>> optim_hook = OptimizerHook()
+    """
+
+    def __init__(self, cumulative_iters: int = 1, **kwargs):
+        super().__init__(**kwargs)
+
+        assert isinstance(cumulative_iters, int) and cumulative_iters > 0, \
+            f'cumulative_iters only accepts positive int, but got ' \
+            f'{type(cumulative_iters)} instead.'
+
+        self.cumulative_iters = cumulative_iters
+        self.divisible_iters = 0
+        self.remainder_iters = 0
+        self.initialized = False
+
+    def has_batch_norm(self, module: nn.Module) -> bool:
+        if isinstance(module, _BatchNorm):
+            return True
+        for m in module.children():
+            if self.has_batch_norm(m):
+                return True
+        return False
+
+    def _init(self, runner):
+        if runner.iter % self.cumulative_iters != 0:
+            runner.logger.warning(
+                'Resume iter number is not divisible by cumulative_iters in '
+                'GradientCumulativeOptimizerHook, which means the gradient of '
+                'some iters is lost and the result may be influenced slightly.'
+            )
+
+        if self.has_batch_norm(runner.model) and self.cumulative_iters > 1:
+            runner.logger.warning(
+                'GradientCumulativeOptimizerHook may slightly decrease '
+                'performance if the model has BatchNorm layers.')
+
+        self.divisible_iters = (
+            runner.max_iters // self.cumulative_iters * self.cumulative_iters)
+        self.remainder_iters = runner.max_iters - self.divisible_iters
+
+        self.initialized = True
+
+    def _get_loss_factor(self, runner):
+        """Get loss division factor for the current iteration."""
+        if runner.iter < runner.max_iters - self.remainder_iters:
+            loss_factor = self.cumulative_iters
+        else:
+            loss_factor = self.remainder_iters
+            runner.logger.warning(
+                f'Loss will be divided by {loss_factor} in the last '
+                f'{self.remainder_iters} iterations because they are not '
+                f'enough for {self.cumulative_iters} cumulative_iters.')
+            assert loss_factor > 0
+
+        return loss_factor
+
+    def after_train_iter(self, runner):
+        if not self.initialized:
+            self._init(runner)
+
+        loss = runner.outputs['loss'] / self._get_loss_factor(runner)
+        loss.backward()
+
+        if (self.every_n_iters(runner, self.cumulative_iters)
+                or self.is_last_iter(runner)):
+
+            if self.grad_clip is not None:
+                grad_norm = self.clip_grads(runner.model.parameters(), runner)
+                if grad_norm is not None:
+                    # Add grad norm to the logger
+                    runner.log_buffer.update({'grad_norm': float(grad_norm)},
+                                             runner.outputs['num_samples'])
+            runner.optimizer.step()
+            runner.optimizer.zero_grad()
+
+
+if (TORCH_VERSION != 'parrots'
+        and digit_version(TORCH_VERSION) >= digit_version('1.6.0')):
+
+    @HOOKS.register_module()
+    class Fp16OptimizerHook(OptimizerHook):
+        """FP16 optimizer hook (using PyTorch's implementation).
+
+        If you are using PyTorch >= 1.6, torch.cuda.amp is used as the backend,
+        to take care of the optimization procedure.
+
+        Args:
+            loss_scale (float | str | dict): Scale factor configuration.
+                If loss_scale is a float, static loss scaling will be used with
+                the specified scale. If loss_scale is a string, it must be
+                'dynamic', then dynamic loss scaling will be used.
+                It can also be a dict containing arguments of GradScalar.
+                Defaults to 512. For Pytorch >= 1.6, mmcv uses official
+                implementation of GradScaler. If you use a dict version of
+                loss_scale to create GradScaler, please refer to:
+                https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler
+                for the parameters.
+
+        Examples:
+            >>> loss_scale = dict(
+            ...     init_scale=65536.0,
+            ...     growth_factor=2.0,
+            ...     backoff_factor=0.5,
+            ...     growth_interval=2000
+            ... )
+            >>> optimizer_hook = Fp16OptimizerHook(loss_scale=loss_scale)
+        """
+
+        def __init__(self,
+                     grad_clip: Optional[dict] = None,
+                     coalesce: bool = True,
+                     bucket_size_mb: int = -1,
+                     loss_scale: Union[float, str, dict] = 512.,
+                     distributed: bool = True):
+            self.grad_clip = grad_clip
+            self.coalesce = coalesce
+            self.bucket_size_mb = bucket_size_mb
+            self.distributed = distributed
+            self._scale_update_param = None
+            if loss_scale == 'dynamic':
+                self.loss_scaler = GradScaler()
+            elif isinstance(loss_scale, float):
+                self._scale_update_param = loss_scale
+                self.loss_scaler = GradScaler(init_scale=loss_scale)
+            elif isinstance(loss_scale, dict):
+                self.loss_scaler = GradScaler(**loss_scale)
+            else:
+                raise ValueError('loss_scale must be of type float, dict, or '
+                                 f'"dynamic", got {loss_scale}')
+
+        def before_run(self, runner) -> None:
+            """Preparing steps before Mixed Precision Training."""
+            # wrap model mode to fp16
+            wrap_fp16_model(runner.model)
+            # resume from state dict
+            if 'fp16' in runner.meta and 'loss_scaler' in runner.meta['fp16']:
+                scaler_state_dict = runner.meta['fp16']['loss_scaler']
+                self.loss_scaler.load_state_dict(scaler_state_dict)
+
+        def copy_grads_to_fp32(self, fp16_net: nn.Module,
+                               fp32_weights: Tensor) -> None:
+            """Copy gradients from fp16 model to fp32 weight copy."""
+            for fp32_param, fp16_param in zip(fp32_weights,
+                                              fp16_net.parameters()):
+                if fp16_param.grad is not None:
+                    if fp32_param.grad is None:
+                        fp32_param.grad = fp32_param.data.new(
+                            fp32_param.size())
+                    fp32_param.grad.copy_(fp16_param.grad)
+
+        def copy_params_to_fp16(self, fp16_net: nn.Module,
+                                fp32_weights: Tensor) -> None:
+            """Copy updated params from fp32 weight copy to fp16 model."""
+            for fp16_param, fp32_param in zip(fp16_net.parameters(),
+                                              fp32_weights):
+                fp16_param.data.copy_(fp32_param.data)
+
+        def after_train_iter(self, runner) -> None:
+            """Backward optimization steps for Mixed Precision Training. For
+            dynamic loss scaling, please refer to
+            https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.
+
+            1. Scale the loss by a scale factor.
+            2. Backward the loss to obtain the gradients.
+            3. Unscale the optimizer’s gradient tensors.
+            4. Call optimizer.step() and update scale factor.
+            5. Save loss_scaler state_dict for resume purpose.
+            """
+            # clear grads of last iteration
+            runner.model.zero_grad()
+            runner.optimizer.zero_grad()
+
+            self.loss_scaler.scale(runner.outputs['loss']).backward()
+            self.loss_scaler.unscale_(runner.optimizer)
+            # grad clip
+            if self.grad_clip is not None:
+                grad_norm = self.clip_grads(runner.model.parameters(), runner)
+                if grad_norm is not None:
+                    # Add grad norm to the logger
+                    runner.log_buffer.update({'grad_norm': float(grad_norm)},
+                                             runner.outputs['num_samples'])
+            # backward and update scaler
+            self.loss_scaler.step(runner.optimizer)
+            self.loss_scaler.update(self._scale_update_param)
+
+            # save state_dict of loss_scaler
+            runner.meta.setdefault(
+                'fp16', {})['loss_scaler'] = self.loss_scaler.state_dict()
+
+    @HOOKS.register_module()
+    class GradientCumulativeFp16OptimizerHook(GradientCumulativeOptimizerHook,
+                                              Fp16OptimizerHook):
+        """Fp16 optimizer Hook (using PyTorch's implementation) implements
+        multi-iters gradient cumulating.
+
+        If you are using PyTorch >= 1.6, torch.cuda.amp is used as the backend,
+        to take care of the optimization procedure.
+        """
+
+        def __init__(self, *args, **kwargs):
+            super().__init__(*args, **kwargs)
+
+        def after_train_iter(self, runner) -> None:
+            if not self.initialized:
+                self._init(runner)
+
+            loss = runner.outputs['loss'] / self._get_loss_factor(runner)
+            self.loss_scaler.scale(loss).backward()
+
+            if (self.every_n_iters(runner, self.cumulative_iters)
+                    or self.is_last_iter(runner)):
+
+                # copy fp16 grads in the model to fp32 params in the optimizer
+                self.loss_scaler.unscale_(runner.optimizer)
+
+                if self.grad_clip is not None:
+                    grad_norm = self.clip_grads(runner.model.parameters(), runner)
+                    if grad_norm is not None:
+                        # Add grad norm to the logger
+                        runner.log_buffer.update(
+                            {'grad_norm': float(grad_norm)},
+                            runner.outputs['num_samples'])
+
+                # backward and update scaler
+                self.loss_scaler.step(runner.optimizer)
+                self.loss_scaler.update(self._scale_update_param)
+
+                # save state_dict of loss_scaler
+                runner.meta.setdefault(
+                    'fp16', {})['loss_scaler'] = self.loss_scaler.state_dict()
+
+                # clear grads
+                runner.model.zero_grad()
+                runner.optimizer.zero_grad()
+
+else:
+
+    @HOOKS.register_module()
+    class Fp16OptimizerHook(OptimizerHook):  # type: ignore
+        """FP16 optimizer hook (mmcv's implementation).
+
+        The steps of fp16 optimizer is as follows.
+        1. Scale the loss value.
+        2. BP in the fp16 model.
+        2. Copy gradients from fp16 model to fp32 weights.
+        3. Update fp32 weights.
+        4. Copy updated parameters from fp32 weights to fp16 model.
+
+        Refer to https://arxiv.org/abs/1710.03740 for more details.
+
+        Args:
+            loss_scale (float | str | dict): Scale factor configuration.
+                If loss_scale is a float, static loss scaling will be used with
+                the specified scale. If loss_scale is a string, it must be
+                'dynamic', then dynamic loss scaling will be used.
+                It can also be a dict containing arguments of LossScaler.
+                Defaults to 512.
+        """
+
+        def __init__(self,
+                     grad_clip: Optional[dict] = None,
+                     coalesce: bool = True,
+                     bucket_size_mb: int = -1,
+                     loss_scale: Union[float, str, dict] = 512.,
+                     distributed: bool = True):
+            self.grad_clip = grad_clip
+            self.coalesce = coalesce
+            self.bucket_size_mb = bucket_size_mb
+            self.distributed = distributed
+            if loss_scale == 'dynamic':
+                self.loss_scaler = LossScaler(mode='dynamic')
+            elif isinstance(loss_scale, float):
+                self.loss_scaler = LossScaler(
+                    init_scale=loss_scale, mode='static')
+            elif isinstance(loss_scale, dict):
+                self.loss_scaler = LossScaler(**loss_scale)
+            else:
+                raise ValueError('loss_scale must be of type float, dict, or '
+                                 f'"dynamic", got {loss_scale}')
+
+        def before_run(self, runner) -> None:
+            """Preparing steps before Mixed Precision Training.
+
+            1. Make a master copy of fp32 weights for optimization.
+            2. Convert the main model from fp32 to fp16.
+            """
+            # keep a copy of fp32 weights
+            old_groups = runner.optimizer.param_groups
+            runner.optimizer.param_groups = copy.deepcopy(
+                runner.optimizer.param_groups)
+            state: defaultdict = defaultdict(dict)
+            p_map = {
+                old_p: p
+                for old_p, p in zip(
+                    chain(*(g['params'] for g in old_groups)),
+                    chain(*(g['params']
+                            for g in runner.optimizer.param_groups)))
+            }
+            for k, v in runner.optimizer.state.items():
+                state[p_map[k]] = v
+            runner.optimizer.state = state
+            # convert model to fp16
+            wrap_fp16_model(runner.model)
+            # resume from state dict
+            if 'fp16' in runner.meta and 'loss_scaler' in runner.meta['fp16']:
+                scaler_state_dict = runner.meta['fp16']['loss_scaler']
+                self.loss_scaler.load_state_dict(scaler_state_dict)
+
+        def copy_grads_to_fp32(self, fp16_net: nn.Module,
+                               fp32_weights: Tensor) -> None:
+            """Copy gradients from fp16 model to fp32 weight copy."""
+            for fp32_param, fp16_param in zip(fp32_weights,
+                                              fp16_net.parameters()):
+                if fp16_param.grad is not None:
+                    if fp32_param.grad is None:
+                        fp32_param.grad = fp32_param.data.new(
+                            fp32_param.size())
+                    fp32_param.grad.copy_(fp16_param.grad)
+
+        def copy_params_to_fp16(self, fp16_net: nn.Module,
+                                fp32_weights: Tensor) -> None:
+            """Copy updated params from fp32 weight copy to fp16 model."""
+            for fp16_param, fp32_param in zip(fp16_net.parameters(),
+                                              fp32_weights):
+                fp16_param.data.copy_(fp32_param.data)
+
+        def after_train_iter(self, runner) -> None:
+            """Backward optimization steps for Mixed Precision Training. For
+            dynamic loss scaling, please refer `loss_scalar.py`
+
+            1. Scale the loss by a scale factor.
+            2. Backward the loss to obtain the gradients (fp16).
+            3. Copy gradients from the model to the fp32 weight copy.
+            4. Scale the gradients back and update the fp32 weight copy.
+            5. Copy back the params from fp32 weight copy to the fp16 model.
+            6. Save loss_scaler state_dict for resume purpose.
+            """
+            # clear grads of last iteration
+            runner.model.zero_grad()
+            runner.optimizer.zero_grad()
+            # scale the loss value
+            scaled_loss = runner.outputs['loss'] * self.loss_scaler.loss_scale
+            scaled_loss.backward()
+            # copy fp16 grads in the model to fp32 params in the optimizer
+
+            fp32_weights = []
+            for param_group in runner.optimizer.param_groups:
+                fp32_weights += param_group['params']
+            self.copy_grads_to_fp32(runner.model, fp32_weights)
+            # allreduce grads
+            if self.distributed:
+                allreduce_grads(fp32_weights, self.coalesce,
+                                self.bucket_size_mb)
+
+            has_overflow = self.loss_scaler.has_overflow(fp32_weights)
+            # if has overflow, skip this iteration
+            if not has_overflow:
+                # scale the gradients back
+                for param in fp32_weights:
+                    if param.grad is not None:
+                        param.grad.div_(self.loss_scaler.loss_scale)
+                if self.grad_clip is not None:
+                    grad_norm = self.clip_grads(fp32_weights, runner)
+                    if grad_norm is not None:
+                        # Add grad norm to the logger
+                        runner.log_buffer.update(
+                            {'grad_norm': float(grad_norm)},
+                            runner.outputs['num_samples'])
+                # update fp32 params
+                runner.optimizer.step()
+                # copy fp32 params to the fp16 model
+                self.copy_params_to_fp16(runner.model, fp32_weights)
+            self.loss_scaler.update_scale(has_overflow)
+            if has_overflow:
+                runner.logger.warning('Check overflow, downscale loss scale '
+                                      f'to {self.loss_scaler.cur_scale}')
+
+            # save state_dict of loss_scaler
+            runner.meta.setdefault(
+                'fp16', {})['loss_scaler'] = self.loss_scaler.state_dict()
+
+    @HOOKS.register_module()
+    class GradientCumulativeFp16OptimizerHook(  # type: ignore
+            GradientCumulativeOptimizerHook, Fp16OptimizerHook):
+        """Fp16 optimizer Hook (using mmcv implementation) implements multi-
+        iters gradient cumulating."""
+
+        def __init__(self, *args, **kwargs):
+            super().__init__(*args, **kwargs)
+
+        def after_train_iter(self, runner) -> None:
+            if not self.initialized:
+                self._init(runner)
+
+            loss = runner.outputs['loss'] / self._get_loss_factor(runner)
+            scaled_loss = loss * self.loss_scaler.loss_scale
+            scaled_loss.backward()
+
+            if (self.every_n_iters(runner, self.cumulative_iters)
+                    or self.is_last_iter(runner)):
+
+                # copy fp16 grads in the model to fp32 params in the optimizer
+                fp32_weights = []
+                for param_group in runner.optimizer.param_groups:
+                    fp32_weights += param_group['params']
+                self.copy_grads_to_fp32(runner.model, fp32_weights)
+                # allreduce grads
+                if self.distributed:
+                    allreduce_grads(fp32_weights, self.coalesce,
+                                    self.bucket_size_mb)
+
+                has_overflow = self.loss_scaler.has_overflow(fp32_weights)
+                # if has overflow, skip this iteration
+                if not has_overflow:
+                    # scale the gradients back
+                    for param in fp32_weights:
+                        if param.grad is not None:
+                            param.grad.div_(self.loss_scaler.loss_scale)
+                    if self.grad_clip is not None:
+                        grad_norm = self.clip_grads(fp32_weights, runner)
+                        if grad_norm is not None:
+                            # Add grad norm to the logger
+                            runner.log_buffer.update(
+                                {'grad_norm': float(grad_norm)},
+                                runner.outputs['num_samples'])
+                    # update fp32 params
+                    runner.optimizer.step()
+                    # copy fp32 params to the fp16 model
+                    self.copy_params_to_fp16(runner.model, fp32_weights)
+                else:
+                    runner.logger.warning(
+                        'Check overflow, downscale loss scale '
+                        f'to {self.loss_scaler.cur_scale}')
+
+                self.loss_scaler.update_scale(has_overflow)
+
+                # save state_dict of loss_scaler
+                runner.meta.setdefault(
+                    'fp16', {})['loss_scaler'] = self.loss_scaler.state_dict()
+
+                # clear grads
+                runner.model.zero_grad()
+                runner.optimizer.zero_grad()
\ No newline at end of file
diff --git a/patch/mmdet/resnet.py b/patch/mmdet/resnet.py
new file mode 100644
index 0000000..149d2d5
--- /dev/null
+++ b/patch/mmdet/resnet.py
@@ -0,0 +1,674 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+import warnings
+
+import torch.nn as nn
+import torch.utils.checkpoint as cp
+from mmcv.cnn import build_conv_layer, build_norm_layer, build_plugin_layer
+from mmcv.runner import BaseModule
+from torch.nn.modules.batchnorm import _BatchNorm
+import mx_driving.fused
+import torch
+import torch_npu
+
+from ..builder import BACKBONES
+from ..utils import ResLayer
+
+
+class BasicBlock(BaseModule):
+    expansion = 1
+
+    def __init__(self,
+                 inplanes,
+                 planes,
+                 stride=1,
+                 dilation=1,
+                 downsample=None,
+                 style='pytorch',
+                 with_cp=False,
+                 conv_cfg=None,
+                 norm_cfg=dict(type='BN'),
+                 dcn=None,
+                 plugins=None,
+                 init_cfg=None):
+        super(BasicBlock, self).__init__(init_cfg)
+        assert dcn is None, 'Not implemented yet.'
+        assert plugins is None, 'Not implemented yet.'
+
+        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)
+        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)
+
+        self.conv1 = build_conv_layer(
+            conv_cfg,
+            inplanes,
+            planes,
+            3,
+            stride=stride,
+            padding=dilation,
+            dilation=dilation,
+            bias=False)
+        self.add_module(self.norm1_name, norm1)
+        self.conv2 = build_conv_layer(
+            conv_cfg, planes, planes, 3, padding=1, bias=False)
+        self.add_module(self.norm2_name, norm2)
+
+        self.relu = nn.ReLU(inplace=True)
+        self.downsample = downsample
+        self.stride = stride
+        self.dilation = dilation
+        self.with_cp = with_cp
+
+    @property
+    def norm1(self):
+        """nn.Module: normalization layer after the first convolution layer"""
+        return getattr(self, self.norm1_name)
+
+    @property
+    def norm2(self):
+        """nn.Module: normalization layer after the second convolution layer"""
+        return getattr(self, self.norm2_name)
+
+    def forward(self, x):
+        """Forward function."""
+
+        def _inner_forward(x):
+            identity = x
+
+            out = self.conv1(x)
+            out = self.norm1(out)
+            out = self.relu(out)
+
+            out = self.conv2(out)
+            out = self.norm2(out)
+
+            if self.downsample is not None:
+                identity = self.downsample(x)
+
+            out += identity
+
+            return out
+
+        if self.with_cp and x.requires_grad:
+            out = cp.checkpoint(_inner_forward, x)
+        else:
+            out = _inner_forward(x)
+
+        out = self.relu(out)
+
+        return out
+
+
+class Bottleneck(BaseModule):
+    expansion = 4
+
+    def __init__(self,
+                 inplanes,
+                 planes,
+                 stride=1,
+                 dilation=1,
+                 downsample=None,
+                 style='pytorch',
+                 with_cp=False,
+                 conv_cfg=None,
+                 norm_cfg=dict(type='BN'),
+                 dcn=None,
+                 plugins=None,
+                 init_cfg=None):
+        """Bottleneck block for ResNet.
+
+        If style is "pytorch", the stride-two layer is the 3x3 conv layer, if
+        it is "caffe", the stride-two layer is the first 1x1 conv layer.
+        """
+        super(Bottleneck, self).__init__(init_cfg)
+        assert style in ['pytorch', 'caffe']
+        assert dcn is None or isinstance(dcn, dict)
+        assert plugins is None or isinstance(plugins, list)
+        if plugins is not None:
+            allowed_position = ['after_conv1', 'after_conv2', 'after_conv3']
+            assert all(p['position'] in allowed_position for p in plugins)
+
+        self.inplanes = inplanes
+        self.planes = planes
+        self.stride = stride
+        self.dilation = dilation
+        self.style = style
+        self.with_cp = with_cp
+        self.conv_cfg = conv_cfg
+        self.norm_cfg = norm_cfg
+        self.dcn = dcn
+        self.with_dcn = dcn is not None
+        self.plugins = plugins
+        self.with_plugins = plugins is not None
+
+        if self.with_plugins:
+            # collect plugins for conv1/conv2/conv3
+            self.after_conv1_plugins = [
+                plugin['cfg'] for plugin in plugins
+                if plugin['position'] == 'after_conv1'
+            ]
+            self.after_conv2_plugins = [
+                plugin['cfg'] for plugin in plugins
+                if plugin['position'] == 'after_conv2'
+            ]
+            self.after_conv3_plugins = [
+                plugin['cfg'] for plugin in plugins
+                if plugin['position'] == 'after_conv3'
+            ]
+
+        if self.style == 'pytorch':
+            self.conv1_stride = 1
+            self.conv2_stride = stride
+        else:
+            self.conv1_stride = stride
+            self.conv2_stride = 1
+
+        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)
+        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)
+        self.norm3_name, norm3 = build_norm_layer(
+            norm_cfg, planes * self.expansion, postfix=3)
+
+        self.conv1 = build_conv_layer(
+            conv_cfg,
+            inplanes,
+            planes,
+            kernel_size=1,
+            stride=self.conv1_stride,
+            bias=False)
+        self.add_module(self.norm1_name, norm1)
+        fallback_on_stride = False
+        if self.with_dcn:
+            fallback_on_stride = dcn.pop('fallback_on_stride', False)
+        if not self.with_dcn or fallback_on_stride:
+            self.conv2 = build_conv_layer(
+                conv_cfg,
+                planes,
+                planes,
+                kernel_size=3,
+                stride=self.conv2_stride,
+                padding=dilation,
+                dilation=dilation,
+                bias=False)
+        else:
+            assert self.conv_cfg is None, 'conv_cfg must be None for DCN'
+            self.conv2 = build_conv_layer(
+                dcn,
+                planes,
+                planes,
+                kernel_size=3,
+                stride=self.conv2_stride,
+                padding=dilation,
+                dilation=dilation,
+                bias=False)
+
+        self.add_module(self.norm2_name, norm2)
+        self.conv3 = build_conv_layer(
+            conv_cfg,
+            planes,
+            planes * self.expansion,
+            kernel_size=1,
+            bias=False)
+        self.add_module(self.norm3_name, norm3)
+
+        self.relu = nn.ReLU(inplace=True)
+        self.downsample = downsample
+
+        if self.with_plugins:
+            self.after_conv1_plugin_names = self.make_block_plugins(
+                planes, self.after_conv1_plugins)
+            self.after_conv2_plugin_names = self.make_block_plugins(
+                planes, self.after_conv2_plugins)
+            self.after_conv3_plugin_names = self.make_block_plugins(
+                planes * self.expansion, self.after_conv3_plugins)
+
+    def make_block_plugins(self, in_channels, plugins):
+        """make plugins for block.
+
+        Args:
+            in_channels (int): Input channels of plugin.
+            plugins (list[dict]): List of plugins cfg to build.
+
+        Returns:
+            list[str]: List of the names of plugin.
+        """
+        assert isinstance(plugins, list)
+        plugin_names = []
+        for plugin in plugins:
+            plugin = plugin.copy()
+            name, layer = build_plugin_layer(
+                plugin,
+                in_channels=in_channels,
+                postfix=plugin.pop('postfix', ''))
+            assert not hasattr(self, name), f'duplicate plugin {name}'
+            self.add_module(name, layer)
+            plugin_names.append(name)
+        return plugin_names
+
+    def forward_plugin(self, x, plugin_names):
+        out = x
+        for name in plugin_names:
+            out = getattr(self, name)(out)
+        return out
+
+    @property
+    def norm1(self):
+        """nn.Module: normalization layer after the first convolution layer"""
+        return getattr(self, self.norm1_name)
+
+    @property
+    def norm2(self):
+        """nn.Module: normalization layer after the second convolution layer"""
+        return getattr(self, self.norm2_name)
+
+    @property
+    def norm3(self):
+        """nn.Module: normalization layer after the third convolution layer"""
+        return getattr(self, self.norm3_name)
+
+    def forward(self, x):
+        """Forward function."""
+
+        def _inner_forward(x):
+            identity = x
+            out = self.conv1(x)
+            out = self.norm1(out)
+            out = self.relu(out)
+
+            if self.with_plugins:
+                out = self.forward_plugin(out, self.after_conv1_plugin_names)
+
+            out = self.conv2(out)
+            out = self.norm2(out)
+            out = self.relu(out)
+
+            if self.with_plugins:
+                out = self.forward_plugin(out, self.after_conv2_plugin_names)
+
+            out = self.conv3(out)
+            out = self.norm3(out)
+
+            if self.with_plugins:
+                out = self.forward_plugin(out, self.after_conv3_plugin_names)
+
+            if self.downsample is not None:
+                identity = self.downsample(x)
+
+            out += identity
+
+            return out
+
+        if self.with_cp and x.requires_grad:
+            out = cp.checkpoint(_inner_forward, x)
+        else:
+            out = _inner_forward(x)
+
+        out = self.relu(out)
+
+        return out
+
+
+@BACKBONES.register_module()
+class ResNet(BaseModule):
+    """ResNet backbone.
+
+    Args:
+        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.
+        stem_channels (int | None): Number of stem channels. If not specified,
+            it will be the same as `base_channels`. Default: None.
+        base_channels (int): Number of base channels of res layer. Default: 64.
+        in_channels (int): Number of input image channels. Default: 3.
+        num_stages (int): Resnet stages. Default: 4.
+        strides (Sequence[int]): Strides of the first block of each stage.
+        dilations (Sequence[int]): Dilation of each stage.
+        out_indices (Sequence[int]): Output from which stages.
+        style (str): `pytorch` or `caffe`. If set to "pytorch", the stride-two
+            layer is the 3x3 conv layer, otherwise the stride-two layer is
+            the first 1x1 conv layer.
+        deep_stem (bool): Replace 7x7 conv in input stem with 3 3x3 conv
+        avg_down (bool): Use AvgPool instead of stride conv when
+            downsampling in the bottleneck.
+        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).
+            -1 means not freezing any parameters.
+        norm_cfg (dict): Dictionary to construct and config norm layer.
+        norm_eval (bool): Whether to set norm layers to eval mode, namely,
+            freeze running stats (mean and var). Note: Effect on Batch Norm
+            and its variants only.
+        plugins (list[dict]): List of plugins for stages, each dict contains:
+
+            - cfg (dict, required): Cfg dict to build plugin.
+            - position (str, required): Position inside block to insert
+              plugin, options are 'after_conv1', 'after_conv2', 'after_conv3'.
+            - stages (tuple[bool], optional): Stages to apply plugin, length
+              should be same as 'num_stages'.
+        with_cp (bool): Use checkpoint or not. Using checkpoint will save some
+            memory while slowing down the training speed.
+        zero_init_residual (bool): Whether to use zero init for last norm layer
+            in resblocks to let them behave as identity.
+        pretrained (str, optional): model pretrained path. Default: None
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+            Default: None
+
+    Example:
+        >>> from mmdet.models import ResNet
+        >>> import torch
+        >>> self = ResNet(depth=18)
+        >>> self.eval()
+        >>> inputs = torch.rand(1, 3, 32, 32)
+        >>> level_outputs = self.forward(inputs)
+        >>> for level_out in level_outputs:
+        ...     print(tuple(level_out.shape))
+        (1, 64, 8, 8)
+        (1, 128, 4, 4)
+        (1, 256, 2, 2)
+        (1, 512, 1, 1)
+    """
+
+    arch_settings = {
+        18: (BasicBlock, (2, 2, 2, 2)),
+        34: (BasicBlock, (3, 4, 6, 3)),
+        50: (Bottleneck, (3, 4, 6, 3)),
+        101: (Bottleneck, (3, 4, 23, 3)),
+        152: (Bottleneck, (3, 8, 36, 3))
+    }
+
+    def __init__(self,
+                 depth,
+                 in_channels=3,
+                 stem_channels=None,
+                 base_channels=64,
+                 num_stages=4,
+                 strides=(1, 2, 2, 2),
+                 dilations=(1, 1, 1, 1),
+                 out_indices=(0, 1, 2, 3),
+                 style='pytorch',
+                 deep_stem=False,
+                 avg_down=False,
+                 frozen_stages=-1,
+                 conv_cfg=None,
+                 norm_cfg=dict(type='BN', requires_grad=True),
+                 norm_eval=True,
+                 dcn=None,
+                 stage_with_dcn=(False, False, False, False),
+                 plugins=None,
+                 with_cp=False,
+                 zero_init_residual=True,
+                 pretrained=None,
+                 init_cfg=None):
+        super(ResNet, self).__init__(init_cfg)
+        self.zero_init_residual = zero_init_residual
+        if depth not in self.arch_settings:
+            raise KeyError(f'invalid depth {depth} for resnet')
+
+        block_init_cfg = None
+        assert not (init_cfg and pretrained), \
+            'init_cfg and pretrained cannot be specified at the same time'
+        if isinstance(pretrained, str):
+            warnings.warn('DeprecationWarning: pretrained is deprecated, '
+                          'please use "init_cfg" instead')
+            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
+        elif pretrained is None:
+            if init_cfg is None:
+                self.init_cfg = [
+                    dict(type='Kaiming', layer='Conv2d'),
+                    dict(
+                        type='Constant',
+                        val=1,
+                        layer=['_BatchNorm', 'GroupNorm'])
+                ]
+                block = self.arch_settings[depth][0]
+                if self.zero_init_residual:
+                    if block is BasicBlock:
+                        block_init_cfg = dict(
+                            type='Constant',
+                            val=0,
+                            override=dict(name='norm2'))
+                    elif block is Bottleneck:
+                        block_init_cfg = dict(
+                            type='Constant',
+                            val=0,
+                            override=dict(name='norm3'))
+        else:
+            raise TypeError('pretrained must be a str or None')
+
+        self.depth = depth
+        if stem_channels is None:
+            stem_channels = base_channels
+        self.stem_channels = stem_channels
+        self.base_channels = base_channels
+        self.num_stages = num_stages
+        assert num_stages >= 1 and num_stages <= 4
+        self.strides = strides
+        self.dilations = dilations
+        assert len(strides) == len(dilations) == num_stages
+        self.out_indices = out_indices
+        assert max(out_indices) < num_stages
+        self.style = style
+        self.deep_stem = deep_stem
+        self.avg_down = avg_down
+        self.frozen_stages = frozen_stages
+        self.conv_cfg = conv_cfg
+        self.norm_cfg = norm_cfg
+        self.with_cp = with_cp
+        self.norm_eval = norm_eval
+        self.dcn = dcn
+        self.stage_with_dcn = stage_with_dcn
+        if dcn is not None:
+            assert len(stage_with_dcn) == num_stages
+        self.plugins = plugins
+        self.block, stage_blocks = self.arch_settings[depth]
+        self.stage_blocks = stage_blocks[:num_stages]
+        self.inplanes = stem_channels
+
+        self._make_stem_layer(in_channels, stem_channels)
+
+        self.res_layers = []
+        for i, num_blocks in enumerate(self.stage_blocks):
+            stride = strides[i]
+            dilation = dilations[i]
+            dcn = self.dcn if self.stage_with_dcn[i] else None
+            if plugins is not None:
+                stage_plugins = self.make_stage_plugins(plugins, i)
+            else:
+                stage_plugins = None
+            planes = base_channels * 2**i
+            res_layer = self.make_res_layer(
+                block=self.block,
+                inplanes=self.inplanes,
+                planes=planes,
+                num_blocks=num_blocks,
+                stride=stride,
+                dilation=dilation,
+                style=self.style,
+                avg_down=self.avg_down,
+                with_cp=with_cp,
+                conv_cfg=conv_cfg,
+                norm_cfg=norm_cfg,
+                dcn=dcn,
+                plugins=stage_plugins,
+                init_cfg=block_init_cfg)
+            self.inplanes = planes * self.block.expansion
+            layer_name = f'layer{i + 1}'
+            self.add_module(layer_name, res_layer)
+            self.res_layers.append(layer_name)
+
+        self._freeze_stages()
+
+        self.feat_dim = self.block.expansion * base_channels * 2**(
+            len(self.stage_blocks) - 1)
+
+    def make_stage_plugins(self, plugins, stage_idx):
+        """Make plugins for ResNet ``stage_idx`` th stage.
+
+        Currently we support to insert ``context_block``,
+        ``empirical_attention_block``, ``nonlocal_block`` into the backbone
+        like ResNet/ResNeXt. They could be inserted after conv1/conv2/conv3 of
+        Bottleneck.
+
+        An example of plugins format could be:
+
+        Examples:
+            >>> plugins=[
+            ...     dict(cfg=dict(type='xxx', arg1='xxx'),
+            ...          stages=(False, True, True, True),
+            ...          position='after_conv2'),
+            ...     dict(cfg=dict(type='yyy'),
+            ...          stages=(True, True, True, True),
+            ...          position='after_conv3'),
+            ...     dict(cfg=dict(type='zzz', postfix='1'),
+            ...          stages=(True, True, True, True),
+            ...          position='after_conv3'),
+            ...     dict(cfg=dict(type='zzz', postfix='2'),
+            ...          stages=(True, True, True, True),
+            ...          position='after_conv3')
+            ... ]
+            >>> self = ResNet(depth=18)
+            >>> stage_plugins = self.make_stage_plugins(plugins, 0)
+            >>> assert len(stage_plugins) == 3
+
+        Suppose ``stage_idx=0``, the structure of blocks in the stage would be:
+
+        .. code-block:: none
+
+            conv1-> conv2->conv3->yyy->zzz1->zzz2
+
+        Suppose 'stage_idx=1', the structure of blocks in the stage would be:
+
+        .. code-block:: none
+
+            conv1-> conv2->xxx->conv3->yyy->zzz1->zzz2
+
+        If stages is missing, the plugin would be applied to all stages.
+
+        Args:
+            plugins (list[dict]): List of plugins cfg to build. The postfix is
+                required if multiple same type plugins are inserted.
+            stage_idx (int): Index of stage to build
+
+        Returns:
+            list[dict]: Plugins for current stage
+        """
+        stage_plugins = []
+        for plugin in plugins:
+            plugin = plugin.copy()
+            stages = plugin.pop('stages', None)
+            assert stages is None or len(stages) == self.num_stages
+            # whether to insert plugin into current stage
+            if stages is None or stages[stage_idx]:
+                stage_plugins.append(plugin)
+
+        return stage_plugins
+
+    def make_res_layer(self, **kwargs):
+        """Pack all blocks in a stage into a ``ResLayer``."""
+        return ResLayer(**kwargs)
+
+    @property
+    def norm1(self):
+        """nn.Module: the normalization layer named "norm1" """
+        return getattr(self, self.norm1_name)
+
+    def _make_stem_layer(self, in_channels, stem_channels):
+        if self.deep_stem:
+            self.stem = nn.Sequential(
+                build_conv_layer(
+                    self.conv_cfg,
+                    in_channels,
+                    stem_channels // 2,
+                    kernel_size=3,
+                    stride=2,
+                    padding=1,
+                    bias=False),
+                build_norm_layer(self.norm_cfg, stem_channels // 2)[1],
+                nn.ReLU(inplace=True),
+                build_conv_layer(
+                    self.conv_cfg,
+                    stem_channels // 2,
+                    stem_channels // 2,
+                    kernel_size=3,
+                    stride=1,
+                    padding=1,
+                    bias=False),
+                build_norm_layer(self.norm_cfg, stem_channels // 2)[1],
+                nn.ReLU(inplace=True),
+                build_conv_layer(
+                    self.conv_cfg,
+                    stem_channels // 2,
+                    stem_channels,
+                    kernel_size=3,
+                    stride=1,
+                    padding=1,
+                    bias=False),
+                build_norm_layer(self.norm_cfg, stem_channels)[1],
+                nn.ReLU(inplace=True))
+        else:
+            self.conv1 = build_conv_layer(
+                self.conv_cfg,
+                in_channels,
+                stem_channels,
+                kernel_size=7,
+                stride=2,
+                padding=3,
+                bias=False)
+            self.norm1_name, norm1 = build_norm_layer(
+                self.norm_cfg, stem_channels, postfix=1)
+            self.add_module(self.norm1_name, norm1)
+            self.relu = nn.ReLU(inplace=True)
+
+    def _freeze_stages(self):
+        if self.frozen_stages >= 0:
+            if self.deep_stem:
+                self.stem.eval()
+                for param in self.stem.parameters():
+                    param.requires_grad = False
+            else:
+                self.norm1.eval()
+                for m in [self.conv1, self.norm1]:
+                    for param in m.parameters():
+                        param.requires_grad = False
+
+        for i in range(1, self.frozen_stages + 1):
+            m = getattr(self, f'layer{i}')
+            m.eval()
+            for param in m.parameters():
+                param.requires_grad = False
+
+    def forward(self, x):
+        """Forward function."""
+        if self.deep_stem:
+            x = self.stem(x)
+        else:
+            x = self.conv1(x)
+            x = self.norm1(x)
+            x = self.relu(x)
+        x = mx_driving.fused.npu_max_pool2d(x, 3, 2, 1)
+        outs = []
+        for i, layer_name in enumerate(self.res_layers):
+            res_layer = getattr(self, layer_name)
+            x = res_layer(x)
+            if i in self.out_indices:
+                outs.append(x)
+        return tuple(outs)
+
+    def train(self, mode=True):
+        """Convert the model into training mode while keep normalization layer
+        freezed."""
+        super(ResNet, self).train(mode)
+        self._freeze_stages()
+        if mode and self.norm_eval:
+            for m in self.modules():
+                # trick: eval have effect on BatchNorm only
+                if isinstance(m, _BatchNorm):
+                    m.eval()
+
+
+@BACKBONES.register_module()
+class ResNetV1d(ResNet):
+    r"""ResNetV1d variant described in `Bag of Tricks
+
+    Compared with default ResNet(ResNetV1b), ResNetV1d replaces the 7x7 conv in
+    the input stem with three 3x3 convs. And in the downsampling block, a 2x2
+    avg_pool with stride 2 is added before conv, whose stride is changed to 1.
+    """
+
+    def __init__(self, **kwargs):
+        super(ResNetV1d, self).__init__(
+            deep_stem=True, avg_down=True, **kwargs)
diff --git a/patch/mmdet3d/__init__.py b/patch/mmdet3d/__init__.py
new file mode 100644
index 0000000..16366c8
--- /dev/null
+++ b/patch/mmdet3d/__init__.py
@@ -0,0 +1,62 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# Copyright (c) OpenMMLab. All rights reserved.
+import mmcv
+
+import mmdet
+import mmseg
+from .version import __version__, short_version
+
+
+def digit_version(version_str):
+    digit_version = []
+    for x in version_str.split('.'):
+        if x.isdigit():
+            digit_version.append(int(x))
+        elif x.find('rc') != -1:
+            patch_version = x.split('rc')
+            digit_version.append(int(patch_version[0]) - 1)
+            digit_version.append(int(patch_version[1]))
+    return digit_version
+
+
+mmcv_minimum_version = '1.5.2'
+mmcv_maximum_version = '1.7.2'
+mmcv_version = digit_version(mmcv.__version__)
+
+
+assert (mmcv_version >= digit_version(mmcv_minimum_version)
+        and mmcv_version <= digit_version(mmcv_maximum_version)), \
+    f'MMCV=={mmcv.__version__} is used but incompatible. ' \
+    f'Please install mmcv>={mmcv_minimum_version}, <={mmcv_maximum_version}.'
+
+mmdet_minimum_version = '2.24.0'
+mmdet_maximum_version = '3.0.0'
+mmdet_version = digit_version(mmdet.__version__)
+assert (mmdet_version >= digit_version(mmdet_minimum_version)
+        and mmdet_version <= digit_version(mmdet_maximum_version)), \
+    f'MMDET=={mmdet.__version__} is used but incompatible. ' \
+    f'Please install mmdet>={mmdet_minimum_version}, ' \
+    f'<={mmdet_maximum_version}.'
+
+mmseg_minimum_version = '0.20.0'
+mmseg_maximum_version = '1.0.0'
+mmseg_version = digit_version(mmseg.__version__)
+assert (mmseg_version >= digit_version(mmseg_minimum_version)
+        and mmseg_version <= digit_version(mmseg_maximum_version)), \
+    f'MMSEG=={mmseg.__version__} is used but incompatible. ' \
+    f'Please install mmseg>={mmseg_minimum_version}, ' \
+    f'<={mmseg_maximum_version}.'
+
+__all__ = ['__version__', 'short_version']
\ No newline at end of file
diff --git a/patch/mmdet3d/runtime.txt b/patch/mmdet3d/runtime.txt
new file mode 100644
index 0000000..eaade94
--- /dev/null
+++ b/patch/mmdet3d/runtime.txt
@@ -0,0 +1,9 @@
+lyft_dataset_sdk
+networkx>=2.2,<2.3
+numpy
+nuscenes-devkit
+plyfile
+scikit-image
+# by default we also use tensorboard to log results
+tensorboard
+trimesh>=2.35.39,<2.35.40
\ No newline at end of file
diff --git a/patch/torch/conv.py b/patch/torch/conv.py
new file mode 100644
index 0000000..5dbfb58
--- /dev/null
+++ b/patch/torch/conv.py
@@ -0,0 +1,1569 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+
+# -*- coding: utf-8 -*-
+import math
+import warnings
+
+import torch
+from torch import Tensor
+from torch.nn.parameter import Parameter, UninitializedParameter
+from .. import functional as F
+from .. import init
+from .lazy import LazyModuleMixin
+from .module import Module
+from .utils import _single, _pair, _triple, _reverse_repeat_tuple
+from torch._torch_docs import reproducibility_notes
+
+from ..common_types import _size_1_t, _size_2_t, _size_3_t
+from typing import Optional, List, Tuple, Union
+
+convolution_notes = \
+    {"groups_note": r"""* :attr:`groups` controls the connections between inputs and outputs.
+      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
+      :attr:`groups`. For example,
+
+        * At groups=1, all inputs are convolved to all outputs.
+        * At groups=2, the operation becomes equivalent to having two conv
+          layers side by side, each seeing half the input channels
+          and producing half the output channels, and both subsequently
+          concatenated.
+        * At groups= :attr:`in_channels`, each input channel is convolved with
+          its own set of filters (of size
+          :math:`\frac{\text{out\_channels}}{\text{in\_channels}}`).""",
+
+        "depthwise_separable_note": r"""When `groups == in_channels` and `out_channels == K * in_channels`,
+        where `K` is a positive integer, this operation is also known as a "depthwise convolution".
+
+        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,
+        a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments
+        :math:`(C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})`."""}  # noqa: B950
+
+
+
+
+
+class _ConvNd(Module):
+
+    __constants__ = ['stride', 'padding', 'dilation', 'groups',
+                     'padding_mode', 'output_padding', 'in_channels',
+                     'out_channels', 'kernel_size']
+    __annotations__ = {'bias': Optional[torch.Tensor]}
+
+    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:
+        ...
+
+    _in_channels: int
+    _reversed_padding_repeated_twice: List[int]
+    out_channels: int
+    kernel_size: Tuple[int, ...]
+    stride: Tuple[int, ...]
+    padding: Union[str, Tuple[int, ...]]
+    dilation: Tuple[int, ...]
+    transposed: bool
+    output_padding: Tuple[int, ...]
+    groups: int
+    padding_mode: str
+    weight: Tensor
+    bias: Optional[Tensor]
+
+    def __init__(self,
+                 in_channels: int,
+                 out_channels: int,
+                 kernel_size: Tuple[int, ...],
+                 stride: Tuple[int, ...],
+                 padding: Tuple[int, ...],
+                 dilation: Tuple[int, ...],
+                 transposed: bool,
+                 output_padding: Tuple[int, ...],
+                 groups: int,
+                 bias: bool,
+                 padding_mode: str,
+                 device=None,
+                 dtype=None) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super(_ConvNd, self).__init__()
+        if in_channels % groups != 0:
+            raise ValueError('in_channels must be divisible by groups')
+        if out_channels % groups != 0:
+            raise ValueError('out_channels must be divisible by groups')
+        valid_padding_strings = {'same', 'valid'}
+        if isinstance(padding, str):
+            if padding not in valid_padding_strings:
+                raise ValueError(
+                    "Invalid padding string {!r}, should be one of {}".format(
+                        padding, valid_padding_strings))
+            if padding == 'same' and any(s != 1 for s in stride):
+                raise ValueError("padding='same' is not supported for strided convolutions")
+
+        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}
+        if padding_mode not in valid_padding_modes:
+            raise ValueError("padding_mode must be one of {}, but got padding_mode='{}'".format(
+                valid_padding_modes, padding_mode))
+        self.in_channels = in_channels
+        self.out_channels = out_channels
+        self.kernel_size = kernel_size
+        self.stride = stride
+        self.padding = padding
+        self.dilation = dilation
+        self.transposed = transposed
+        self.output_padding = output_padding
+        self.groups = groups
+        self.padding_mode = padding_mode
+        # `_reversed_padding_repeated_twice` is the padding to be passed to
+        # `F.pad` if needed (e.g., for non-zero padding types that are
+        # implemented as two ops: padding + conv). `F.pad` accepts paddings in
+        # reverse order than the dimension.
+        if isinstance(self.padding, str):
+            self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size)
+            if padding == 'same':
+                for d, k, i in zip(dilation, kernel_size,
+                                   range(len(kernel_size) - 1, -1, -1)):
+                    total_padding = d * (k - 1)
+                    left_pad = total_padding // 2
+                    self._reversed_padding_repeated_twice[2 * i] = left_pad
+                    self._reversed_padding_repeated_twice[2 * i + 1] = (
+                        total_padding - left_pad)
+        else:
+            self._reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)
+
+        if transposed:
+            self.weight = Parameter(torch.empty(
+                (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))
+        else:
+            self.weight = Parameter(torch.empty(
+                (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))
+        if bias:
+            self.bias = Parameter(torch.empty(out_channels, **factory_kwargs))
+        else:
+            self.register_parameter('bias', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with
+        # uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)
+        # For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573
+        init.kaiming_uniform_(self.weight, a=math.sqrt(5))
+        if self.bias is not None:
+            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
+            if fan_in != 0:
+                bound = 1 / math.sqrt(fan_in)
+                init.uniform_(self.bias, -bound, bound)
+
+    def extra_repr(self):
+        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'
+             ', stride={stride}')
+        if self.padding != (0,) * len(self.padding):
+            s += ', padding={padding}'
+        if self.dilation != (1,) * len(self.dilation):
+            s += ', dilation={dilation}'
+        if self.output_padding != (0,) * len(self.output_padding):
+            s += ', output_padding={output_padding}'
+        if self.groups != 1:
+            s += ', groups={groups}'
+        if self.bias is None:
+            s += ', bias=False'
+        if self.padding_mode != 'zeros':
+            s += ', padding_mode={padding_mode}'
+        return s.format(**self.__dict__)
+
+    def __setstate__(self, state):
+        super(_ConvNd, self).__setstate__(state)
+        if not hasattr(self, 'padding_mode'):
+            self.padding_mode = 'zeros'
+
+
+class Conv1d(_ConvNd):
+    __doc__ = r"""Applies a 1D convolution over an input signal composed of several input
+    planes.
+
+    In the simplest case, the output value of the layer with input size
+    :math:`(N, C_{\text{in}}, L)` and output :math:`(N, C_{\text{out}}, L_{\text{out}})` can be
+    precisely described as:
+
+    .. math::
+        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
+        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
+        \star \text{input}(N_i, k)
+
+    where :math:`\star` is the valid `cross-correlation`_ operator,
+    :math:`N` is a batch size, :math:`C` denotes a number of channels,
+    :math:`L` is a length of signal sequence.
+    """ + r"""
+
+    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
+
+    * :attr:`stride` controls the stride for the cross-correlation, a single
+      number or a one-element tuple.
+
+    * :attr:`padding` controls the amount of padding applied to the input. It
+      can be either a string {{'valid', 'same'}} or a tuple of ints giving the
+      amount of implicit padding applied on both sides.
+
+    * :attr:`dilation` controls the spacing between the kernel points; also
+      known as the à trous algorithm. It is harder to describe, but this `link`_
+      has a nice visualization of what :attr:`dilation` does.
+
+    {groups_note}
+
+    Note:
+        {depthwise_separable_note}
+    Note:
+        {cudnn_reproducibility_note}
+
+    Note:
+        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
+        the input so the output has the shape as the input. However, this mode
+        doesn't support any stride values other than 1.
+
+    Args:
+        in_channels (int): Number of channels in the input image
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int, tuple or str, optional): Padding added to both sides of
+            the input. Default: 0
+        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
+            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
+        dilation (int or tuple, optional): Spacing between kernel
+            elements. Default: 1
+        groups (int, optional): Number of blocked connections from input
+            channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the
+            output. Default: ``True``
+
+    """.format(**reproducibility_notes, **convolution_notes) + r"""
+
+    Shape:
+        - Input: :math:`(N, C_{in}, L_{in})` or :math:`(C_{in}, L_{in})`
+        - Output: :math:`(N, C_{out}, L_{out})` or :math:`(C_{out}, L_{out})`, where
+
+          .. math::
+              L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
+                        \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor
+
+    Attributes:
+        weight (Tensor): the learnable weights of the module of shape
+            :math:`(\text{out\_channels},
+            \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})`.
+            The values of these weights are sampled from
+            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`
+        bias (Tensor):   the learnable bias of the module of shape
+            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are
+            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`
+
+    Examples::
+
+        >>> m = nn.Conv1d(16, 33, 3, stride=2)
+        >>> input = torch.randn(20, 16, 50)
+        >>> output = m(input)
+
+    .. _cross-correlation:
+        https://en.wikipedia.org/wiki/Cross-correlation
+
+    .. _link:
+        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_1_t,
+        stride: _size_1_t = 1,
+        padding: Union[str, _size_1_t] = 0,
+        dilation: _size_1_t = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = 'zeros',  # TODO: refine this type
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        # we create new variables below to make mypy happy since kernel_size has
+        # type Union[int, Tuple[int]] and kernel_size_ has type Tuple[int]
+        kernel_size_ = _single(kernel_size)
+        stride_ = _single(stride)
+        padding_ = padding if isinstance(padding, str) else _single(padding)
+        dilation_ = _single(dilation)
+        super(Conv1d, self).__init__(
+            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,
+            False, _single(0), groups, bias, padding_mode, **factory_kwargs)
+
+    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
+        if self.padding_mode != 'zeros':
+            return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
+                            weight, bias, self.stride,
+                            _single(0), self.dilation, self.groups)
+        return F.conv1d(input, weight, bias, self.stride,
+                        self.padding, self.dilation, self.groups)
+
+    def forward(self, input: Tensor) -> Tensor:
+        return self._conv_forward(input, self.weight, self.bias)
+
+
+class Conv2d(_ConvNd):
+    __doc__ = r"""Applies a 2D convolution over an input signal composed of several input
+    planes.
+
+    In the simplest case, the output value of the layer with input size
+    :math:`(N, C_{\text{in}}, H, W)` and output :math:`(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})`
+    can be precisely described as:
+
+    .. math::
+        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
+        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)
+
+
+    where :math:`\star` is the valid 2D `cross-correlation`_ operator,
+    :math:`N` is a batch size, :math:`C` denotes a number of channels,
+    :math:`H` is a height of input planes in pixels, and :math:`W` is
+    width in pixels.
+    """ + r"""
+
+    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
+
+    * :attr:`stride` controls the stride for the cross-correlation, a single
+      number or a tuple.
+
+    * :attr:`padding` controls the amount of padding applied to the input. It
+      can be either a string {{'valid', 'same'}} or a tuple of ints giving the
+      amount of implicit padding applied on both sides.
+
+    * :attr:`dilation` controls the spacing between the kernel points; also
+      known as the à trous algorithm. It is harder to describe, but this `link`_
+      has a nice visualization of what :attr:`dilation` does.
+
+    {groups_note}
+
+    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:
+
+        - a single ``int`` -- in which case the same value is used for the height and width dimension
+        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
+          and the second `int` for the width dimension
+
+    Note:
+        {depthwise_separable_note}
+
+    Note:
+        {cudnn_reproducibility_note}
+
+    Note:
+        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
+        the input so the output has the shape as the input. However, this mode
+        doesn't support any stride values other than 1.
+
+    Args:
+        in_channels (int): Number of channels in the input image
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int, tuple or str, optional): Padding added to all four sides of
+            the input. Default: 0
+        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
+            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+        groups (int, optional): Number of blocked connections from input
+            channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the
+            output. Default: ``True``
+    """.format(**reproducibility_notes, **convolution_notes) + r"""
+
+    Shape:
+        - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`
+        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where
+
+          .. math::
+              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
+                        \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
+
+          .. math::
+              W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
+                        \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
+
+    Attributes:
+        weight (Tensor): the learnable weights of the module of shape
+            :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`
+            :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.
+            The values of these weights are sampled from
+            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
+        bias (Tensor):   the learnable bias of the module of shape
+            (out_channels). If :attr:`bias` is ``True``,
+            then the values of these weights are
+            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
+
+    Examples:
+
+        >>> # With square kernels and equal stride
+        >>> m = nn.Conv2d(16, 33, 3, stride=2)
+        >>> # non-square kernels and unequal stride and with padding
+        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
+        >>> # non-square kernels and unequal stride and with padding and dilation
+        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
+        >>> input = torch.randn(20, 16, 50, 100)
+        >>> output = m(input)
+
+    .. _cross-correlation:
+        https://en.wikipedia.org/wiki/Cross-correlation
+
+    .. _link:
+        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_2_t,
+        stride: _size_2_t = 1,
+        padding: Union[str, _size_2_t] = 0,
+        dilation: _size_2_t = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = 'zeros',  # TODO: refine this type
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        kernel_size_ = _pair(kernel_size)
+        stride_ = _pair(stride)
+        padding_ = padding if isinstance(padding, str) else _pair(padding)
+        dilation_ = _pair(dilation)
+        super(Conv2d, self).__init__(
+            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,
+            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)
+
+    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
+        if self.padding_mode != 'zeros':
+            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
+                            weight, bias, self.stride,
+                            _pair(0), self.dilation, self.groups)
+        return F.conv2d(input, weight, bias, self.stride,
+                        self.padding, self.dilation, self.groups)
+
+    def forward(self, input: Tensor) -> Tensor:
+        return self._conv_forward(input, self.weight, self.bias)
+
+class Conv3d(_ConvNd):
+    __doc__ = r"""Applies a 3D convolution over an input signal composed of several input
+    planes.
+
+    In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`
+    and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:
+
+    .. math::
+        out(N_i, C_{out_j}) = bias(C_{out_j}) +
+                                \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)
+
+    where :math:`\star` is the valid 3D `cross-correlation`_ operator
+    """ + r"""
+
+    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
+
+    * :attr:`stride` controls the stride for the cross-correlation.
+
+    * :attr:`padding` controls the amount of padding applied to the input. It
+      can be either a string {{'valid', 'same'}} or a tuple of ints giving the
+      amount of implicit padding applied on both sides.
+
+    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
+      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.
+
+    {groups_note}
+
+    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:
+
+        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension
+        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
+          the second `int` for the height dimension and the third `int` for the width dimension
+
+    Note:
+        {depthwise_separable_note}
+
+    Note:
+        {cudnn_reproducibility_note}
+
+    Note:
+        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads
+        the input so the output has the shape as the input. However, this mode
+        doesn't support any stride values other than 1.
+
+    Args:
+        in_channels (int): Number of channels in the input image
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int, tuple or str, optional): Padding added to all six sides of
+            the input. Default: 0
+        padding_mode (string, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
+    """.format(**reproducibility_notes, **convolution_notes) + r"""
+
+    Shape:
+        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` or :math:`(C_{in}, D_{in}, H_{in}, W_{in})`
+        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` or :math:`(C_{out}, D_{out}, H_{out}, W_{out})`,
+          where
+
+          .. math::
+              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
+                    \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor
+
+          .. math::
+              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
+                    \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor
+
+          .. math::
+              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
+                    \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor
+
+    Attributes:
+        weight (Tensor): the learnable weights of the module of shape
+                         :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`
+                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.
+                         The values of these weights are sampled from
+                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
+        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,
+                         then the values of these weights are
+                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
+
+    Examples::
+
+        >>> # With square kernels and equal stride
+        >>> m = nn.Conv3d(16, 33, 3, stride=2)
+        >>> # non-square kernels and unequal stride and with padding
+        >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
+        >>> input = torch.randn(20, 16, 10, 50, 100)
+        >>> output = m(input)
+
+    .. _cross-correlation:
+        https://en.wikipedia.org/wiki/Cross-correlation
+
+    .. _link:
+        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_3_t,
+        stride: _size_3_t = 1,
+        padding: Union[str, _size_3_t] = 0,
+        dilation: _size_3_t = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        kernel_size_ = _triple(kernel_size)
+        stride_ = _triple(stride)
+        padding_ = padding if isinstance(padding, str) else _triple(padding)
+        dilation_ = _triple(dilation)
+        super(Conv3d, self).__init__(
+            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,
+            False, _triple(0), groups, bias, padding_mode, **factory_kwargs)
+
+    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):
+        if self.padding_mode != "zeros":
+            return F.conv3d(
+                F.pad(
+                    input, self._reversed_padding_repeated_twice, mode=self.padding_mode
+                ),
+                weight,
+                bias,
+                self.stride,
+                _triple(0),
+                self.dilation,
+                self.groups,
+            )
+        return F.conv3d(
+            input.to(torch.bfloat16), weight.to(torch.bfloat16), bias, self.stride, self.padding, self.dilation, self.groups
+        ).float()
+
+    def forward(self, input: Tensor) -> Tensor:
+        return self._conv_forward(input, self.weight, self.bias)
+
+
+
+class _ConvTransposeNd(_ConvNd):
+    def __init__(self, in_channels, out_channels, kernel_size, stride,
+                 padding, dilation, transposed, output_padding,
+                 groups, bias, padding_mode, device=None, dtype=None) -> None:
+        if padding_mode != 'zeros':
+            raise ValueError('Only "zeros" padding mode is supported for {}'.format(self.__class__.__name__))
+
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super(_ConvTransposeNd, self).__init__(
+            in_channels, out_channels, kernel_size, stride,
+            padding, dilation, transposed, output_padding,
+            groups, bias, padding_mode, **factory_kwargs)
+
+    # dilation being an optional parameter is for backwards
+    # compatibility
+    def _output_padding(self, input: Tensor, output_size: Optional[List[int]],
+                        stride: List[int], padding: List[int], kernel_size: List[int],
+                        dilation: Optional[List[int]] = None) -> List[int]:
+        if output_size is None:
+            ret = _single(self.output_padding)  # converting to list if was not already
+        else:
+            k = input.dim() - 2
+            if len(output_size) == k + 2:
+                output_size = output_size[2:]
+            if len(output_size) != k:
+                raise ValueError(
+                    "output_size must have {} or {} elements (got {})"
+                    .format(k, k + 2, len(output_size)))
+
+            min_sizes = torch.jit.annotate(List[int], [])
+            max_sizes = torch.jit.annotate(List[int], [])
+            for d in range(k):
+                dim_size = ((input.size(d + 2) - 1) * stride[d] -
+                            2 * padding[d] +
+                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)
+                min_sizes.append(dim_size)
+                max_sizes.append(min_sizes[d] + stride[d] - 1)
+
+            for i in range(len(output_size)):
+                size = output_size[i]
+                min_size = min_sizes[i]
+                max_size = max_sizes[i]
+                if size < min_size or size > max_size:
+                    raise ValueError((
+                        "requested an output size of {}, but valid sizes range "
+                        "from {} to {} (for an input of {})").format(
+                            output_size, min_sizes, max_sizes, input.size()[2:]))
+
+            res = torch.jit.annotate(List[int], [])
+            for d in range(k):
+                res.append(output_size[d] - min_sizes[d])
+
+            ret = res
+        return ret
+
+
+class ConvTranspose1d(_ConvTransposeNd):
+    __doc__ = r"""Applies a 1D transposed convolution operator over an input image
+    composed of several input planes.
+
+    This module can be seen as the gradient of Conv1d with respect to its input.
+    It is also known as a fractionally-strided convolution or
+    a deconvolution (although it is not an actual deconvolution operation as it does
+    not compute a true inverse of convolution). For more information, see the visualizations
+    `here`_ and the `Deconvolutional Networks`_ paper.
+
+    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
+
+    * :attr:`stride` controls the stride for the cross-correlation.
+
+    * :attr:`padding` controls the amount of implicit zero padding on both
+      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
+      below for details.
+
+    * :attr:`output_padding` controls the additional size added to one side
+      of the output shape. See note below for details.
+
+    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
+      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.
+
+    {groups_note}
+
+    Note:
+        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
+        amount of zero padding to both sizes of the input. This is set so that
+        when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`
+        are initialized with same parameters, they are inverses of each other in
+        regard to the input and output shapes. However, when ``stride > 1``,
+        :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output
+        shape. :attr:`output_padding` is provided to resolve this ambiguity by
+        effectively increasing the calculated output shape on one side. Note
+        that :attr:`output_padding` is only used to find output shape, but does
+        not actually add zero-padding to output.
+
+    Note:
+        In some circumstances when using the CUDA backend with CuDNN, this operator
+        may select a nondeterministic algorithm to increase performance. If this is
+        undesirable, you can try to make the operation deterministic (potentially at
+        a performance cost) by setting ``torch.backends.cudnn.deterministic =
+        True``.
+        Please see the notes on :doc:`/notes/randomness` for background.
+
+
+    Args:
+        in_channels (int): Number of channels in the input image
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
+            will be added to both sides of the input. Default: 0
+        output_padding (int or tuple, optional): Additional size added to one side
+            of the output shape. Default: 0
+        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+    """.format(**reproducibility_notes, **convolution_notes) + r"""
+
+    Shape:
+        - Input: :math:`(N, C_{in}, L_{in})` or :math:`(C_{in}, L_{in})`
+        - Output: :math:`(N, C_{out}, L_{out})` or :math:`(C_{out}, L_{out})`, where
+
+          .. math::
+              L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
+                        \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1
+
+    Attributes:
+        weight (Tensor): the learnable weights of the module of shape
+                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
+                         :math:`\text{kernel\_size})`.
+                         The values of these weights are sampled from
+                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
+        bias (Tensor):   the learnable bias of the module of shape (out_channels).
+                         If :attr:`bias` is ``True``, then the values of these weights are
+                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
+
+    .. _`here`:
+        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
+
+    .. _`Deconvolutional Networks`:
+        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_1_t,
+        stride: _size_1_t = 1,
+        padding: _size_1_t = 0,
+        output_padding: _size_1_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: _size_1_t = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        kernel_size = _single(kernel_size)
+        stride = _single(stride)
+        padding = _single(padding)
+        dilation = _single(dilation)
+        output_padding = _single(output_padding)
+        super(ConvTranspose1d, self).__init__(
+            in_channels, out_channels, kernel_size, stride, padding, dilation,
+            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
+
+    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
+        if self.padding_mode != 'zeros':
+            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose1d')
+
+        assert isinstance(self.padding, tuple)
+        # One cannot replace List by Tuple or Sequence in "_output_padding" because
+        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
+        output_padding = self._output_padding(
+            input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]
+        return F.conv_transpose1d(
+            input, self.weight, self.bias, self.stride, self.padding,
+            output_padding, self.groups, self.dilation)
+
+
+class ConvTranspose2d(_ConvTransposeNd):
+    __doc__ = r"""Applies a 2D transposed convolution operator over an input image
+    composed of several input planes.
+
+    This module can be seen as the gradient of Conv2d with respect to its input.
+    It is also known as a fractionally-strided convolution or
+    a deconvolution (although it is not an actual deconvolution operation as it does
+    not compute a true inverse of convolution). For more information, see the visualizations
+    `here`_ and the `Deconvolutional Networks`_ paper.
+
+    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
+
+    * :attr:`stride` controls the stride for the cross-correlation.
+
+    * :attr:`padding` controls the amount of implicit zero padding on both
+      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
+      below for details.
+
+    * :attr:`output_padding` controls the additional size added to one side
+      of the output shape. See note below for details.
+
+    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
+      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.
+
+    {groups_note}
+
+    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`
+    can either be:
+
+        - a single ``int`` -- in which case the same value is used for the height and width dimensions
+        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
+          and the second `int` for the width dimension
+
+    Note:
+        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
+        amount of zero padding to both sizes of the input. This is set so that
+        when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`
+        are initialized with same parameters, they are inverses of each other in
+        regard to the input and output shapes. However, when ``stride > 1``,
+        :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output
+        shape. :attr:`output_padding` is provided to resolve this ambiguity by
+        effectively increasing the calculated output shape on one side. Note
+        that :attr:`output_padding` is only used to find output shape, but does
+        not actually add zero-padding to output.
+
+    Note:
+        {cudnn_reproducibility_note}
+
+    Args:
+        in_channels (int): Number of channels in the input image
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
+            will be added to both sides of each dimension in the input. Default: 0
+        output_padding (int or tuple, optional): Additional size added to one side
+            of each dimension in the output shape. Default: 0
+        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+    """.format(**reproducibility_notes, **convolution_notes) + r"""
+
+    Shape:
+        - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`
+        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where
+
+        .. math::
+              H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
+                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
+        .. math::
+              W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
+                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1
+
+    Attributes:
+        weight (Tensor): the learnable weights of the module of shape
+                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
+                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.
+                         The values of these weights are sampled from
+                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
+        bias (Tensor):   the learnable bias of the module of shape (out_channels)
+                         If :attr:`bias` is ``True``, then the values of these weights are
+                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
+
+    Examples::
+
+        >>> # With square kernels and equal stride
+        >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)
+        >>> # non-square kernels and unequal stride and with padding
+        >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
+        >>> input = torch.randn(20, 16, 50, 100)
+        >>> output = m(input)
+        >>> # exact output size can be also specified as an argument
+        >>> input = torch.randn(1, 16, 12, 12)
+        >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
+        >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
+        >>> h = downsample(input)
+        >>> h.size()
+        torch.Size([1, 16, 6, 6])
+        >>> output = upsample(h, output_size=input.size())
+        >>> output.size()
+        torch.Size([1, 16, 12, 12])
+
+    .. _`here`:
+        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
+
+    .. _`Deconvolutional Networks`:
+        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_2_t,
+        stride: _size_2_t = 1,
+        padding: _size_2_t = 0,
+        output_padding: _size_2_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: int = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        kernel_size = _pair(kernel_size)
+        stride = _pair(stride)
+        padding = _pair(padding)
+        dilation = _pair(dilation)
+        output_padding = _pair(output_padding)
+        super(ConvTranspose2d, self).__init__(
+            in_channels, out_channels, kernel_size, stride, padding, dilation,
+            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
+
+    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
+        if self.padding_mode != 'zeros':
+            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose2d')
+
+        assert isinstance(self.padding, tuple)
+        # One cannot replace List by Tuple or Sequence in "_output_padding" because
+        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
+        output_padding = self._output_padding(
+            input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]
+
+        return F.conv_transpose2d(
+            input, self.weight, self.bias, self.stride, self.padding,
+            output_padding, self.groups, self.dilation)
+
+
+class ConvTranspose3d(_ConvTransposeNd):
+    __doc__ = r"""Applies a 3D transposed convolution operator over an input image composed of several input
+    planes.
+    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
+    and sums over the outputs from all input feature planes.
+
+    This module can be seen as the gradient of Conv3d with respect to its input.
+    It is also known as a fractionally-strided convolution or
+    a deconvolution (although it is not an actual deconvolution operation as it does
+    not compute a true inverse of convolution). For more information, see the visualizations
+    `here`_ and the `Deconvolutional Networks`_ paper.
+
+    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.
+
+    * :attr:`stride` controls the stride for the cross-correlation.
+
+    * :attr:`padding` controls the amount of implicit zero padding on both
+      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
+      below for details.
+
+    * :attr:`output_padding` controls the additional size added to one side
+      of the output shape. See note below for details.
+
+    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
+      It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.
+
+    {groups_note}
+
+    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`
+    can either be:
+
+        - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions
+        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
+          the second `int` for the height dimension and the third `int` for the width dimension
+
+    Note:
+        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
+        amount of zero padding to both sizes of the input. This is set so that
+        when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`
+        are initialized with same parameters, they are inverses of each other in
+        regard to the input and output shapes. However, when ``stride > 1``,
+        :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output
+        shape. :attr:`output_padding` is provided to resolve this ambiguity by
+        effectively increasing the calculated output shape on one side. Note
+        that :attr:`output_padding` is only used to find output shape, but does
+        not actually add zero-padding to output.
+
+    Note:
+        {cudnn_reproducibility_note}
+
+    Args:
+        in_channels (int): Number of channels in the input image
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
+            will be added to both sides of each dimension in the input. Default: 0
+        output_padding (int or tuple, optional): Additional size added to one side
+            of each dimension in the output shape. Default: 0
+        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+    """.format(**reproducibility_notes, **convolution_notes) + r"""
+
+    Shape:
+        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` or :math:`(C_{in}, D_{in}, H_{in}, W_{in})`
+        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` or
+          :math:`(C_{out}, D_{out}, H_{out}, W_{out})`, where
+
+        .. math::
+              D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
+                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
+        .. math::
+              H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
+                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1
+        .. math::
+              W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
+                        \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1
+
+
+    Attributes:
+        weight (Tensor): the learnable weights of the module of shape
+                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
+                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.
+                         The values of these weights are sampled from
+                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
+        bias (Tensor):   the learnable bias of the module of shape (out_channels)
+                         If :attr:`bias` is ``True``, then the values of these weights are
+                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
+                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
+
+    Examples::
+
+        >>> # With square kernels and equal stride
+        >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2)
+        >>> # non-square kernels and unequal stride and with padding
+        >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
+        >>> input = torch.randn(20, 16, 10, 50, 100)
+        >>> output = m(input)
+
+    .. _`here`:
+        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
+
+    .. _`Deconvolutional Networks`:
+        https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf
+    """
+
+    def __init__(
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: _size_3_t,
+        stride: _size_3_t = 1,
+        padding: _size_3_t = 0,
+        output_padding: _size_3_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: _size_3_t = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        kernel_size = _triple(kernel_size)
+        stride = _triple(stride)
+        padding = _triple(padding)
+        dilation = _triple(dilation)
+        output_padding = _triple(output_padding)
+        super(ConvTranspose3d, self).__init__(
+            in_channels, out_channels, kernel_size, stride, padding, dilation,
+            True, output_padding, groups, bias, padding_mode, **factory_kwargs)
+
+    def forward(self, input: Tensor, output_size: Optional[List[int]] = None) -> Tensor:
+        if self.padding_mode != 'zeros':
+            raise ValueError('Only `zeros` padding mode is supported for ConvTranspose3d')
+
+        assert isinstance(self.padding, tuple)
+        # One cannot replace List by Tuple or Sequence in "_output_padding" because
+        # TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.
+        output_padding = self._output_padding(
+            input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]
+
+        return F.conv_transpose3d(
+            input.to(torch.bfloat16), self.weight.to(torch.bfloat16), self.bias, self.stride, self.padding,
+            output_padding, self.groups, self.dilation).float()
+
+
+# TODO: Deprecate and remove the following alias `_ConvTransposeMixin`.
+#
+# `_ConvTransposeMixin` was a mixin that was removed.  It is meant to be used
+# with `_ConvNd` to construct actual module classes that implements conv
+# transpose ops:
+#
+#   class MyConvTranspose(_ConvNd, _ConvTransposeMixin):
+#       ...
+#
+# In PyTorch, it has been replaced by `_ConvTransposeNd`, which is a proper
+# subclass of `_ConvNd`.  However, some user code in the wild still (incorrectly)
+# use the internal class `_ConvTransposeMixin`.  Hence, we provide this alias
+# for BC, because it is cheap and easy for us to do so, even though that
+# `_ConvTransposeNd` is really not a mixin anymore (but multiple inheritance as
+# above would still work).
+class _ConvTransposeMixin(_ConvTransposeNd):
+    def __init__(self, *args, **kwargs):
+        warnings.warn(
+            "_ConvTransposeMixin is a deprecated internal class. "
+            "Please consider using public APIs.")
+        super(_ConvTransposeMixin, self).__init__(*args, **kwargs)
+
+
+# TODO: Conv2dLocal
+# TODO: Conv2dMap
+# TODO: ConvTranspose2dMap
+
+
+class _LazyConvXdMixin(LazyModuleMixin):
+    groups: int
+    transposed: bool
+    in_channels: int
+    out_channels: int
+    kernel_size: Tuple[int, ...]
+    weight: UninitializedParameter
+    bias: UninitializedParameter
+
+    def reset_parameters(self) -> None:
+        # has_uninitialized_params is defined in parent class and it is using a protocol on self
+        if not self.has_uninitialized_params() and self.in_channels != 0:  # type: ignore[misc]
+            # "type:ignore[..]" is required because mypy thinks that "reset_parameters" is undefined
+            # in super class. Turns out that it is defined in _ConvND which is inherited by any class
+            # that also inherits _LazyConvXdMixin
+            super().reset_parameters()  # type: ignore[misc]
+
+    # Signature of "initialize_parameters" is incompatible with the definition in supertype LazyModuleMixin
+    def initialize_parameters(self, input) -> None:  # type: ignore[override]
+        # defined by parent class but using a protocol
+        if self.has_uninitialized_params():  # type: ignore[misc]
+            self.in_channels = self._get_in_channels(input)
+            if self.in_channels % self.groups != 0:
+                raise ValueError('in_channels must be divisible by groups')
+            assert isinstance(self.weight, UninitializedParameter)
+            if self.transposed:
+                self.weight.materialize((
+                    self.in_channels, self.out_channels // self.groups, *self.kernel_size))
+            else:
+                self.weight.materialize((
+                    self.out_channels, self.in_channels // self.groups, *self.kernel_size))
+            if self.bias is not None:
+                assert isinstance(self.bias, UninitializedParameter)
+                self.bias.materialize((self.out_channels,))
+            self.reset_parameters()
+
+    # Function to extract in_channels from first input.
+    def _get_in_channels(self, input: Tensor) -> int:
+        num_spatial_dims = self._get_num_spatial_dims()
+        num_dims_no_batch = num_spatial_dims + 1  # +1 for channels dim
+        num_dims_batch = num_dims_no_batch + 1
+        if input.dim() not in (num_dims_no_batch, num_dims_batch):
+            raise RuntimeError("Expected {}D (unbatched) or {}D (batched) input to {}, but "
+                               "got input of size: {}".format(num_dims_no_batch, num_dims_batch,
+                                                              self.__class__.__name__, input.shape))
+        return input.shape[1] if input.dim() == num_dims_batch else input.shape[0]
+
+    # Function to return the number of spatial dims expected for inputs to the module.
+    # This is expected to be implemented by subclasses.
+    def _get_num_spatial_dims(self) -> int:
+        raise NotImplementedError()
+
+
+# LazyConv1d defines weight as a Tensor but derived class defines it as UnitializeParameter
+class LazyConv1d(_LazyConvXdMixin, Conv1d):  # type: ignore[misc]
+    r"""A :class:`torch.nn.Conv1d` module with lazy initialization of
+    the ``in_channels`` argument of the :class:`Conv1d` that is inferred from
+    the ``input.size(1)``.
+    The attributes that will be lazily initialized are `weight` and `bias`.
+
+    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
+    on lazy modules and their limitations.
+
+    Args:
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): Zero-padding added to both sides of
+            the input. Default: 0
+        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
+            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
+        dilation (int or tuple, optional): Spacing between kernel
+            elements. Default: 1
+        groups (int, optional): Number of blocked connections from input
+            channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the
+            output. Default: ``True``
+
+    .. seealso:: :class:`torch.nn.Conv1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
+    """
+
+    # super class define this variable as None. "type: ignore[..] is required
+    # since we are redefining the variable.
+    cls_to_become = Conv1d  # type: ignore[assignment]
+
+    def __init__(
+        self,
+        out_channels: int,
+        kernel_size: _size_1_t,
+        stride: _size_1_t = 1,
+        padding: _size_1_t = 0,
+        dilation: _size_1_t = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super().__init__(
+            0,
+            0,
+            kernel_size,
+            stride,
+            padding,
+            dilation,
+            groups,
+            # bias is hardcoded to False to avoid creating tensor
+            # that will soon be overwritten.
+            False,
+            padding_mode,
+            **factory_kwargs
+        )
+        self.weight = UninitializedParameter(**factory_kwargs)
+        self.out_channels = out_channels
+        if bias:
+            self.bias = UninitializedParameter(**factory_kwargs)
+
+    def _get_num_spatial_dims(self) -> int:
+        return 1
+
+
+# LazyConv2d defines weight as a Tensor but derived class defines it as UnitializeParameter
+class LazyConv2d(_LazyConvXdMixin, Conv2d):  # type: ignore[misc]
+    r"""A :class:`torch.nn.Conv2d` module with lazy initialization of
+    the ``in_channels`` argument of the :class:`Conv2d` that is inferred from
+    the ``input.size(1)``.
+    The attributes that will be lazily initialized are `weight` and `bias`.
+
+    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
+    on lazy modules and their limitations.
+
+    Args:
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): Zero-padding added to both sides of
+            the input. Default: 0
+        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
+            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
+        dilation (int or tuple, optional): Spacing between kernel
+            elements. Default: 1
+        groups (int, optional): Number of blocked connections from input
+            channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the
+            output. Default: ``True``
+
+    .. seealso:: :class:`torch.nn.Conv2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
+    """
+
+    # super class define this variable as None. "type: ignore[..] is required
+    # since we are redefining the variable.
+    cls_to_become = Conv2d  # type: ignore[assignment]
+
+    def __init__(
+        self,
+        out_channels: int,
+        kernel_size: _size_2_t,
+        stride: _size_2_t = 1,
+        padding: _size_2_t = 0,
+        dilation: _size_2_t = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = 'zeros',  # TODO: refine this type
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super().__init__(
+            0,
+            0,
+            kernel_size,
+            stride,
+            padding,
+            dilation,
+            groups,
+            # bias is hardcoded to False to avoid creating tensor
+            # that will soon be overwritten.
+            False,
+            padding_mode,
+            **factory_kwargs
+        )
+        self.weight = UninitializedParameter(**factory_kwargs)
+        self.out_channels = out_channels
+        if bias:
+            self.bias = UninitializedParameter(**factory_kwargs)
+
+    def _get_num_spatial_dims(self) -> int:
+        return 2
+
+
+# LazyConv3d defines weight as a Tensor but derived class defines it as UnitializeParameter
+class LazyConv3d(_LazyConvXdMixin, Conv3d):  # type: ignore[misc]
+    r"""A :class:`torch.nn.Conv3d` module with lazy initialization of
+    the ``in_channels`` argument of the :class:`Conv3d` that is inferred from
+    the ``input.size(1)``.
+    The attributes that will be lazily initialized are `weight` and `bias`.
+
+    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
+    on lazy modules and their limitations.
+
+    Args:
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): Zero-padding added to both sides of
+            the input. Default: 0
+        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
+            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
+        dilation (int or tuple, optional): Spacing between kernel
+            elements. Default: 1
+        groups (int, optional): Number of blocked connections from input
+            channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the
+            output. Default: ``True``
+
+    .. seealso:: :class:`torch.nn.Conv3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
+    """
+
+    # super class define this variable as None. "type: ignore[..] is required
+    # since we are redefining the variable.
+    cls_to_become = Conv3d  # type: ignore[assignment]
+
+    def __init__(
+        self,
+        out_channels: int,
+        kernel_size: _size_3_t,
+        stride: _size_3_t = 1,
+        padding: _size_3_t = 0,
+        dilation: _size_3_t = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super().__init__(
+            0,
+            0,
+            kernel_size,
+            stride,
+            padding,
+            dilation,
+            groups,
+            # bias is hardcoded to False to avoid creating tensor
+            # that will soon be overwritten.
+            False,
+            padding_mode,
+            **factory_kwargs
+        )
+        self.weight = UninitializedParameter(**factory_kwargs)
+        self.out_channels = out_channels
+        if bias:
+            self.bias = UninitializedParameter(**factory_kwargs)
+
+    def _get_num_spatial_dims(self) -> int:
+        return 3
+
+
+# LazyConvTranspose1d defines weight as a Tensor but derived class defines it as UnitializeParameter
+class LazyConvTranspose1d(_LazyConvXdMixin, ConvTranspose1d):  # type: ignore[misc]
+    r"""A :class:`torch.nn.ConvTranspose1d` module with lazy initialization of
+    the ``in_channels`` argument of the :class:`ConvTranspose1d` that is inferred from
+    the ``input.size(1)``.
+    The attributes that will be lazily initialized are `weight` and `bias`.
+
+    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
+    on lazy modules and their limitations.
+
+    Args:
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
+            will be added to both sides of the input. Default: 0
+        output_padding (int or tuple, optional): Additional size added to one side
+            of the output shape. Default: 0
+        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+
+    .. seealso:: :class:`torch.nn.ConvTranspose1d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
+    """
+
+    # super class define this variable as None. "type: ignore[..] is required
+    # since we are redefining the variable.
+    cls_to_become = ConvTranspose1d  # type: ignore[assignment]
+
+    def __init__(
+        self,
+        out_channels: int,
+        kernel_size: _size_1_t,
+        stride: _size_1_t = 1,
+        padding: _size_1_t = 0,
+        output_padding: _size_1_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: _size_1_t = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super().__init__(
+            0,
+            0,
+            kernel_size,
+            stride,
+            padding,
+            output_padding,
+            groups,
+            # bias is hardcoded to False to avoid creating tensor
+            # that will soon be overwritten.
+            False,
+            dilation,
+            padding_mode,
+            **factory_kwargs
+        )
+        self.weight = UninitializedParameter(**factory_kwargs)
+        self.out_channels = out_channels
+        if bias:
+            self.bias = UninitializedParameter(**factory_kwargs)
+
+    def _get_num_spatial_dims(self) -> int:
+        return 1
+
+
+# LazyConvTranspose2d defines weight as a Tensor but derived class defines it as UnitializeParameter
+class LazyConvTranspose2d(_LazyConvXdMixin, ConvTranspose2d):  # type: ignore[misc]
+    r"""A :class:`torch.nn.ConvTranspose2d` module with lazy initialization of
+    the ``in_channels`` argument of the :class:`ConvTranspose2d` that is inferred from
+    the ``input.size(1)``.
+    The attributes that will be lazily initialized are `weight` and `bias`.
+
+    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
+    on lazy modules and their limitations.
+
+    Args:
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
+            will be added to both sides of each dimension in the input. Default: 0
+        output_padding (int or tuple, optional): Additional size added to one side
+            of each dimension in the output shape. Default: 0
+        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+
+    .. seealso:: :class:`torch.nn.ConvTranspose2d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
+    """
+
+    # super class define this variable as None. "type: ignore[..] is required
+    # since we are redefining the variable.
+    cls_to_become = ConvTranspose2d  # type: ignore[assignment]
+
+    def __init__(
+        self,
+        out_channels: int,
+        kernel_size: _size_2_t,
+        stride: _size_2_t = 1,
+        padding: _size_2_t = 0,
+        output_padding: _size_2_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: int = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super().__init__(
+            0,
+            0,
+            kernel_size,
+            stride,
+            padding,
+            output_padding,
+            groups,
+            # bias is hardcoded to False to avoid creating tensor
+            # that will soon be overwritten.
+            False,
+            dilation,
+            padding_mode,
+            **factory_kwargs
+        )
+        self.weight = UninitializedParameter(**factory_kwargs)
+        self.out_channels = out_channels
+        if bias:
+            self.bias = UninitializedParameter(**factory_kwargs)
+
+    def _get_num_spatial_dims(self) -> int:
+        return 2
+
+
+# LazyConvTranspose3d defines weight as a Tensor but derived class defines it as UnitializeParameter
+class LazyConvTranspose3d(_LazyConvXdMixin, ConvTranspose3d):  # type: ignore[misc]
+    r"""A :class:`torch.nn.ConvTranspose3d` module with lazy initialization of
+    the ``in_channels`` argument of the :class:`ConvTranspose3d` that is inferred from
+    the ``input.size(1)``.
+    The attributes that will be lazily initialized are `weight` and `bias`.
+
+    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation
+    on lazy modules and their limitations.
+
+    Args:
+        out_channels (int): Number of channels produced by the convolution
+        kernel_size (int or tuple): Size of the convolving kernel
+        stride (int or tuple, optional): Stride of the convolution. Default: 1
+        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
+            will be added to both sides of each dimension in the input. Default: 0
+        output_padding (int or tuple, optional): Additional size added to one side
+            of each dimension in the output shape. Default: 0
+        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
+        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
+        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
+
+    .. seealso:: :class:`torch.nn.ConvTranspose3d` and :class:`torch.nn.modules.lazy.LazyModuleMixin`
+    """
+
+    # super class define this variable as None. "type: ignore[..] is required
+    # since we are redefining the variable.
+    cls_to_become = ConvTranspose3d  # type: ignore[assignment]
+
+    def __init__(
+        self,
+        out_channels: int,
+        kernel_size: _size_3_t,
+        stride: _size_3_t = 1,
+        padding: _size_3_t = 0,
+        output_padding: _size_3_t = 0,
+        groups: int = 1,
+        bias: bool = True,
+        dilation: _size_3_t = 1,
+        padding_mode: str = 'zeros',
+        device=None,
+        dtype=None
+    ) -> None:
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        super().__init__(
+            0,
+            0,
+            kernel_size,
+            stride,
+            padding,
+            output_padding,
+            groups,
+            # bias is hardcoded to False to avoid creating tensor
+            # that will soon be overwritten.
+            False,
+            dilation,
+            padding_mode,
+            **factory_kwargs
+        )
+        self.weight = UninitializedParameter(**factory_kwargs)
+        self.out_channels = out_channels
+        if bias:
+            self.bias = UninitializedParameter(**factory_kwargs)
+
+    def _get_num_spatial_dims(self) -> int:
+        return 3
\ No newline at end of file
diff --git a/projects/__pycache__/__init__.cpython-37.pyc b/projects/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 28859a6..0000000
Binary files a/projects/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/__pycache__/__init__.cpython-38.pyc b/projects/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 7988dbf..0000000
Binary files a/projects/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/configs/surroundocc/surroundocc.py b/projects/configs/surroundocc/surroundocc.py
index f921741..7efaabc 100644
--- a/projects/configs/surroundocc/surroundocc.py
+++ b/projects/configs/surroundocc/surroundocc.py
@@ -16,7 +16,7 @@ use_semantic = True
 img_norm_cfg = dict(
     mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
 
-class_names =  ['barrier','bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle',
+class_names =  ['barrier', 'bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle',
                 'pedestrian', 'traffic_cone', 'trailer', 'truck', 'driveable_surface',
                 'other_flat', 'sidewalk', 'terrain', 'manmade','vegetation']
 
@@ -43,7 +43,7 @@ model = dict(
        type='ResNet',
        depth=101,
        num_stages=4,
-       out_indices=(1,2,3),
+       out_indices=(1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
@@ -69,7 +69,7 @@ model = dict(
         conv_input=[_dim_[2], 256, _dim_[1], 128, _dim_[0], 64, 64],
         conv_output=[256, _dim_[1], 128, _dim_[0], 64, 64, 32],
         out_indices=[0, 2, 4, 6],
-        upsample_strides=[1,2,1,2,1,2,1],
+        upsample_strides=[1, 2, 1, 2, 1, 2, 1],
         embed_dims=_dim_,
         img_channels=[512, 512, 512],
         use_semantic=use_semantic,
@@ -125,7 +125,7 @@ test_pipeline = [
     dict(type='NormalizeMultiviewImage', **img_norm_cfg),
     dict(type='PadMultiViewImage', size_divisor=32),
     dict(type='DefaultFormatBundle3D', class_names=class_names, with_label=False),
-    dict(type='CustomCollect3D', keys=['img','gt_occ'])
+    dict(type='CustomCollect3D', keys=['img', 'gt_occ'])
 ]
 
 find_unused_parameters = True
@@ -168,7 +168,7 @@ data = dict(
 )
 
 optimizer = dict(
-    type='AdamW',
+    type='NpuFusedAdamW',
     lr=2e-4,
     paramwise_cfg=dict(
         custom_keys={
@@ -176,7 +176,8 @@ optimizer = dict(
         }),
     weight_decay=0.01)
 
-optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
+optimizer_config = dict(type='GradientCumulativeOptimizerHook', cumulative_iters=2, grad_clip=dict(max_norm=35, norm_type=2))
+
 # learning policy
 lr_config = dict(
     policy='CosineAnnealing',
@@ -184,13 +185,13 @@ lr_config = dict(
     warmup_iters=500,
     warmup_ratio=1.0 / 3,
     min_lr_ratio=1e-3)
-total_epochs = 24
+total_epochs = 15
 evaluation = dict(interval=1, pipeline=test_pipeline)
 
 runner = dict(type='EpochBasedRunner', max_epochs=total_epochs)
 load_from = 'ckpts/r101_dcn_fcos3d_pretrain.pth'
 log_config = dict(
-    interval=50,
+    interval=1,
     hooks=[
         dict(type='TextLoggerHook'),
         dict(type='TensorboardLoggerHook')
diff --git a/projects/configs/surroundocc/surroundocc_inference.py b/projects/configs/surroundocc/surroundocc_inference.py
index 2d514ed..2533799 100644
--- a/projects/configs/surroundocc/surroundocc_inference.py
+++ b/projects/configs/surroundocc/surroundocc_inference.py
@@ -16,9 +16,9 @@ use_semantic = True
 img_norm_cfg = dict(
     mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
 
-class_names =  ['barrier','bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle',
+class_names =  ['barrier', 'bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle',
                 'pedestrian', 'traffic_cone', 'trailer', 'truck', 'driveable_surface',
-                'other_flat', 'sidewalk', 'terrain', 'manmade','vegetation']
+                'other_flat', 'sidewalk', 'terrain', 'manmade', 'vegetation']
 
 input_modality = dict(
     use_lidar=False,
@@ -44,7 +44,7 @@ model = dict(
        type='ResNet',
        depth=101,
        num_stages=4,
-       out_indices=(1,2,3),
+       out_indices=(1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
@@ -70,7 +70,7 @@ model = dict(
         conv_input=[_dim_[2], 256, _dim_[1], 128, _dim_[0], 64, 64],
         conv_output=[256, _dim_[1], 128, _dim_[0], 64, 64, 32],
         out_indices=[0, 2, 4, 6],
-        upsample_strides=[1,2,1,2,1,2,1],
+        upsample_strides=[1, 2, 1, 2, 1, 2, 1],
         embed_dims=_dim_,
         img_channels=[512, 512, 512],
         use_semantic=use_semantic,
diff --git a/projects/configs/surroundocc/surroundocc_nosemantic.py b/projects/configs/surroundocc/surroundocc_nosemantic.py
index 2f3b790..46d19e8 100644
--- a/projects/configs/surroundocc/surroundocc_nosemantic.py
+++ b/projects/configs/surroundocc/surroundocc_nosemantic.py
@@ -16,9 +16,9 @@ use_semantic = False
 img_norm_cfg = dict(
     mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
 
-class_names =  ['barrier','bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle',
+class_names =  ['barrier', 'bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle',
                 'pedestrian', 'traffic_cone', 'trailer', 'truck', 'driveable_surface',
-                'other_flat', 'sidewalk', 'terrain', 'manmade','vegetation']
+                'other_flat', 'sidewalk', 'terrain', 'manmade', 'vegetation']
 
 input_modality = dict(
     use_lidar=False,
@@ -43,7 +43,7 @@ model = dict(
        type='ResNet',
        depth=101,
        num_stages=4,
-       out_indices=(1,2,3),
+       out_indices=(1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
@@ -69,7 +69,7 @@ model = dict(
         conv_input=[_dim_[2], 256, _dim_[1], 128, _dim_[0], 64, 64],
         conv_output=[256, _dim_[1], 128, _dim_[0], 64, 64, 32],
         out_indices=[0, 2, 4, 6],
-        upsample_strides=[1,2,1,2,1,2,1],
+        upsample_strides=[1, 2, 1, 2, 1, 2, 1],
         embed_dims=_dim_,
         img_channels=[512, 512, 512],
         use_semantic=use_semantic,
@@ -125,7 +125,7 @@ test_pipeline = [
     dict(type='NormalizeMultiviewImage', **img_norm_cfg),
     dict(type='PadMultiViewImage', size_divisor=32),
     dict(type='DefaultFormatBundle3D', class_names=class_names, with_label=False),
-    dict(type='CustomCollect3D', keys=['img','gt_occ'])
+    dict(type='CustomCollect3D', keys=['img', 'gt_occ'])
 ]
 
 find_unused_parameters = True
diff --git a/projects/configs/surroundocc/surroundocc_performance.py b/projects/configs/surroundocc/surroundocc_performance.py
new file mode 100644
index 0000000..30b39a0
--- /dev/null
+++ b/projects/configs/surroundocc/surroundocc_performance.py
@@ -0,0 +1,200 @@
+_base_ = [
+    '../datasets/custom_nus-3d.py',
+    '../_base_/default_runtime.py'
+]
+#
+plugin = True
+plugin_dir = 'projects/mmdet3d_plugin/'
+
+# If point cloud range is changed, the models should also change their point
+# cloud range accordingly
+point_cloud_range = [-50, -50, -5.0, 50, 50, 3.0]
+occ_size = [200, 200, 16]
+use_semantic = True
+
+
+img_norm_cfg = dict(
+    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
+
+class_names =  ['barrier', 'bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle',
+                'pedestrian', 'traffic_cone', 'trailer', 'truck', 'driveable_surface',
+                'other_flat', 'sidewalk', 'terrain', 'manmade', 'vegetation']
+
+input_modality = dict(
+    use_lidar=False,
+    use_camera=True,
+    use_radar=False,
+    use_map=False,
+    use_external=True)
+
+_dim_ = [128, 256, 512]
+_ffn_dim_ = [256, 512, 1024]
+volume_h_ = [100, 50, 25]
+volume_w_ = [100, 50, 25]
+volume_z_ = [8, 4, 2]
+_num_points_ = [2, 4, 8]
+_num_layers_ = [1, 3, 6]
+
+model = dict(
+    type='SurroundOcc',
+    use_grid_mask=True,
+    use_semantic=use_semantic,
+    img_backbone=dict(
+       type='ResNet',
+       depth=101,
+       num_stages=4,
+       out_indices=(1, 2, 3),
+       frozen_stages=1,
+       norm_cfg=dict(type='BN2d', requires_grad=False),
+       norm_eval=True,
+       style='caffe',
+       #with_cp=True, # using checkpoint to save GPU memory
+       dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False), # original DCNv2 will print log when perform load_state_dict
+       stage_with_dcn=(False, False, True, True)),
+    img_neck=dict(
+        type='FPN',
+        in_channels=[512, 1024, 2048],
+        out_channels=512,
+        start_level=0,
+        add_extra_convs='on_output',
+        num_outs=3,
+        relu_before_extra_convs=True),
+    pts_bbox_head=dict(
+        type='OccHead',
+        volume_h=volume_h_,
+        volume_w=volume_w_,
+        volume_z=volume_z_,
+        num_query=900,
+        num_classes=17,
+        conv_input=[_dim_[2], 256, _dim_[1], 128, _dim_[0], 64, 64],
+        conv_output=[256, _dim_[1], 128, _dim_[0], 64, 64, 32],
+        out_indices=[0, 2, 4, 6],
+        upsample_strides=[1, 2, 1, 2, 1, 2, 1],
+        embed_dims=_dim_,
+        img_channels=[512, 512, 512],
+        use_semantic=use_semantic,
+        transformer_template=dict(
+            type='PerceptionTransformer',
+            embed_dims=_dim_,
+            encoder=dict(
+                type='OccEncoder',
+                num_layers=_num_layers_,
+                pc_range=point_cloud_range,
+                return_intermediate=False,
+                transformerlayers=dict(
+                    type='OccLayer',
+                    attn_cfgs=[
+                        dict(
+                            type='SpatialCrossAttention',
+                            pc_range=point_cloud_range,
+                            deformable_attention=dict(
+                                type='MSDeformableAttention3D',
+                                embed_dims=_dim_,
+                                num_points=_num_points_,
+                                num_levels=1),
+                            embed_dims=_dim_,
+                        )
+                    ],
+                    feedforward_channels=_ffn_dim_,
+                    ffn_dropout=0.1,
+                    embed_dims=_dim_,
+                    conv_num=2,
+                    operation_order=('cross_attn', 'norm',
+                                     'ffn', 'norm', 'conv')))),
+),
+)
+
+dataset_type = 'CustomNuScenesOccDataset'
+data_root = 'data/nuscenes/'
+file_client_args = dict(backend='disk')
+
+
+train_pipeline = [
+    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
+    dict(type='PhotoMetricDistortionMultiViewImage'),
+    dict(type='LoadOccupancy', use_semantic=use_semantic),
+    dict(type='NormalizeMultiviewImage', **img_norm_cfg),
+    dict(type='PadMultiViewImage', size_divisor=32),
+    dict(type='DefaultFormatBundle3D', class_names=class_names, with_label=False),
+    dict(type='CustomCollect3D', keys=['img', 'gt_occ'])
+]
+
+test_pipeline = [
+    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
+    dict(type='LoadOccupancy', use_semantic=use_semantic),
+    dict(type='NormalizeMultiviewImage', **img_norm_cfg),
+    dict(type='PadMultiViewImage', size_divisor=32),
+    dict(type='DefaultFormatBundle3D', class_names=class_names, with_label=False),
+    dict(type='CustomCollect3D', keys=['img', 'gt_occ'])
+]
+
+find_unused_parameters = True
+data = dict(
+    samples_per_gpu=1,
+    workers_per_gpu=4,
+    train=dict(
+        type=dataset_type,
+        data_root=data_root,
+        ann_file='data/nuscenes_infos_train.pkl',
+        pipeline=train_pipeline,
+        modality=input_modality,
+        test_mode=False,
+        use_valid_flag=True,
+        occ_size=occ_size,
+        pc_range=point_cloud_range,
+        use_semantic=use_semantic,
+        classes=class_names,
+        box_type_3d='LiDAR'),
+    val=dict(type=dataset_type,
+             data_root=data_root,
+             ann_file='data/nuscenes_infos_val.pkl',
+             pipeline=test_pipeline,  
+             occ_size=occ_size,
+             pc_range=point_cloud_range,
+             use_semantic=use_semantic,
+             classes=class_names,
+             modality=input_modality),
+    test=dict(type=dataset_type,
+              data_root=data_root,
+              ann_file='data/nuscenes_infos_val.pkl',
+              pipeline=test_pipeline, 
+              occ_size=occ_size,
+              pc_range=point_cloud_range,
+              use_semantic=use_semantic,
+              classes=class_names,
+              modality=input_modality),
+    shuffler_sampler=dict(type='DistributedGroupSampler'),
+    nonshuffler_sampler=dict(type='DistributedSampler')
+)
+
+optimizer = dict(
+    type='NpuFusedAdamW',
+    lr=2e-4,
+    paramwise_cfg=dict(
+        custom_keys={
+            'img_backbone': dict(lr_mult=0.1),
+        }),
+    weight_decay=0.01)
+
+optimizer_config = dict(type='GradientCumulativeOptimizerHook', cumulative_iters=2, grad_clip=dict(max_norm=35, norm_type=2))
+
+# learning policy
+lr_config = dict(
+    policy='CosineAnnealing',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 3,
+    min_lr_ratio=1e-3)
+total_epochs = 1
+evaluation = dict(interval=1, pipeline=test_pipeline)
+
+runner = dict(type='EpochBasedRunner', max_epochs=total_epochs)
+load_from = 'ckpts/r101_dcn_fcos3d_pretrain.pth'
+log_config = dict(
+    interval=1,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        dict(type='TensorboardLoggerHook')
+    ])
+
+checkpoint_config = dict(interval=1)
diff --git a/projects/mmdet3d_plugin/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index f431b50..0000000
Binary files a/projects/mmdet3d_plugin/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 328b863..0000000
Binary files a/projects/mmdet3d_plugin/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/__pycache__/util.cpython-37.pyc b/projects/mmdet3d_plugin/core/bbox/__pycache__/util.cpython-37.pyc
deleted file mode 100644
index 3d404c2..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/__pycache__/util.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/__pycache__/util.cpython-38.pyc b/projects/mmdet3d_plugin/core/bbox/__pycache__/util.cpython-38.pyc
deleted file mode 100644
index b6807c0..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/__pycache__/util.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 47feabe..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index f99c57a..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/hungarian_assigner_3d.cpython-37.pyc b/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/hungarian_assigner_3d.cpython-37.pyc
deleted file mode 100644
index 9aa54c1..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/hungarian_assigner_3d.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/hungarian_assigner_3d.cpython-38.pyc b/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/hungarian_assigner_3d.cpython-38.pyc
deleted file mode 100644
index 4e9a93a..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/assigners/__pycache__/hungarian_assigner_3d.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index b9399de..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 0e86493..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/nms_free_coder.cpython-37.pyc b/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/nms_free_coder.cpython-37.pyc
deleted file mode 100644
index 07e2d8c..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/nms_free_coder.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/nms_free_coder.cpython-38.pyc b/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/nms_free_coder.cpython-38.pyc
deleted file mode 100644
index 209d850..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/coders/__pycache__/nms_free_coder.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index a950071..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index e538a19..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/match_cost.cpython-37.pyc b/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/match_cost.cpython-37.pyc
deleted file mode 100644
index 5e5bc22..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/match_cost.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/match_cost.cpython-38.pyc b/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/match_cost.cpython-38.pyc
deleted file mode 100644
index fc38ce2..0000000
Binary files a/projects/mmdet3d_plugin/core/bbox/match_costs/__pycache__/match_cost.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/evaluation/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/core/evaluation/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index eafa119..0000000
Binary files a/projects/mmdet3d_plugin/core/evaluation/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/evaluation/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/core/evaluation/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 2f18d50..0000000
Binary files a/projects/mmdet3d_plugin/core/evaluation/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/evaluation/__pycache__/eval_hooks.cpython-37.pyc b/projects/mmdet3d_plugin/core/evaluation/__pycache__/eval_hooks.cpython-37.pyc
deleted file mode 100644
index 6f120e9..0000000
Binary files a/projects/mmdet3d_plugin/core/evaluation/__pycache__/eval_hooks.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/core/evaluation/__pycache__/eval_hooks.cpython-38.pyc b/projects/mmdet3d_plugin/core/evaluation/__pycache__/eval_hooks.cpython-38.pyc
deleted file mode 100644
index ffaa77b..0000000
Binary files a/projects/mmdet3d_plugin/core/evaluation/__pycache__/eval_hooks.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 64c1b88..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 92be29d..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/builder.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/builder.cpython-37.pyc
deleted file mode 100644
index b5bcbf7..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/builder.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/builder.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/builder.cpython-38.pyc
deleted file mode 100644
index d3344b6..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/builder.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/evaluation_metrics.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/evaluation_metrics.cpython-37.pyc
deleted file mode 100644
index 9a90636..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/evaluation_metrics.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/kitti_dataset.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/kitti_dataset.cpython-37.pyc
deleted file mode 100644
index 625c838..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/kitti_dataset.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/kitti_metric.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/kitti_metric.cpython-37.pyc
deleted file mode 100644
index 42172c3..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/kitti_metric.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_dataset.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_dataset.cpython-37.pyc
deleted file mode 100644
index 02d79ca..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_dataset.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_dataset.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_dataset.cpython-38.pyc
deleted file mode 100644
index ae89b35..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_dataset.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_occupancy_dataset.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_occupancy_dataset.cpython-37.pyc
deleted file mode 100644
index 005c0ea..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_occupancy_dataset.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_occupancy_dataset.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_occupancy_dataset.cpython-38.pyc
deleted file mode 100644
index 8dc424a..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/nuscenes_occupancy_dataset.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/nuscnes_eval.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/nuscnes_eval.cpython-37.pyc
deleted file mode 100644
index 7e2941a..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/nuscnes_eval.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/__pycache__/nuscnes_eval.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/__pycache__/nuscnes_eval.cpython-38.pyc
deleted file mode 100644
index da93eb7..0000000
Binary files a/projects/mmdet3d_plugin/datasets/__pycache__/nuscnes_eval.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/evaluation_metrics.py b/projects/mmdet3d_plugin/datasets/evaluation_metrics.py
index 1b0f356..e0b8025 100644
--- a/projects/mmdet3d_plugin/datasets/evaluation_metrics.py
+++ b/projects/mmdet3d_plugin/datasets/evaluation_metrics.py
@@ -1,6 +1,20 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import numpy as np
 import torch
-import chamfer
+
 
 def voxel_to_vertices(voxel, img_metas, thresh=0.5):
     x = torch.linspace(0, voxel.shape[0] - 1, voxel.shape[0])
@@ -10,46 +24,51 @@ def voxel_to_vertices(voxel, img_metas, thresh=0.5):
     vv = torch.stack([X, Y, Z], dim=-1).to(voxel.device)
 
     vertices = vv[voxel > thresh]
-    vertices[:, 0] = (vertices[:, 0] + 0.5) * (img_metas['pc_range'][3] - img_metas['pc_range'][0]) /  img_metas['occ_size'][0]  + img_metas['pc_range'][0]
-    vertices[:, 1] = (vertices[:, 1] + 0.5) * (img_metas['pc_range'][4] - img_metas['pc_range'][1]) /  img_metas['occ_size'][1]  + img_metas['pc_range'][1]
-    vertices[:, 2] = (vertices[:, 2] + 0.5) * (img_metas['pc_range'][5] - img_metas['pc_range'][2]) /  img_metas['occ_size'][2]  + img_metas['pc_range'][2]
+    vertices[:, 0] = (vertices[:, 0] + 0.5) * (img_metas['pc_range'][3] - img_metas['pc_range'][0]) / img_metas['occ_size'][0] + img_metas['pc_range'][0]
+    vertices[:, 1] = (vertices[:, 1] + 0.5) * (img_metas['pc_range'][4] - img_metas['pc_range'][1]) / img_metas['occ_size'][1] + img_metas['pc_range'][1]
+    vertices[:, 2] = (vertices[:, 2] + 0.5) * (img_metas['pc_range'][5] - img_metas['pc_range'][2]) / img_metas['occ_size'][2] + img_metas['pc_range'][2]
 
     return vertices
 
+
 def gt_to_vertices(gt, img_metas):
-    gt[:, 0] = (gt[:, 0] + 0.5) * (img_metas['pc_range'][3] - img_metas['pc_range'][0]) /  img_metas['occ_size'][0]  + img_metas['pc_range'][0]
-    gt[:, 1] = (gt[:, 1] + 0.5) * (img_metas['pc_range'][4] - img_metas['pc_range'][1]) /  img_metas['occ_size'][1]  + img_metas['pc_range'][1]
-    gt[:, 2] = (gt[:, 2] + 0.5) * (img_metas['pc_range'][5] - img_metas['pc_range'][2]) /  img_metas['occ_size'][2]  + img_metas['pc_range'][2]
+    gt[:, 0] = (gt[:, 0] + 0.5) * (img_metas['pc_range'][3] - img_metas['pc_range'][0]) / img_metas['occ_size'][0] + img_metas['pc_range'][0]
+    gt[:, 1] = (gt[:, 1] + 0.5) * (img_metas['pc_range'][4] - img_metas['pc_range'][1]) / img_metas['occ_size'][1] + img_metas['pc_range'][1]
+    gt[:, 2] = (gt[:, 2] + 0.5) * (img_metas['pc_range'][5] - img_metas['pc_range'][2]) / img_metas['occ_size'][2] + img_metas['pc_range'][2]
     return gt
 
+
 def gt_to_voxel(gt, img_metas):
     voxel = np.zeros(img_metas['occ_size'])
     voxel[gt[:, 0].astype(np.int), gt[:, 1].astype(np.int), gt[:, 2].astype(np.int)] = gt[:, 3]
 
     return voxel
 
+
 def eval_3d(verts_pred, verts_trgt, threshold=.5):
     d1, d2, idx1, idx2 = chamfer.forward(verts_pred.unsqueeze(0).type(torch.float), verts_trgt.unsqueeze(0).type(torch.float))
     dist1 = torch.sqrt(d1).cpu().numpy()
     dist2 = torch.sqrt(d2).cpu().numpy()
     cd = dist1.mean() + dist2.mean()
-    precision = np.mean((dist1<threshold).astype('float'))
+    precision = np.mean((dist1 < threshold).astype('float'))
     recal = np.mean((dist2<threshold).astype('float'))
     fscore = 2 * precision * recal / (precision + recal)
-    metrics = np.array([np.mean(dist1),np.mean(dist2),cd, precision,recal,fscore])
+    metrics = np.array([np.mean(dist1), np.mean(dist2), cd, precision, recal, fscore])
     return metrics
 
+
 def evaluation_reconstruction(pred_occ, gt_occ, img_metas):
     results = []
     for i in range(pred_occ.shape[0]):
-        
-        vertices_pred = voxel_to_vertices(pred_occ[i], img_metas, thresh=0.25) #set low thresh for class imbalance problem
+        #set low thresh for class imbalance problem
+        vertices_pred = voxel_to_vertices(pred_occ[i], img_metas, thresh=0.25) 
         vertices_gt = gt_to_vertices(gt_occ[i][..., :3], img_metas)
-        
-        metrics = eval_3d(vertices_pred.type(torch.double), vertices_gt.type(torch.double)) #must convert to double, a bug in chamfer
+        #must convert to double, a bug in chamfer
+        metrics = eval_3d(vertices_pred.type(torch.double), vertices_gt.type(torch.double)) 
         results.append(metrics)
     return np.stack(results, axis=0)
 
+
 def evaluation_semantic(pred_occ, gt_occ, img_metas, class_num):
     results = []
 
@@ -59,7 +78,8 @@ def evaluation_semantic(pred_occ, gt_occ, img_metas, class_num):
         mask = (gt_i != 255)
         score = np.zeros((class_num, 3))
         for j in range(class_num):
-            if j == 0: #class 0 for geometry IoU
+            #class 0 for geometry IoU
+            if j == 0: 
                 score[j][0] += ((gt_i[mask] != 0) * (pred_i[mask] != 0)).sum()
                 score[j][1] += (gt_i[mask] != 0).sum()
                 score[j][2] += (pred_i[mask] != 0).sum()
diff --git a/projects/mmdet3d_plugin/datasets/nuscenes_occupancy_dataset.py b/projects/mmdet3d_plugin/datasets/nuscenes_occupancy_dataset.py
index ab976f4..9328afe 100644
--- a/projects/mmdet3d_plugin/datasets/nuscenes_occupancy_dataset.py
+++ b/projects/mmdet3d_plugin/datasets/nuscenes_occupancy_dataset.py
@@ -12,7 +12,6 @@ from nuscenes.eval.common.utils import quaternion_yaw, Quaternion
 from projects.mmdet3d_plugin.models.utils.visual import save_tensor
 from mmcv.parallel import DataContainer as DC
 import random
-import pdb, os
 
 
 @DATASETS.register_module()
@@ -70,8 +69,8 @@ class CustomNuScenesOccDataset(NuScenesDataset):
         # standard protocal modified from SECOND.Pytorch
         input_dict = dict(
             occ_path=info['occ_path'],
-            occ_size = np.array(self.occ_size),
-            pc_range = np.array(self.pc_range)
+            occ_size=np.array(self.occ_size),
+            pc_range=np.array(self.pc_range)
         )
 
         if self.modality['use_camera']:
@@ -217,7 +216,7 @@ class CustomNuScenesOccDataset(NuScenesDataset):
 
         else:
             results = np.stack(results, axis=0).mean(0)
-            results_dict={'Acc':results[0],
+            results_dict = {'Acc':results[0],
                           'Comp':results[1],
                           'CD':results[2],
                           'Prec':results[3],
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index b47512a..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index e1b9b6e..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/formating.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/formating.cpython-37.pyc
deleted file mode 100644
index 0cea597..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/formating.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/formating.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/formating.cpython-38.pyc
deleted file mode 100644
index b59bc3e..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/formating.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/loading.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/loading.cpython-37.pyc
deleted file mode 100644
index e1ace61..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/loading.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/loading.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/loading.cpython-38.pyc
deleted file mode 100644
index 7af0ef8..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/loading.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/transform_3d.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/transform_3d.cpython-37.pyc
deleted file mode 100644
index 96f4c11..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/transform_3d.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/transform_3d.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/transform_3d.cpython-38.pyc
deleted file mode 100644
index 7feba8b..0000000
Binary files a/projects/mmdet3d_plugin/datasets/pipelines/__pycache__/transform_3d.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/transform_3d.py b/projects/mmdet3d_plugin/datasets/pipelines/transform_3d.py
index b6ff218..783b0e3 100644
--- a/projects/mmdet3d_plugin/datasets/pipelines/transform_3d.py
+++ b/projects/mmdet3d_plugin/datasets/pipelines/transform_3d.py
@@ -344,7 +344,7 @@ class Augmentation(object):
         self.data_config = data_config
 
 
-    def get_rot(self,h):
+    def get_rot(self, h):
         return np.array([
             [np.cos(h), np.sin(h)],
             [-np.sin(h), np.cos(h)],
@@ -382,11 +382,11 @@ class Augmentation(object):
         
         return img
 
-    def sample_augmentation(self, H , W, flip=None, scale=None, decay_aug=False):
+    def sample_augmentation(self, H, W, flip=None, scale=None, decay_aug=False):
         fH, fW = self.data_config['input_size']
         
         if self.is_train and (not decay_aug):
-            resize = float(fW)/float(W)
+            resize = float(fW) / float(W)
             resize += np.random.uniform(*self.data_config['resize'])
             resize_dims = (int(W * resize), int(H * resize))
             newW, newH = resize_dims
@@ -398,7 +398,7 @@ class Augmentation(object):
             rotate = np.random.uniform(*self.data_config['rot'])
         
         else:
-            resize = float(fW)/float(W)
+            resize = float(fW) / float(W)
             resize += self.data_config.get('resize_test', 0.0)
             if scale is not None:
                 resize = scale
@@ -432,7 +432,7 @@ class Augmentation(object):
             resize, resize_dims, crop, flip, rotate = img_augs
             img, post_rot2, post_tran2 = \
                 self.img_transform(img, post_rot, post_tran, resize=resize, 
-                    resize_dims=resize_dims, crop=crop,flip=flip, rotate=rotate)
+                    resize_dims=resize_dims, crop=crop, flip=flip, rotate=rotate)
     
             # for convenience, make augmentation matrices 3x3
             post_rot = np.eye(4)
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 6ac3819..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 78794c3..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/distributed_sampler.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/distributed_sampler.cpython-37.pyc
deleted file mode 100644
index c209ce2..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/distributed_sampler.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/distributed_sampler.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/distributed_sampler.cpython-38.pyc
deleted file mode 100644
index 57c7a39..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/distributed_sampler.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/group_sampler.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/group_sampler.cpython-37.pyc
deleted file mode 100644
index 635436e..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/group_sampler.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/group_sampler.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/group_sampler.cpython-38.pyc
deleted file mode 100644
index a7161ef..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/group_sampler.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/sampler.cpython-37.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/sampler.cpython-37.pyc
deleted file mode 100644
index 651b3f9..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/sampler.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/sampler.cpython-38.pyc b/projects/mmdet3d_plugin/datasets/samplers/__pycache__/sampler.cpython-38.pyc
deleted file mode 100644
index c363f85..0000000
Binary files a/projects/mmdet3d_plugin/datasets/samplers/__pycache__/sampler.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/backbones/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/models/backbones/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index ff31dfe..0000000
Binary files a/projects/mmdet3d_plugin/models/backbones/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/backbones/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/models/backbones/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index d4565e3..0000000
Binary files a/projects/mmdet3d_plugin/models/backbones/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/backbones/__pycache__/efficientnet.cpython-37.pyc b/projects/mmdet3d_plugin/models/backbones/__pycache__/efficientnet.cpython-37.pyc
deleted file mode 100644
index 296c0d4..0000000
Binary files a/projects/mmdet3d_plugin/models/backbones/__pycache__/efficientnet.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/backbones/__pycache__/vovnet.cpython-37.pyc b/projects/mmdet3d_plugin/models/backbones/__pycache__/vovnet.cpython-37.pyc
deleted file mode 100644
index 350d38f..0000000
Binary files a/projects/mmdet3d_plugin/models/backbones/__pycache__/vovnet.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/backbones/__pycache__/vovnet.cpython-38.pyc b/projects/mmdet3d_plugin/models/backbones/__pycache__/vovnet.cpython-38.pyc
deleted file mode 100644
index 3c8f38d..0000000
Binary files a/projects/mmdet3d_plugin/models/backbones/__pycache__/vovnet.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/backbones/efficientnet.py b/projects/mmdet3d_plugin/models/backbones/efficientnet.py
index 5b0b47a..d7dd0fe 100644
--- a/projects/mmdet3d_plugin/models/backbones/efficientnet.py
+++ b/projects/mmdet3d_plugin/models/backbones/efficientnet.py
@@ -12,6 +12,7 @@ from mmcv.runner import BaseModule, Sequential
 from mmdet3d.models.builder import BACKBONES
 from mmdet.models.utils import SELayer, make_divisible
 
+
 class EdgeResidual(BaseModule):
     """Edge Residual Block.
     Args:
@@ -109,6 +110,7 @@ class EdgeResidual(BaseModule):
 
         return out
 
+
 class InvertedResidual(BaseModule):
     """Inverted Residual Block.
     Args:
@@ -227,10 +229,12 @@ class InvertedResidual(BaseModule):
             out = _inner_forward(x)
 
         return out
+        
 
 def model_scaling(layer_setting, arch_setting):
     """Scaling operation to the layer's parameters according to the
-    arch_setting."""
+    arch_setting.
+    """
     # scale width
     new_layer_setting = copy.deepcopy(layer_setting)
     for layer_cfg in new_layer_setting:
diff --git a/projects/mmdet3d_plugin/models/opt/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/models/opt/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 69b29d8..0000000
Binary files a/projects/mmdet3d_plugin/models/opt/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/opt/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/models/opt/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index e0f777a..0000000
Binary files a/projects/mmdet3d_plugin/models/opt/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/opt/__pycache__/adamw.cpython-37.pyc b/projects/mmdet3d_plugin/models/opt/__pycache__/adamw.cpython-37.pyc
deleted file mode 100644
index 6708e2a..0000000
Binary files a/projects/mmdet3d_plugin/models/opt/__pycache__/adamw.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/opt/__pycache__/adamw.cpython-38.pyc b/projects/mmdet3d_plugin/models/opt/__pycache__/adamw.cpython-38.pyc
deleted file mode 100644
index b591e50..0000000
Binary files a/projects/mmdet3d_plugin/models/opt/__pycache__/adamw.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index e62261e..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 0887ef6..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/bricks.cpython-37.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/bricks.cpython-37.pyc
deleted file mode 100644
index 357e40d..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/bricks.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/bricks.cpython-38.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/bricks.cpython-38.pyc
deleted file mode 100644
index c6013d9..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/bricks.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/grid_mask.cpython-37.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/grid_mask.cpython-37.pyc
deleted file mode 100644
index 1aa6865..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/grid_mask.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/grid_mask.cpython-38.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/grid_mask.cpython-38.pyc
deleted file mode 100644
index 98c6722..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/grid_mask.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/position_embedding.cpython-37.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/position_embedding.cpython-37.pyc
deleted file mode 100644
index 25375d1..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/position_embedding.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/position_embedding.cpython-38.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/position_embedding.cpython-38.pyc
deleted file mode 100644
index f1c04a7..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/position_embedding.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/visual.cpython-37.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/visual.cpython-37.pyc
deleted file mode 100644
index d06f57f..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/visual.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/__pycache__/visual.cpython-38.pyc b/projects/mmdet3d_plugin/models/utils/__pycache__/visual.cpython-38.pyc
deleted file mode 100644
index 20d8df7..0000000
Binary files a/projects/mmdet3d_plugin/models/utils/__pycache__/visual.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/models/utils/position_embedding.py b/projects/mmdet3d_plugin/models/utils/position_embedding.py
index dccb4f2..dbb20b4 100644
--- a/projects/mmdet3d_plugin/models/utils/position_embedding.py
+++ b/projects/mmdet3d_plugin/models/utils/position_embedding.py
@@ -36,6 +36,7 @@ class RelPositionEmbedding(nn.Module):
 from mmcv.cnn.bricks.transformer import POSITIONAL_ENCODING
 from mmcv.runner import BaseModule
 
+
 @POSITIONAL_ENCODING.register_module()
 class LearnedPositionalEncoding3D(BaseModule):
     """Position embedding with learnable embedding weights.
diff --git a/projects/mmdet3d_plugin/surroundocc/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index ecdac07..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 5f90edc..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index e906ee2..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 02f8174..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/mmdet_train.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/mmdet_train.cpython-37.pyc
deleted file mode 100644
index 28bf545..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/mmdet_train.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/mmdet_train.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/mmdet_train.cpython-38.pyc
deleted file mode 100644
index 402aa06..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/mmdet_train.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/test.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/test.cpython-37.pyc
deleted file mode 100644
index 5dbc4c7..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/test.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/test.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/test.cpython-38.pyc
deleted file mode 100644
index 6faf288..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/test.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/train.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/train.cpython-37.pyc
deleted file mode 100644
index f8e9e2d..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/train.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/train.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/train.cpython-38.pyc
deleted file mode 100644
index 76a4422..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/apis/__pycache__/train.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/mmdet_train.py b/projects/mmdet3d_plugin/surroundocc/apis/mmdet_train.py
index 108d6e4..c124f8e 100644
--- a/projects/mmdet3d_plugin/surroundocc/apis/mmdet_train.py
+++ b/projects/mmdet3d_plugin/surroundocc/apis/mmdet_train.py
@@ -1,3 +1,17 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # ---------------------------------------------
 # Copyright (c) OpenMMLab. All rights reserved.
 # ---------------------------------------------
@@ -9,7 +23,7 @@ import warnings
 import numpy as np
 import torch
 import torch.distributed as dist
-from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
                          Fp16OptimizerHook, OptimizerHook, build_optimizer,
                          build_runner, get_dist_info)
@@ -73,23 +87,23 @@ def custom_train_detector(model,
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
         print(torch.cuda.current_device(), cfg.gpu_ids)
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             #device_ids=[4,5,7],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
         if eval_model is not None:
-            eval_model = MMDistributedDataParallel(
+            eval_model = NPUDistributedDataParallel(
                 eval_model.cuda(),
                 device_ids=[torch.cuda.current_device()],
                 broadcast_buffers=False,
                 find_unused_parameters=find_unused_parameters)
     else:
-        model = MMDataParallel(
+        model = NPUDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
         if eval_model is not None:
-            eval_model = MMDataParallel(
+            eval_model = NPUDataParallel(
                 eval_model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
 
diff --git a/projects/mmdet3d_plugin/surroundocc/apis/test.py b/projects/mmdet3d_plugin/surroundocc/apis/test.py
index 551bb6a..61e5338 100644
--- a/projects/mmdet3d_plugin/surroundocc/apis/test.py
+++ b/projects/mmdet3d_plugin/surroundocc/apis/test.py
@@ -22,7 +22,7 @@ import mmcv
 import numpy as np
 import pycocotools.mask as mask_util
 #import open3d as o3d
-import pdb
+
 
 def custom_encode_mask_results(mask_results):
     """Encode bitmap mask to RLE code. Semantic Masks only
@@ -44,6 +44,7 @@ def custom_encode_mask_results(mask_results):
                         dtype='uint8'))[0])  # encoded with RLE
     return [encoded_mask_results]
 
+
 def custom_multi_gpu_test(model, data_loader, tmpdir=None, gpu_collect=False, is_vis=False):
     """Test model with multiple gpus.
     This method tests model with multiple gpus and collects the results
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index eaf3aa7..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 35c0acb..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevformer_head.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevformer_head.cpython-37.pyc
deleted file mode 100644
index b303186..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevformer_head.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevformer_head.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevformer_head.cpython-38.pyc
deleted file mode 100644
index c3f7509..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevformer_head.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevocc_head.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevocc_head.cpython-37.pyc
deleted file mode 100644
index fa08a7d..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevocc_head.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevocc_head.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevocc_head.cpython-38.pyc
deleted file mode 100644
index b450e35..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/bevocc_head.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/occ_head.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/occ_head.cpython-37.pyc
deleted file mode 100644
index 9bf7c6f..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/dense_heads/__pycache__/occ_head.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/dense_heads/occ_head.py b/projects/mmdet3d_plugin/surroundocc/dense_heads/occ_head.py
index a69fb4a..eeb3f64 100644
--- a/projects/mmdet3d_plugin/surroundocc/dense_heads/occ_head.py
+++ b/projects/mmdet3d_plugin/surroundocc/dense_heads/occ_head.py
@@ -22,9 +22,10 @@ from mmcv.cnn.utils.weight_init import constant_init
 import os
 from torch.autograd import Variable
 try:
-    from itertools import  ifilterfalse
+    from itertools import ifilterfalse
 except ImportError: # py3k
-    from itertools import  filterfalse as ifilterfalse
+    from itertools import filterfalse as ifilterfalse
+
 
 @HEADS.register_module()
 class OccHead(nn.Module): 
@@ -100,9 +101,9 @@ class OccHead(nn.Module):
         out_channels = self.conv_output
         in_channels = self.conv_input
 
-        norm_cfg=dict(type='GN', num_groups=16, requires_grad=True)
-        upsample_cfg=dict(type='deconv3d', bias=False)
-        conv_cfg=dict(type='Conv3d', bias=False)
+        norm_cfg = dict(type='GN', num_groups=16, requires_grad=True)
+        upsample_cfg = dict(type='deconv3d', bias=False)
+        conv_cfg = dict(type='Conv3d', bias=False)
 
         for i, out_channel in enumerate(out_channels):
             stride = upsample_strides[i]
@@ -159,8 +160,8 @@ class OccHead(nn.Module):
 
 
         self.transfer_conv = nn.ModuleList()
-        norm_cfg=dict(type='GN', num_groups=16, requires_grad=True)
-        conv_cfg=dict(type='Conv2d', bias=True)
+        norm_cfg = dict(type='GN', num_groups=16, requires_grad=True)
+        conv_cfg = dict(type='Conv2d', bias=True)
         for i in range(self.fpn_level):
             transfer_layer = build_conv_layer(
                     conv_cfg,
@@ -199,7 +200,7 @@ class OccHead(nn.Module):
             volume_z = self.volume_z[i]
 
             _, _, C, H, W = mlvl_feats[i].shape
-            view_features = self.transfer_conv[i](mlvl_feats[i].reshape(bs*num_cam, C, H, W)).reshape(bs, num_cam, -1, H, W)
+            view_features = self.transfer_conv[i](mlvl_feats[i].reshape(bs * num_cam, C, H, W)).reshape(bs, num_cam, -1, H, W)
 
             volume_embed_i = self.transformer[i](
                 [view_features],
@@ -268,8 +269,8 @@ class OccHead(nn.Module):
                 #gt = torch.mode(gt, dim=-1)[0].float()
                     
                 loss_occ_i = (F.binary_cross_entropy_with_logits(pred, gt) + geo_scal_loss(pred, gt.long(), semantic=False))
-                    
-                loss_occ_i =  loss_occ_i * ((0.5)**(len(preds_dicts['occ_preds']) - 1 -i)) #* focal_weight
+                #* focal_weight
+                loss_occ_i = loss_occ_i * ((0.5) ** (len(preds_dicts['occ_preds']) - 1 - i)) 
 
                 loss_dict['loss_occ_{}'.format(i)] = loss_occ_i
     
@@ -291,7 +292,7 @@ class OccHead(nn.Module):
 
                 loss_occ_i = (criterion(pred, gt.long()) + sem_scal_loss(pred, gt.long()) + geo_scal_loss(pred, gt.long()))
 
-                loss_occ_i = loss_occ_i * ((0.5)**(len(preds_dicts['occ_preds']) - 1 -i))
+                loss_occ_i = loss_occ_i * ((0.5) ** (len(preds_dicts['occ_preds']) - 1 - i))
 
                 loss_dict['loss_occ_{}'.format(i)] = loss_occ_i
 
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 6315f38..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index f27b05d..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer.cpython-37.pyc
deleted file mode 100644
index c0c4bdc..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer.cpython-38.pyc
deleted file mode 100644
index 0af8ea1..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer_fp16.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer_fp16.cpython-37.pyc
deleted file mode 100644
index 0216b58..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer_fp16.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer_fp16.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer_fp16.cpython-38.pyc
deleted file mode 100644
index 29c86fe..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevformer_fp16.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevocc.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevocc.cpython-37.pyc
deleted file mode 100644
index 190caa4..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevocc.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevocc.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevocc.cpython-38.pyc
deleted file mode 100644
index a13104b..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/bevocc.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/surroundocc.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/surroundocc.cpython-37.pyc
deleted file mode 100644
index c61da8f..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/detectors/__pycache__/surroundocc.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/detectors/surroundocc.py b/projects/mmdet3d_plugin/surroundocc/detectors/surroundocc.py
index 7145d1e..aba0994 100644
--- a/projects/mmdet3d_plugin/surroundocc/detectors/surroundocc.py
+++ b/projects/mmdet3d_plugin/surroundocc/detectors/surroundocc.py
@@ -21,7 +21,6 @@ from projects.mmdet3d_plugin.datasets.evaluation_metrics import evaluation_recon
 from sklearn.metrics import confusion_matrix as CM
 import time, yaml, os
 import torch.nn as nn
-import pdb
 
 
 @DETECTORS.register_module()
@@ -92,7 +91,7 @@ class SurroundOcc(MVXTwoStageDetector):
         for img_feat in img_feats:
             BN, C, H, W = img_feat.size()
             if len_queue is not None:
-                img_feats_reshaped.append(img_feat.view(int(B/len_queue), len_queue, int(BN / B), C, H, W))
+                img_feats_reshaped.append(img_feat.view(int(B / len_queue), len_queue, int(BN / B), C, H, W))
             else:
                 img_feats_reshaped.append(img_feat.view(B, int(BN / B), C, H, W))
         return img_feats_reshaped
@@ -232,9 +231,9 @@ class SurroundOcc(MVXTwoStageDetector):
             vv = torch.stack([X, Y, Z], dim=-1).to(voxel.device)
         
             vertices = vv[voxel[i] > 0.5]
-            vertices[:, 0] = (vertices[:, 0] + 0.5) * (img_metas[i]['pc_range'][3] - img_metas[i]['pc_range'][0]) /  img_metas[i]['occ_size'][0]  + img_metas[i]['pc_range'][0]
-            vertices[:, 1] = (vertices[:, 1] + 0.5) * (img_metas[i]['pc_range'][4] - img_metas[i]['pc_range'][1]) /  img_metas[i]['occ_size'][1]  + img_metas[i]['pc_range'][1]
-            vertices[:, 2] = (vertices[:, 2] + 0.5) * (img_metas[i]['pc_range'][5] - img_metas[i]['pc_range'][2]) /  img_metas[i]['occ_size'][2]  + img_metas[i]['pc_range'][2]
+            vertices[:, 0] = (vertices[:, 0] + 0.5) * (img_metas[i]['pc_range'][3] - img_metas[i]['pc_range'][0]) / img_metas[i]['occ_size'][0] + img_metas[i]['pc_range'][0]
+            vertices[:, 1] = (vertices[:, 1] + 0.5) * (img_metas[i]['pc_range'][4] - img_metas[i]['pc_range'][1]) / img_metas[i]['occ_size'][1] + img_metas[i]['pc_range'][1]
+            vertices[:, 2] = (vertices[:, 2] + 0.5) * (img_metas[i]['pc_range'][5] - img_metas[i]['pc_range'][2]) / img_metas[i]['occ_size'][2] + img_metas[i]['pc_range'][2]
             
             vertices = vertices.cpu().numpy()
     
@@ -252,8 +251,6 @@ class SurroundOcc(MVXTwoStageDetector):
 
             o3d.io.write_point_cloud(os.path.join(save_dir, 'pred.ply'), pcd)
             np.save(os.path.join(save_dir, 'pred.npy'), vertices)
-            for cam_id, cam_path in enumerate(img_metas[i]['filename']):
-                os.system('cp {} {}/{}.jpg'.format(cam_path, save_dir, cam_id))
 
 
     
diff --git a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index d1eaf05..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 5373887..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/custom_hooks.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/custom_hooks.cpython-37.pyc
deleted file mode 100644
index d93bf0c..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/custom_hooks.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/custom_hooks.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/custom_hooks.cpython-38.pyc
deleted file mode 100644
index e79a954..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/hooks/__pycache__/custom_hooks.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/loss/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/loss/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index e22289b..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/loss/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/loss/__pycache__/loss_utils.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/loss/__pycache__/loss_utils.cpython-37.pyc
deleted file mode 100644
index 7090cdd..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/loss/__pycache__/loss_utils.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/loss/loss_utils.py b/projects/mmdet3d_plugin/surroundocc/loss/loss_utils.py
index dd74441..843a4c0 100644
--- a/projects/mmdet3d_plugin/surroundocc/loss/loss_utils.py
+++ b/projects/mmdet3d_plugin/surroundocc/loss/loss_utils.py
@@ -1,7 +1,21 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-import pdb
+
 
 def multiscale_supervision(gt_occ, ratio, gt_shape):
     '''
@@ -11,10 +25,11 @@ def multiscale_supervision(gt_occ, ratio, gt_shape):
     gt = torch.zeros([gt_shape[0], gt_shape[2], gt_shape[3], gt_shape[4]]).to(gt_occ.device).type(torch.float) 
     for i in range(gt.shape[0]):
         coords = gt_occ[i][:, :3].type(torch.long) // ratio
-        gt[i, coords[:, 0], coords[:, 1], coords[:, 2]] =  gt_occ[i][:, 3]
+        gt[i, coords[:, 0], coords[:, 1], coords[:, 2]] = gt_occ[i][:, 3]
     
     return gt
 
+
 def geo_scal_loss(pred, ssc_target, semantic=True):
 
     # Get softmax probabilities
@@ -30,9 +45,9 @@ def geo_scal_loss(pred, ssc_target, semantic=True):
     # Remove unknown voxels
     mask = ssc_target != 255
     nonempty_target = ssc_target != 0
-    nonempty_target = nonempty_target[mask].float()
-    nonempty_probs = nonempty_probs[mask]
-    empty_probs = empty_probs[mask]
+    nonempty_target = torch.where(mask, nonempty_target, 0).float()
+    nonempty_probs = torch.where(mask, nonempty_probs, 0)
+    empty_probs = torch.where(mask, empty_probs, 0)
 
     intersection = (nonempty_target * nonempty_probs).sum()
     precision = intersection / nonempty_probs.sum()
@@ -59,13 +74,14 @@ def sem_scal_loss(pred, ssc_target):
 
         # Remove unknown voxels
         target_ori = ssc_target
-        p = p[mask]
-        target = ssc_target[mask]
 
+        p = torch.where(mask, p, 0)
+        target = torch.where(mask, ssc_target, i + 1)
         completion_target = torch.ones_like(target)
-        completion_target[target != i] = 0
-        completion_target_ori = torch.ones_like(target_ori).float()
-        completion_target_ori[target_ori != i] = 0
+        completion_target *= ~(target != i)
+        completion_target_ori = torch.ones_like(target_ori.to(torch.float))
+        completion_target_ori *= ~(target_ori != i)
+
         if torch.sum(completion_target) > 0:
             count += 1.0
             nominator = torch.sum(p * completion_target)
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/__init__.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 07c2324..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/__init__.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 8a7bc0a..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/backbone3d.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/backbone3d.cpython-37.pyc
deleted file mode 100644
index ede6618..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/backbone3d.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/custom_base_transformer_layer.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/custom_base_transformer_layer.cpython-37.pyc
deleted file mode 100644
index 0e0dc5b..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/custom_base_transformer_layer.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/custom_base_transformer_layer.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/custom_base_transformer_layer.cpython-38.pyc
deleted file mode 100644
index 95707d3..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/custom_base_transformer_layer.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/decoder.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/decoder.cpython-37.pyc
deleted file mode 100644
index b8cfe42..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/decoder.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/decoder.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/decoder.cpython-38.pyc
deleted file mode 100644
index 9cf7dde..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/decoder.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/encoder.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/encoder.cpython-37.pyc
deleted file mode 100644
index dd3cba0..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/encoder.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/encoder.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/encoder.cpython-38.pyc
deleted file mode 100644
index df38fd9..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/encoder.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/multi_scale_deformable_attn_function.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/multi_scale_deformable_attn_function.cpython-37.pyc
deleted file mode 100644
index 3a5d966..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/multi_scale_deformable_attn_function.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/multi_scale_deformable_attn_function.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/multi_scale_deformable_attn_function.cpython-38.pyc
deleted file mode 100644
index 71f26d1..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/multi_scale_deformable_attn_function.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/spatial_cross_attention.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/spatial_cross_attention.cpython-37.pyc
deleted file mode 100644
index 56aac97..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/spatial_cross_attention.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/spatial_cross_attention.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/spatial_cross_attention.cpython-38.pyc
deleted file mode 100644
index 1df5c70..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/spatial_cross_attention.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/temporal_self_attention.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/temporal_self_attention.cpython-37.pyc
deleted file mode 100644
index 8a544cd..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/temporal_self_attention.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/temporal_self_attention.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/temporal_self_attention.cpython-38.pyc
deleted file mode 100644
index a7d34c6..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/temporal_self_attention.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/transformer.cpython-37.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/transformer.cpython-37.pyc
deleted file mode 100644
index 28d30ff..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/transformer.cpython-37.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/transformer.cpython-38.pyc b/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/transformer.cpython-38.pyc
deleted file mode 100644
index a981ae9..0000000
Binary files a/projects/mmdet3d_plugin/surroundocc/modules/__pycache__/transformer.cpython-38.pyc and /dev/null differ
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/encoder.py b/projects/mmdet3d_plugin/surroundocc/modules/encoder.py
index 24d8e53..bdd4aeb 100644
--- a/projects/mmdet3d_plugin/surroundocc/modules/encoder.py
+++ b/projects/mmdet3d_plugin/surroundocc/modules/encoder.py
@@ -1,3 +1,16 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 # ---------------------------------------------
 # Copyright (c) OpenMMLab. All rights reserved.
@@ -21,7 +34,6 @@ import cv2 as cv
 import mmcv
 from mmcv.utils import TORCH_VERSION, digit_version
 from mmcv.utils import ext_loader
-import pdb
 import torch.nn.functional as F
 import torch.nn as nn
 from mmcv.cnn import build_conv_layer, build_norm_layer, build_upsample_layer
@@ -73,7 +85,6 @@ class OccEncoder(TransformerLayerSequence):
         ref_3d = ref_3d[None, None].repeat(bs, 1, 1, 1)
         return ref_3d
 
-
     # This function must use fp32!!!
     @force_fp32(apply_to=('reference_points', 'img_metas'))
     def point_sampling(self, reference_points, pc_range,  img_metas):
@@ -104,9 +115,10 @@ class OccEncoder(TransformerLayerSequence):
 
         lidar2img = lidar2img.view(
             1, B, num_cam, 1, 4, 4).repeat(D, 1, 1, num_query, 1, 1)
+        
+        reference_points_cam = torch.mul(lidar2img.to(torch.float32),
+                                            reference_points.to(torch.float32).transpose(-1, -2)).sum(-1, keepdim=True).squeeze(-1)
 
-        reference_points_cam = torch.matmul(lidar2img.to(torch.float32),
-                                            reference_points.to(torch.float32)).squeeze(-1)
         eps = 1e-5
 
         volume_mask = (reference_points_cam[..., 2:3] > eps)
@@ -164,7 +176,7 @@ class OccEncoder(TransformerLayerSequence):
         intermediate = []
 
         ref_3d = self.get_reference_points(
-                    volume_h, volume_w, volume_z, bs=volume_query.size(1),  device=volume_query.device, dtype=volume_query.dtype)
+                    volume_h, volume_w, volume_z, bs=volume_query.size(1), device=volume_query.device, dtype=volume_query.dtype)
 
         reference_points_cam, volume_mask = self.point_sampling(
             ref_3d, self.pc_range, kwargs['img_metas'])
@@ -245,8 +257,8 @@ class OccLayer(MyCustomBaseTransformerLayer):
         self.fp16_enabled = False
 
         self.deblock = nn.ModuleList()
-        conv_cfg=dict(type='Conv3d', bias=False)
-        norm_cfg=dict(type='GN', num_groups=16, requires_grad=True)
+        conv_cfg = dict(type='Conv3d', bias=False)
+        norm_cfg = dict(type='GN', num_groups=16, requires_grad=True)
         for i in range(conv_num):
             conv_layer = build_conv_layer(
                     conv_cfg,
@@ -338,7 +350,7 @@ class OccLayer(MyCustomBaseTransformerLayer):
                 query = query.reshape(bs, volume_z, volume_h, volume_w, -1).permute(0, 4, 3, 2, 1)
                 for i in range(len(self.deblock)):
                     query = self.deblock[i](query)
-                query = query.permute(0, 4, 3, 2, 1).reshape(bs, volume_z*volume_h*volume_w, -1)
+                query = query.permute(0, 4, 3, 2, 1).reshape(bs, volume_z * volume_h * volume_w, -1)
                 query = query + identity
     
             elif layer == 'norm':
diff --git a/projects/mmdet3d_plugin/surroundocc/modules/spatial_cross_attention.py b/projects/mmdet3d_plugin/surroundocc/modules/spatial_cross_attention.py
index 2ad500c..f73adc5 100644
--- a/projects/mmdet3d_plugin/surroundocc/modules/spatial_cross_attention.py
+++ b/projects/mmdet3d_plugin/surroundocc/modules/spatial_cross_attention.py
@@ -1,3 +1,16 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 # ---------------------------------------------
 # Copyright (c) OpenMMLab. All rights reserved.
@@ -26,8 +39,7 @@ from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFuncti
 from projects.mmdet3d_plugin.models.utils.bricks import run_time
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
-import pdb
-
+from mx_driving.fused import npu_multi_scale_deformable_attn_function
 
 @ATTENTION.register_module()
 class SpatialCrossAttention(BaseModule):
@@ -383,17 +395,10 @@ class MSDeformableAttention3D(BaseModule):
         #  attention_weights.shape: bs, num_query, num_heads, num_levels, num_all_points
         #
 
-        if torch.cuda.is_available() and value.is_cuda:
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+        if value.shape[3] == 64:
+            output = multi_scale_deformable_attn_pytorch(value, spatial_shapes, sampling_locations, attention_weights)
         else:
-            output = multi_scale_deformable_attn_pytorch(
-                value, spatial_shapes, sampling_locations, attention_weights)
+            output = npu_multi_scale_deformable_attn_function(value, spatial_shapes, level_start_index, sampling_locations, attention_weights)
         if not self.batch_first:
             output = output.permute(1, 0, 2)
 
diff --git a/replace_patch.sh b/replace_patch.sh
new file mode 100644
index 0000000..f265fe5
--- /dev/null
+++ b/replace_patch.sh
@@ -0,0 +1,14 @@
+#!/bin/bash
+for para in $*
+do
+    if [[ $para == --packages_path* ]];then
+        packages_path=`echo ${para#*=}`
+    fi
+done
+
+cp -f patch/torch/conv.py ${packages_path}/torch/nn/modules/conv.py
+cp -f patch/mmdet/resnet.py ${packages_path}/mmdet/models/backbones/resnet.py
+cp -f patch/mmcv/optimizer.py mmcv/mmcv/runner/hooks/optimizer.py
+cp -f patch/mmcv/epoch_based_runner.py mmcv/mmcv/runner/epoch_based_runner.py
+cp -f patch/mmcv/distributed.py mmcv/mmcv/parallel/distributed.py
+cp -f patch/mmcv/modulated_deform_conv.py mmcv/mmcv/ops/modulated_deform_conv.py
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..148c789
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,12 @@
+torchaudio==0.11.0
+torchvision==0.12.0
+mmdet==2.28.0
+mmsegmentation==0.30.0
+timm==0.9.16
+open3d-python==0.3.0.0
+numba==0.58.1
+numpy==1.23.0
+ipython
+sympy
+psutil
+attrs
diff --git a/tools/data_converter/__pycache__/__init__.cpython-37.pyc b/tools/data_converter/__pycache__/__init__.cpython-37.pyc
deleted file mode 100644
index 1bb9964..0000000
Binary files a/tools/data_converter/__pycache__/__init__.cpython-37.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/__init__.cpython-38.pyc b/tools/data_converter/__pycache__/__init__.cpython-38.pyc
deleted file mode 100644
index 49b2260..0000000
Binary files a/tools/data_converter/__pycache__/__init__.cpython-38.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/create_gt_database.cpython-37.pyc b/tools/data_converter/__pycache__/create_gt_database.cpython-37.pyc
deleted file mode 100644
index d697fd8..0000000
Binary files a/tools/data_converter/__pycache__/create_gt_database.cpython-37.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/create_gt_database.cpython-38.pyc b/tools/data_converter/__pycache__/create_gt_database.cpython-38.pyc
deleted file mode 100644
index 322ebbf..0000000
Binary files a/tools/data_converter/__pycache__/create_gt_database.cpython-38.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/indoor_converter.cpython-38.pyc b/tools/data_converter/__pycache__/indoor_converter.cpython-38.pyc
deleted file mode 100644
index 063b9d7..0000000
Binary files a/tools/data_converter/__pycache__/indoor_converter.cpython-38.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/kitti_converter.cpython-37.pyc b/tools/data_converter/__pycache__/kitti_converter.cpython-37.pyc
deleted file mode 100644
index c85d3e8..0000000
Binary files a/tools/data_converter/__pycache__/kitti_converter.cpython-37.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/kitti_converter.cpython-38.pyc b/tools/data_converter/__pycache__/kitti_converter.cpython-38.pyc
deleted file mode 100644
index 419d9f6..0000000
Binary files a/tools/data_converter/__pycache__/kitti_converter.cpython-38.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/kitti_data_utils.cpython-37.pyc b/tools/data_converter/__pycache__/kitti_data_utils.cpython-37.pyc
deleted file mode 100644
index ab0cebd..0000000
Binary files a/tools/data_converter/__pycache__/kitti_data_utils.cpython-37.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/kitti_data_utils.cpython-38.pyc b/tools/data_converter/__pycache__/kitti_data_utils.cpython-38.pyc
deleted file mode 100644
index e8d9eb0..0000000
Binary files a/tools/data_converter/__pycache__/kitti_data_utils.cpython-38.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/lyft_converter.cpython-37.pyc b/tools/data_converter/__pycache__/lyft_converter.cpython-37.pyc
deleted file mode 100644
index 251e1ec..0000000
Binary files a/tools/data_converter/__pycache__/lyft_converter.cpython-37.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/lyft_converter.cpython-38.pyc b/tools/data_converter/__pycache__/lyft_converter.cpython-38.pyc
deleted file mode 100644
index 63632b7..0000000
Binary files a/tools/data_converter/__pycache__/lyft_converter.cpython-38.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/nuscenes_converter.cpython-37.pyc b/tools/data_converter/__pycache__/nuscenes_converter.cpython-37.pyc
deleted file mode 100644
index caa540b..0000000
Binary files a/tools/data_converter/__pycache__/nuscenes_converter.cpython-37.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/nuscenes_converter.cpython-38.pyc b/tools/data_converter/__pycache__/nuscenes_converter.cpython-38.pyc
deleted file mode 100644
index de2d2a8..0000000
Binary files a/tools/data_converter/__pycache__/nuscenes_converter.cpython-38.pyc and /dev/null differ
diff --git a/tools/data_converter/__pycache__/nuscenes_converter_new.cpython-37.pyc b/tools/data_converter/__pycache__/nuscenes_converter_new.cpython-37.pyc
deleted file mode 100644
index 3012ef6..0000000
Binary files a/tools/data_converter/__pycache__/nuscenes_converter_new.cpython-37.pyc and /dev/null differ
diff --git a/tools/data_converter/nuimage_converter.py b/tools/data_converter/nuimage_converter.py
deleted file mode 100644
index 92be1de..0000000
--- a/tools/data_converter/nuimage_converter.py
+++ /dev/null
@@ -1,225 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import base64
-import mmcv
-import numpy as np
-from nuimages import NuImages
-from nuimages.utils.utils import mask_decode, name_to_index_mapping
-from os import path as osp
-
-nus_categories = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',
-                  'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
-                  'barrier')
-
-NAME_MAPPING = {
-    'movable_object.barrier': 'barrier',
-    'vehicle.bicycle': 'bicycle',
-    'vehicle.bus.bendy': 'bus',
-    'vehicle.bus.rigid': 'bus',
-    'vehicle.car': 'car',
-    'vehicle.construction': 'construction_vehicle',
-    'vehicle.motorcycle': 'motorcycle',
-    'human.pedestrian.adult': 'pedestrian',
-    'human.pedestrian.child': 'pedestrian',
-    'human.pedestrian.construction_worker': 'pedestrian',
-    'human.pedestrian.police_officer': 'pedestrian',
-    'movable_object.trafficcone': 'traffic_cone',
-    'vehicle.trailer': 'trailer',
-    'vehicle.truck': 'truck',
-}
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(description='Data converter arg parser')
-    parser.add_argument(
-        '--data-root',
-        type=str,
-        default='./data/nuimages',
-        help='specify the root path of dataset')
-    parser.add_argument(
-        '--version',
-        type=str,
-        nargs='+',
-        default=['v1.0-mini'],
-        required=False,
-        help='specify the dataset version')
-    parser.add_argument(
-        '--out-dir',
-        type=str,
-        default='./data/nuimages/annotations/',
-        required=False,
-        help='path to save the exported json')
-    parser.add_argument(
-        '--nproc',
-        type=int,
-        default=4,
-        required=False,
-        help='workers to process semantic masks')
-    parser.add_argument('--extra-tag', type=str, default='nuimages')
-    args = parser.parse_args()
-    return args
-
-
-def get_img_annos(nuim, img_info, cat2id, out_dir, data_root, seg_root):
-    """Get semantic segmentation map for an image.
-
-    Args:
-        nuim (obj:`NuImages`): NuImages dataset object
-        img_info (dict): Meta information of img
-
-    Returns:
-        np.ndarray: Semantic segmentation map of the image
-    """
-    sd_token = img_info['token']
-    image_id = img_info['id']
-    name_to_index = name_to_index_mapping(nuim.category)
-
-    # Get image data.
-    width, height = img_info['width'], img_info['height']
-    semseg_mask = np.zeros((height, width)).astype('uint8')
-
-    # Load stuff / surface regions.
-    surface_anns = [
-        o for o in nuim.surface_ann if o['sample_data_token'] == sd_token
-    ]
-
-    # Draw stuff / surface regions.
-    for ann in surface_anns:
-        # Get color and mask.
-        category_token = ann['category_token']
-        category_name = nuim.get('category', category_token)['name']
-        if ann['mask'] is None:
-            continue
-        mask = mask_decode(ann['mask'])
-
-        # Draw mask for semantic segmentation.
-        semseg_mask[mask == 1] = name_to_index[category_name]
-
-    # Load object instances.
-    object_anns = [
-        o for o in nuim.object_ann if o['sample_data_token'] == sd_token
-    ]
-
-    # Sort by token to ensure that objects always appear in the
-    # instance mask in the same order.
-    object_anns = sorted(object_anns, key=lambda k: k['token'])
-
-    # Draw object instances.
-    # The 0 index is reserved for background; thus, the instances
-    # should start from index 1.
-    annotations = []
-    for i, ann in enumerate(object_anns, start=1):
-        # Get color, box, mask and name.
-        category_token = ann['category_token']
-        category_name = nuim.get('category', category_token)['name']
-        if ann['mask'] is None:
-            continue
-        mask = mask_decode(ann['mask'])
-
-        # Draw masks for semantic segmentation and instance segmentation.
-        semseg_mask[mask == 1] = name_to_index[category_name]
-
-        if category_name in NAME_MAPPING:
-            cat_name = NAME_MAPPING[category_name]
-            cat_id = cat2id[cat_name]
-
-            x_min, y_min, x_max, y_max = ann['bbox']
-            # encode calibrated instance mask
-            mask_anno = dict()
-            mask_anno['counts'] = base64.b64decode(
-                ann['mask']['counts']).decode()
-            mask_anno['size'] = ann['mask']['size']
-
-            data_anno = dict(
-                image_id=image_id,
-                category_id=cat_id,
-                bbox=[x_min, y_min, x_max - x_min, y_max - y_min],
-                area=(x_max - x_min) * (y_max - y_min),
-                segmentation=mask_anno,
-                iscrowd=0)
-            annotations.append(data_anno)
-
-    # after process, save semantic masks
-    img_filename = img_info['file_name']
-    seg_filename = img_filename.replace('jpg', 'png')
-    seg_filename = osp.join(seg_root, seg_filename)
-    mmcv.imwrite(semseg_mask, seg_filename)
-    return annotations, np.max(semseg_mask)
-
-
-def export_nuim_to_coco(nuim, data_root, out_dir, extra_tag, version, nproc):
-    print('Process category information')
-    categories = []
-    categories = [
-        dict(id=nus_categories.index(cat_name), name=cat_name)
-        for cat_name in nus_categories
-    ]
-    cat2id = {k_v['name']: k_v['id'] for k_v in categories}
-
-    images = []
-    print('Process image meta information...')
-    for sample_info in mmcv.track_iter_progress(nuim.sample_data):
-        if sample_info['is_key_frame']:
-            img_idx = len(images)
-            images.append(
-                dict(
-                    id=img_idx,
-                    token=sample_info['token'],
-                    file_name=sample_info['filename'],
-                    width=sample_info['width'],
-                    height=sample_info['height']))
-
-    seg_root = f'{out_dir}semantic_masks'
-    mmcv.mkdir_or_exist(seg_root)
-    mmcv.mkdir_or_exist(osp.join(data_root, 'calibrated'))
-
-    global process_img_anno
-
-    def process_img_anno(img_info):
-        single_img_annos, max_cls_id = get_img_annos(nuim, img_info, cat2id,
-                                                     out_dir, data_root,
-                                                     seg_root)
-        return single_img_annos, max_cls_id
-
-    print('Process img annotations...')
-    if nproc > 1:
-        outputs = mmcv.track_parallel_progress(
-            process_img_anno, images, nproc=nproc)
-    else:
-        outputs = []
-        for img_info in mmcv.track_iter_progress(images):
-            outputs.append(process_img_anno(img_info))
-
-    # Determine the index of object annotation
-    print('Process annotation information...')
-    annotations = []
-    max_cls_ids = []
-    for single_img_annos, max_cls_id in outputs:
-        max_cls_ids.append(max_cls_id)
-        for img_anno in single_img_annos:
-            img_anno.update(id=len(annotations))
-            annotations.append(img_anno)
-
-    max_cls_id = max(max_cls_ids)
-    print(f'Max ID of class in the semantic map: {max_cls_id}')
-
-    coco_format_json = dict(
-        images=images, annotations=annotations, categories=categories)
-
-    mmcv.mkdir_or_exist(out_dir)
-    out_file = osp.join(out_dir, f'{extra_tag}_{version}.json')
-    print(f'Annotation dumped to {out_file}')
-    mmcv.dump(coco_format_json, out_file)
-
-
-def main():
-    args = parse_args()
-    for version in args.version:
-        nuim = NuImages(
-            dataroot=args.data_root, version=version, verbose=True, lazy=True)
-        export_nuim_to_coco(nuim, args.data_root, args.out_dir, args.extra_tag,
-                            version, args.nproc)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/generate_occupancy_nuscenes/generate_occupancy_nuscenes.py b/tools/generate_occupancy_nuscenes/generate_occupancy_nuscenes.py
index e04ba5f..9ace9ba 100644
--- a/tools/generate_occupancy_nuscenes/generate_occupancy_nuscenes.py
+++ b/tools/generate_occupancy_nuscenes/generate_occupancy_nuscenes.py
@@ -1,6 +1,5 @@
 import os
 import sys
-import pdb
 import time
 import yaml
 import torch
@@ -36,7 +35,8 @@ def run_poisson(pcd, depth, n_threads, min_density=None):
 
     return mesh, densities
 
-def create_mesh_from_map(buffer, depth, n_threads, min_density=None, point_cloud_original= None):
+
+def create_mesh_from_map(buffer, depth, n_threads, min_density=None, point_cloud_original=None):
 
     if point_cloud_original is None:
         pcd = buffer_to_pointcloud(buffer)
@@ -45,6 +45,7 @@ def create_mesh_from_map(buffer, depth, n_threads, min_density=None, point_cloud
 
     return run_poisson(pcd, depth, n_threads, min_density)
 
+
 def buffer_to_pointcloud(buffer, compute_normals=False):
     pcd = o3d.geometry.PointCloud()
     for cloud in buffer:
@@ -77,6 +78,7 @@ def preprocess(pcd, config):
         normals=True
     )
 
+
 def nn_correspondance(verts1, verts2):
     """ for each vertex in verts2 find the nearest vertex in verts1
 
@@ -105,8 +107,7 @@ def nn_correspondance(verts1, verts2):
 
 
 
-
-def lidar_to_world_to_lidar(pc,lidar_calibrated_sensor,lidar_ego_pose,
+def lidar_to_world_to_lidar(pc, lidar_calibrated_sensor, lidar_ego_pose,
     cam_calibrated_sensor,
     cam_ego_pose):
 
@@ -205,7 +206,7 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
         object_points_list = []
         j = 0
         while j < points_in_boxes.shape[-1]:
-            object_points_mask = points_in_boxes[0][:,j].bool()
+            object_points_mask = points_in_boxes[0][:, j].bool()
             object_points = pc0[object_points_mask]
             object_points_list.append(object_points)
             j = j + 1
@@ -274,7 +275,7 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
     object_token_zoo = []
     object_semantic = []
     for dict in dict_list:
-        for i,object_token in enumerate(dict['object_tokens']):
+        for i, object_token in enumerate(dict['object_tokens']):
             if object_token not in object_token_zoo:
                 if (dict['object_points_list'][i].shape[0] > 0):
                     object_token_zoo.append(object_token)
@@ -291,7 +292,7 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
                 if query_object_token == object_token:
                     object_points = dict['object_points_list'][i]
                     if object_points.shape[0] > 0:
-                        object_points = object_points[:,:3] - dict['gt_bbox_3d'][i][:3]
+                        object_points = object_points[:, :3] - dict['gt_bbox_3d'][i][:3]
                         rots = dict['gt_bbox_3d'][i][6]
                         Rot = Rotation.from_euler('z', -rots, degrees=False)
                         rotated_object_points = Rot.apply(object_points)
@@ -305,7 +306,7 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
     object_points_vertice = []
     for key in object_points_dict.keys():
         point_cloud = object_points_dict[key]
-        object_points_vertice.append(point_cloud[:,:3])
+        object_points_vertice.append(point_cloud[:, :3])
     # print('object finish')
 
 
@@ -333,7 +334,7 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
                                                       lidar_ego_pose0.copy(),
                                                       lidar_calibrated_sensor,
                                                       lidar_ego_pose)
-        point_cloud = lidar_pc_i.points.T[:,:3]
+        point_cloud = lidar_pc_i.points.T[:, :3]
         point_cloud_with_semantic = lidar_pc_i_semantic.points.T
 
         ################## load bbox of target frame ##############
@@ -347,26 +348,26 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
         gt_bbox_3d[:, 2] -= dims[:, 2] / 2.
         gt_bbox_3d[:, 2] = gt_bbox_3d[:, 2] - 0.1
         gt_bbox_3d[:, 3:6] = gt_bbox_3d[:, 3:6] * 1.1
-        rots = gt_bbox_3d[:,6:7]
-        locs = gt_bbox_3d[:,0:3]
+        rots = gt_bbox_3d[:, 6:7]
+        locs = gt_bbox_3d[:, 0:3]
 
         ################## bbox placement ##############
         object_points_list = []
         object_semantic_list = []
         for j, object_token in enumerate(dict['object_tokens']):
             for k, object_token_in_zoo in enumerate(object_token_zoo):
-                if object_token==object_token_in_zoo:
+                if object_token == object_token_in_zoo:
                     points = object_points_vertice[k]
                     Rot = Rotation.from_euler('z', rots[j], degrees=False)
                     rotated_object_points = Rot.apply(points)
                     points = rotated_object_points + locs[j]
                     if points.shape[0] >= 5:
                         points_in_boxes = points_in_boxes_cpu(torch.from_numpy(points[:, :3][np.newaxis, :, :]),
-                                                              torch.from_numpy(gt_bbox_3d[j:j+1][np.newaxis, :]))
-                        points = points[points_in_boxes[0,:,0].bool()]
+                                                              torch.from_numpy(gt_bbox_3d[j:j + 1][np.newaxis, :]))
+                        points = points[points_in_boxes[0, :, 0].bool()]
 
                     object_points_list.append(points)
-                    semantics = np.ones_like(points[:,0:1]) * object_semantic[k]
+                    semantics = np.ones_like(points[:, 0:1]) * object_semantic[k]
                     object_semantic_list.append(np.concatenate([points[:, :3], semantics], axis=1))
 
         try: # avoid concatenate an empty array
@@ -433,8 +434,8 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
         sparse_voxels_semantic = scene_semantic_points
 
         x = torch.from_numpy(dense_voxels).cuda().unsqueeze(0).float()
-        y = torch.from_numpy(sparse_voxels_semantic[:,:3]).cuda().unsqueeze(0).float()
-        d1, d2, idx1, idx2 = chamfer.forward(x,y)
+        y = torch.from_numpy(sparse_voxels_semantic[:, :3]).cuda().unsqueeze(0).float()
+        d1, d2, idx1, idx2 = chamfer.forward(x, y)
         indices = idx1[0].cpu().numpy()
 
 
@@ -459,7 +460,7 @@ def main(nusc, val_list, indice, nuscenesyaml, args, config):
 
 def save_ply(points, name):
     point_cloud_original = o3d.geometry.PointCloud()
-    point_cloud_original.points = o3d.utility.Vector3dVector(points[:,:3])
+    point_cloud_original.points = o3d.utility.Vector3dVector(points[:, :3])
     o3d.io.write_point_cloud("{}.ply".format(name), point_cloud_original)
 
 
@@ -476,10 +477,10 @@ if __name__ == '__main__':
     parse.add_argument('--dataroot', type=str, default='./data/nuScenes/')
     parse.add_argument('--nusc_val_list', type=str, default='./nuscenes_val_list.txt')
     parse.add_argument('--label_mapping', type=str, default='nuscenes.yaml')
-    args=parse.parse_args()
+    args = parse.parse_args()
 
 
-    if args.dataset=='nuscenes':
+    if args.dataset == 'nuscenes':
         val_list = []
         with open(args.nusc_val_list, 'r') as file:
             for item in file:
@@ -504,7 +505,7 @@ if __name__ == '__main__':
         nuscenesyaml = yaml.safe_load(stream)
 
 
-    for i in range(args.start,args.end):
+    for i in range(args.start, args.end):
         print('processing sequecne:', i)
         main(nusc, val_list, indice=i,
              nuscenesyaml=nuscenesyaml, args=args, config=config)
diff --git a/tools/generate_occupancy_with_own_data/process_your_own_data.py b/tools/generate_occupancy_with_own_data/process_your_own_data.py
index 2395da0..bc2af65 100644
--- a/tools/generate_occupancy_with_own_data/process_your_own_data.py
+++ b/tools/generate_occupancy_with_own_data/process_your_own_data.py
@@ -26,7 +26,8 @@ def run_poisson(pcd, depth, n_threads, min_density=None):
 
     return mesh, densities
 
-def create_mesh_from_map(buffer, depth, n_threads, min_density=None, point_cloud_original= None):
+
+def create_mesh_from_map(buffer, depth, n_threads, min_density=None, point_cloud_original=None):
 
     if point_cloud_original is None:
         pcd = buffer_to_pointcloud(buffer)
@@ -35,6 +36,7 @@ def create_mesh_from_map(buffer, depth, n_threads, min_density=None, point_cloud
 
     return run_poisson(pcd, depth, n_threads, min_density)
 
+
 def buffer_to_pointcloud(buffer, compute_normals=False):
     pcd = o3d.geometry.PointCloud()
     for cloud in buffer:
@@ -67,6 +69,7 @@ def preprocess(pcd, config):
         normals=True
     )
 
+
 def nn_correspondance(verts1, verts2):
     """ for each vertex in verts2 find the nearest vertex in verts1
 
@@ -94,9 +97,7 @@ def nn_correspondance(verts1, verts2):
     return indices, distances
 
 
-
-
-def lidar_to_world_to_lidar(pc,lidar_calibrated_sensor,lidar_ego_pose,
+def lidar_to_world_to_lidar(pc, lidar_calibrated_sensor, lidar_ego_pose,
     cam_calibrated_sensor,
     cam_ego_pose):
 
@@ -128,7 +129,7 @@ if __name__ == '__main__':
     parse.add_argument('--whole_scene_to_mesh', action='store_true', default=False)
 
 
-    args=parse.parse_args()
+    args = parse.parse_args()
 
     # load config
     with open(args.config_path, 'r') as stream:
@@ -140,11 +141,11 @@ if __name__ == '__main__':
 
 
     path = args.data_path
-    pc_path = os.path.join(path,'pc/')
-    pc_seman_path = os.path.join(path,'pc_seman/')
-    bbox_path = os.path.join(path,'bbox/')
-    calib_path = os.path.join(path,'calib/')
-    pose_path = os.path.join(path,'pose/')
+    pc_path = os.path.join(path, 'pc/')
+    pc_seman_path = os.path.join(path, 'pc_seman/')
+    bbox_path = os.path.join(path, 'bbox/')
+    calib_path = os.path.join(path, 'calib/')
+    pose_path = os.path.join(path, 'pose/')
 
     lidar_ego_pose0 = np.load(os.path.join(pose_path, 'lidar_ego_pose0.npy'), allow_pickle=True).item()
     lidar_calibrated_sensor0 = np.load(os.path.join(calib_path, 'lidar_calibrated_sensor0.npy'), allow_pickle=True).item()
@@ -167,7 +168,7 @@ if __name__ == '__main__':
         object_points_list = []
         j = 0
         while j < points_in_boxes.shape[-1]:
-            object_points_mask = points_in_boxes[0][:,j].bool()
+            object_points_mask = points_in_boxes[0][:, j].bool()
             object_points = pc0[object_points_mask]
             object_points_list.append(object_points)
             j = j + 1
diff --git a/tools/model_converters/convert_votenet_checkpoints.py b/tools/model_converters/convert_votenet_checkpoints.py
deleted file mode 100644
index 33792b0..0000000
--- a/tools/model_converters/convert_votenet_checkpoints.py
+++ /dev/null
@@ -1,152 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import tempfile
-import torch
-from mmcv import Config
-from mmcv.runner import load_state_dict
-
-from mmdet3d.models import build_detector
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(
-        description='MMDet3D upgrade model version(before v0.6.0) of VoteNet')
-    parser.add_argument('checkpoint', help='checkpoint file')
-    parser.add_argument('--out', help='path of the output checkpoint file')
-    args = parser.parse_args()
-    return args
-
-
-def parse_config(config_strings):
-    """Parse config from strings.
-
-    Args:
-        config_strings (string): strings of model config.
-
-    Returns:
-        Config: model config
-    """
-    temp_file = tempfile.NamedTemporaryFile()
-    config_path = f'{temp_file.name}.py'
-    with open(config_path, 'w') as f:
-        f.write(config_strings)
-
-    config = Config.fromfile(config_path)
-
-    # Update backbone config
-    if 'pool_mod' in config.model.backbone:
-        config.model.backbone.pop('pool_mod')
-
-    if 'sa_cfg' not in config.model.backbone:
-        config.model.backbone['sa_cfg'] = dict(
-            type='PointSAModule',
-            pool_mod='max',
-            use_xyz=True,
-            normalize_xyz=True)
-
-    if 'type' not in config.model.bbox_head.vote_aggregation_cfg:
-        config.model.bbox_head.vote_aggregation_cfg['type'] = 'PointSAModule'
-
-    # Update bbox_head config
-    if 'pred_layer_cfg' not in config.model.bbox_head:
-        config.model.bbox_head['pred_layer_cfg'] = dict(
-            in_channels=128, shared_conv_channels=(128, 128), bias=True)
-
-    if 'feat_channels' in config.model.bbox_head:
-        config.model.bbox_head.pop('feat_channels')
-
-    if 'vote_moudule_cfg' in config.model.bbox_head:
-        config.model.bbox_head['vote_module_cfg'] = config.model.bbox_head.pop(
-            'vote_moudule_cfg')
-
-    if config.model.bbox_head.vote_aggregation_cfg.use_xyz:
-        config.model.bbox_head.vote_aggregation_cfg.mlp_channels[0] -= 3
-
-    temp_file.close()
-
-    return config
-
-
-def main():
-    """Convert keys in checkpoints for VoteNet.
-
-    There can be some breaking changes during the development of mmdetection3d,
-    and this tool is used for upgrading checkpoints trained with old versions
-    (before v0.6.0) to the latest one.
-    """
-    args = parse_args()
-    checkpoint = torch.load(args.checkpoint)
-    cfg = parse_config(checkpoint['meta']['config'])
-    # Build the model and load checkpoint
-    model = build_detector(
-        cfg.model,
-        train_cfg=cfg.get('train_cfg'),
-        test_cfg=cfg.get('test_cfg'))
-    orig_ckpt = checkpoint['state_dict']
-    converted_ckpt = orig_ckpt.copy()
-
-    if cfg['dataset_type'] == 'ScanNetDataset':
-        NUM_CLASSES = 18
-    elif cfg['dataset_type'] == 'SUNRGBDDataset':
-        NUM_CLASSES = 10
-    else:
-        raise NotImplementedError
-
-    RENAME_PREFIX = {
-        'bbox_head.conv_pred.0': 'bbox_head.conv_pred.shared_convs.layer0',
-        'bbox_head.conv_pred.1': 'bbox_head.conv_pred.shared_convs.layer1'
-    }
-
-    DEL_KEYS = [
-        'bbox_head.conv_pred.0.bn.num_batches_tracked',
-        'bbox_head.conv_pred.1.bn.num_batches_tracked'
-    ]
-
-    EXTRACT_KEYS = {
-        'bbox_head.conv_pred.conv_cls.weight':
-        ('bbox_head.conv_pred.conv_out.weight', [(0, 2), (-NUM_CLASSES, -1)]),
-        'bbox_head.conv_pred.conv_cls.bias':
-        ('bbox_head.conv_pred.conv_out.bias', [(0, 2), (-NUM_CLASSES, -1)]),
-        'bbox_head.conv_pred.conv_reg.weight':
-        ('bbox_head.conv_pred.conv_out.weight', [(2, -NUM_CLASSES)]),
-        'bbox_head.conv_pred.conv_reg.bias':
-        ('bbox_head.conv_pred.conv_out.bias', [(2, -NUM_CLASSES)])
-    }
-
-    # Delete some useless keys
-    for key in DEL_KEYS:
-        converted_ckpt.pop(key)
-
-    # Rename keys with specific prefix
-    RENAME_KEYS = dict()
-    for old_key in converted_ckpt.keys():
-        for rename_prefix in RENAME_PREFIX.keys():
-            if rename_prefix in old_key:
-                new_key = old_key.replace(rename_prefix,
-                                          RENAME_PREFIX[rename_prefix])
-                RENAME_KEYS[new_key] = old_key
-    for new_key, old_key in RENAME_KEYS.items():
-        converted_ckpt[new_key] = converted_ckpt.pop(old_key)
-
-    # Extract weights and rename the keys
-    for new_key, (old_key, indices) in EXTRACT_KEYS.items():
-        cur_layers = orig_ckpt[old_key]
-        converted_layers = []
-        for (start, end) in indices:
-            if end != -1:
-                converted_layers.append(cur_layers[start:end])
-            else:
-                converted_layers.append(cur_layers[start:])
-        converted_layers = torch.cat(converted_layers, 0)
-        converted_ckpt[new_key] = converted_layers
-        if old_key in converted_ckpt.keys():
-            converted_ckpt.pop(old_key)
-
-    # Check the converted checkpoint by loading to the model
-    load_state_dict(model, converted_ckpt, strict=True)
-    checkpoint['state_dict'] = converted_ckpt
-    torch.save(checkpoint, args.out)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/model_converters/publish_model.py b/tools/model_converters/publish_model.py
deleted file mode 100644
index 318fd46..0000000
--- a/tools/model_converters/publish_model.py
+++ /dev/null
@@ -1,35 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import subprocess
-import torch
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Process a checkpoint to be published')
-    parser.add_argument('in_file', help='input checkpoint filename')
-    parser.add_argument('out_file', help='output checkpoint filename')
-    args = parser.parse_args()
-    return args
-
-
-def process_checkpoint(in_file, out_file):
-    checkpoint = torch.load(in_file, map_location='cpu')
-    # remove optimizer for smaller file size
-    if 'optimizer' in checkpoint:
-        del checkpoint['optimizer']
-    # if it is necessary to remove some sensitive data in checkpoint['meta'],
-    # add the code here.
-    torch.save(checkpoint, out_file)
-    sha = subprocess.check_output(['sha256sum', out_file]).decode()
-    final_file = out_file.rstrip('.pth') + '-{}.pth'.format(sha[:8])
-    subprocess.Popen(['mv', out_file, final_file])
-
-
-def main():
-    args = parse_args()
-    process_checkpoint(args.in_file, args.out_file)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/model_converters/regnet2mmdet.py b/tools/model_converters/regnet2mmdet.py
deleted file mode 100644
index 9dee3c8..0000000
--- a/tools/model_converters/regnet2mmdet.py
+++ /dev/null
@@ -1,89 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import torch
-from collections import OrderedDict
-
-
-def convert_stem(model_key, model_weight, state_dict, converted_names):
-    new_key = model_key.replace('stem.conv', 'conv1')
-    new_key = new_key.replace('stem.bn', 'bn1')
-    state_dict[new_key] = model_weight
-    converted_names.add(model_key)
-    print(f'Convert {model_key} to {new_key}')
-
-
-def convert_head(model_key, model_weight, state_dict, converted_names):
-    new_key = model_key.replace('head.fc', 'fc')
-    state_dict[new_key] = model_weight
-    converted_names.add(model_key)
-    print(f'Convert {model_key} to {new_key}')
-
-
-def convert_reslayer(model_key, model_weight, state_dict, converted_names):
-    split_keys = model_key.split('.')
-    layer, block, module = split_keys[:3]
-    block_id = int(block[1:])
-    layer_name = f'layer{int(layer[1:])}'
-    block_name = f'{block_id - 1}'
-
-    if block_id == 1 and module == 'bn':
-        new_key = f'{layer_name}.{block_name}.downsample.1.{split_keys[-1]}'
-    elif block_id == 1 and module == 'proj':
-        new_key = f'{layer_name}.{block_name}.downsample.0.{split_keys[-1]}'
-    elif module == 'f':
-        if split_keys[3] == 'a_bn':
-            module_name = 'bn1'
-        elif split_keys[3] == 'b_bn':
-            module_name = 'bn2'
-        elif split_keys[3] == 'c_bn':
-            module_name = 'bn3'
-        elif split_keys[3] == 'a':
-            module_name = 'conv1'
-        elif split_keys[3] == 'b':
-            module_name = 'conv2'
-        elif split_keys[3] == 'c':
-            module_name = 'conv3'
-        new_key = f'{layer_name}.{block_name}.{module_name}.{split_keys[-1]}'
-    else:
-        raise ValueError(f'Unsupported conversion of key {model_key}')
-    print(f'Convert {model_key} to {new_key}')
-    state_dict[new_key] = model_weight
-    converted_names.add(model_key)
-
-
-def convert(src, dst):
-    """Convert keys in pycls pretrained RegNet models to mmdet style."""
-    # load caffe model
-    regnet_model = torch.load(src)
-    blobs = regnet_model['model_state']
-    # convert to pytorch style
-    state_dict = OrderedDict()
-    converted_names = set()
-    for key, weight in blobs.items():
-        if 'stem' in key:
-            convert_stem(key, weight, state_dict, converted_names)
-        elif 'head' in key:
-            convert_head(key, weight, state_dict, converted_names)
-        elif key.startswith('s'):
-            convert_reslayer(key, weight, state_dict, converted_names)
-
-    # check if all layers are converted
-    for key in blobs:
-        if key not in converted_names:
-            print(f'not converted: {key}')
-    # save checkpoint
-    checkpoint = dict()
-    checkpoint['state_dict'] = state_dict
-    torch.save(checkpoint, dst)
-
-
-def main():
-    parser = argparse.ArgumentParser(description='Convert model keys')
-    parser.add_argument('src', help='src detectron model path')
-    parser.add_argument('dst', help='save path')
-    args = parser.parse_args()
-    convert(args.src, args.dst)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/train.py b/tools/train.py
index c172243..b24347c 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -1,3 +1,17 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 # ---------------------------------------------
 # Copyright (c) OpenMMLab. All rights reserved.
 # ---------------------------------------------
@@ -28,8 +42,10 @@ from mmdet.apis import set_random_seed
 from mmseg import __version__ as mmseg_version
 
 from mmcv.utils import TORCH_VERSION, digit_version
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 
-
+torch.npu.config.allow_internal_format = False
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Train a detector')
@@ -81,13 +97,14 @@ def parse_args():
         default='none',
         help='job launcher')
     parser.add_argument('--local_rank', type=int, default=0)
+    parser.add_argument('--local-rank', type=int, default=0)
     parser.add_argument(
         '--autoscale-lr',
         action='store_true',
         help='automatically scale lr with the number of gpus')
     args = parser.parse_args()
     if 'LOCAL_RANK' not in os.environ:
-        os.environ['LOCAL_RANK'] = str(args.local_rank)
+        os.environ['LOCAL_RANK'] = str(args.local-rank)
 
     if args.options and args.cfg_options:
         raise ValueError(
diff --git a/tools/visual.py b/tools/visual.py
index 34b513a..f67f743 100644
--- a/tools/visual.py
+++ b/tools/visual.py
@@ -34,7 +34,7 @@ colors = np.array(
 #mlab.options.offscreen = True
 
 voxel_size = 0.5
-pc_range = [-50, -50,  -5, 50, 50, 3]
+pc_range = [-50, -50, -5, 50, 50, 3]
 
 visual_path = sys.argv[1]
 fov_voxels = np.load(visual_path)
@@ -48,14 +48,13 @@ fov_voxels[:, 2] += pc_range[2]
 
 #figure = mlab.figure(size=(600, 600), bgcolor=(1, 1, 1))
 figure = mlab.figure(size=(2560, 1440), bgcolor=(1, 1, 1))
-# pdb.set_trace()
 plt_plot_fov = mlab.points3d(
     fov_voxels[:, 0],
     fov_voxels[:, 1],
     fov_voxels[:, 2],
     fov_voxels[:, 3],
     colormap="viridis",
-    scale_factor=voxel_size - 0.05*voxel_size,
+    scale_factor=voxel_size - 0.05 * voxel_size,
     mode="cube",
     opacity=1.0,
     vmin=0,
