diff --git a/demo_detailed_usage.py b/demo_detailed_usage.py
new file mode 100644
index 0000000..95c261f
--- /dev/null
+++ b/demo_detailed_usage.py
@@ -0,0 +1,69 @@
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+from vggt.models.vggt import VGGT
+from vggt.utils.load_fn import load_and_preprocess_images
+from vggt.utils.pose_enc import pose_encoding_to_extri_intri
+
+import os
+import argparse
+
+def get_all_files_paths(directory):
+    file_paths = []
+    for root, dirs, files in os.walk(directory):
+        for file in files:
+            file_path = os.path.join(root, file)
+            file_paths.append(file_path)
+    return file_paths
+
+def detailed_usage(pt_path, images_path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    # bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+)
+    dtype = torch.bfloat16
+
+    # Initialize the model and load the pretrained weights.
+    # This will automatically download the model weights the first time it's run, which may take a while.
+    model = VGGT()
+    model.load_state_dict(torch.load(pt_path))
+    model = model.to(device)
+
+    # Load and preprocess example images (replace with your own image paths)
+    image_names = get_all_files_paths(images_path)
+    images = load_and_preprocess_images(image_names).to(device)
+
+    with torch.no_grad():
+        with torch.cuda.amp.autocast(dtype=dtype):
+            images = images[None]  # add batch dimension
+            aggregated_tokens_list, ps_idx = model.aggregator(images)
+
+        # Predict Cameras
+        pose_enc = model.camera_head(aggregated_tokens_list)[-1]
+        # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)
+        extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])
+
+        # Predict Depth Maps
+        depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)
+
+        # Predict Point Maps
+        point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)
+
+        # Predict Tracks
+        # choose your own points to track, with shape (N, 2) for one scene
+        query_points = torch.FloatTensor([[100.0, 200.0],
+                                            [60.72, 259.94]]).to(device)
+        track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='VGGT quick start')
+    parser.add_argument('--images_path', type=str, default='examples/kitchen/images', help='images_path')
+    parser.add_argument('--pt_path', type=str, default='model.pt', help='pretrained_model')
+    args = parser.parse_args()
+
+    return args
+
+def main():
+    args = parse_args()
+    detailed_usage(args.pt_path, args.images_path)
+
+if __name__ == '__main__':
+    main()
\ No newline at end of file
diff --git a/demo_quick_start.py b/demo_quick_start.py
new file mode 100644
index 0000000..efd191a
--- /dev/null
+++ b/demo_quick_start.py
@@ -0,0 +1,49 @@
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+from vggt.models.vggt import VGGT
+from vggt.utils.load_fn import load_and_preprocess_images
+
+import os
+import argparse
+
+def get_all_files_paths(directory):
+    file_paths = []
+    for root, dirs, files in os.walk(directory):
+        for file in files:
+            file_path = os.path.join(root, file)
+            file_paths.append(file_path)
+    return file_paths
+
+def quick_start(pt_path, images_path):
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    # bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+)
+    dtype = torch.bfloat16
+
+    # Initialize the model and load the pretrained weights.
+    # This will automatically download the model weights the first time it's run, which may take a while.
+    model = VGGT()
+    model.load_state_dict(torch.load(pt_path))
+    model = model.to(device)
+
+    image_names = get_all_files_paths(images_path)
+    images = load_and_preprocess_images(image_names).to(device)
+
+    with torch.no_grad():
+        with torch.cuda.amp.autocast(dtype=dtype):
+            # Predict attributes including cameras, depth maps, and point maps.
+            predictions = model(images)
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='VGGT quick start')
+    parser.add_argument('--images_path', type=str, default='examples/kitchen/images', help='images_path')
+    parser.add_argument('--pt_path', type=str, default='model.pt', help='pretrained_model')
+    args = parser.parse_args()
+
+    return args
+def main():
+    args = parse_args()
+    quick_start(args.pt_path, args.images_path)
+
+if __name__ == '__main__':
+    main()
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index 1cb61a2..3e6b49b 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,7 +1,15 @@
-torch==2.3.1
-torchvision==0.18.1
-numpy==1.26.1
+torchvision==0.16.0
+numpy==1.23.5
 Pillow
 huggingface_hub
 einops
 safetensors
+attrs
+decorator
+psutil
+scipy
+hydra-core==1.3.2
+iopath
+wcmatch
+tensorboard
+fvcore
diff --git a/training/config/default.yaml b/training/config/default.yaml
index 56a78e5..ca2d8a3 100644
--- a/training/config/default.yaml
+++ b/training/config/default.yaml
@@ -8,7 +8,7 @@ seed_value: 42
 accum_steps: 2    # We did not use gradient accumulation in our training, while if you suffer from OOM, you can try to use it.
 patch_size: 14
 val_epoch_freq: 5
-max_img_per_gpu: 48
+max_img_per_gpu: 30
 
 limit_train_batches: 800
 limit_val_batches: 400
@@ -30,8 +30,8 @@ data:
       dataset_configs:
         - _target_: data.datasets.co3d.Co3dDataset
           split: train
-          CO3D_DIR: /YOUR/PATH/TO/CO3D
-          CO3D_ANNOTATION_DIR: /YOUR/PATH/TO/CO3D_ANNOTATION
+          CO3D_DIR: co3D/co3D
+          CO3D_ANNOTATION_DIR: co3D/co3D_annotations
   val:
     _target_: data.dynamic_dataloader.DynamicTorchDataset
     num_workers: ${num_workers}
@@ -45,8 +45,8 @@ data:
       dataset_configs:
         - _target_: data.datasets.co3d.Co3dDataset
           split: test
-          CO3D_DIR: /YOUR/PATH/TO/CO3D
-          CO3D_ANNOTATION_DIR: /YOUR/PATH/TO/CO3D_ANNOTATION
+          CO3D_DIR: co3D/co3D
+          CO3D_ANNOTATION_DIR: co3D/co3D_annotations
 
 
 logging:
@@ -86,7 +86,7 @@ logging:
 checkpoint:
   save_dir: logs/${exp_name}/ckpts
   save_freq: 5
-  resume_checkpoint_path: /YOUR/PATH/TO/CKPT
+  resume_checkpoint_path: ../model.pt
   strict: False
 
 
diff --git a/training/config/default_dataset.yaml b/training/config/default_dataset.yaml
index 0390dfa..dde636a 100644
--- a/training/config/default_dataset.yaml
+++ b/training/config/default_dataset.yaml
@@ -25,7 +25,7 @@ data:
       debug: False
       get_nearby: True
       load_depth: True
-      img_nums: [2, 24]
+      img_nums: [2, 15]
       max_img_per_gpu: 48
       allow_duplicate_img: True
       repeat_batch: False
diff --git a/training/data/datasets/co3d.py b/training/data/datasets/co3d.py
index 5636626..0573afb 100644
--- a/training/data/datasets/co3d.py
+++ b/training/data/datasets/co3d.py
@@ -20,47 +20,7 @@ from data.base_dataset import BaseDataset
 
 
 SEEN_CATEGORIES = [
-    "apple",
-    "backpack",
-    "banana",
-    "baseballbat",
-    "baseballglove",
-    "bench",
-    "bicycle",
-    "bottle",
-    "bowl",
-    "broccoli",
-    "cake",
-    "car",
-    "carrot",
-    "cellphone",
-    "chair",
-    "cup",
-    "donut",
-    "hairdryer",
-    "handbag",
-    "hydrant",
-    "keyboard",
-    "laptop",
-    "microwave",
-    "motorcycle",
-    "mouse",
-    "orange",
-    "parkingmeter",
-    "pizza",
-    "plant",
-    "stopsign",
-    "teddybear",
-    "toaster",
-    "toilet",
-    "toybus",
-    "toyplane",
-    "toytrain",
-    "toytruck",
     "tv",
-    "umbrella",
-    "vase",
-    "wineglass",
 ]
 
 
@@ -104,7 +64,7 @@ class Co3dDataset(BaseDataset):
         category = sorted(SEEN_CATEGORIES)
 
         if self.debug:
-            category = ["apple"]
+            category = ["tv"]
 
         if split == "train":
             split_name_list = ["train"]
diff --git a/training/data/dynamic_dataloader.py b/training/data/dynamic_dataloader.py
index 11d1224..b17f156 100644
--- a/training/data/dynamic_dataloader.py
+++ b/training/data/dynamic_dataloader.py
@@ -162,6 +162,8 @@ class DynamicBatchSampler(Sampler):
                 # Sample random image number and aspect ratio
                 random_image_num = int(np.random.choice(self.possible_nums, p=self.normalized_weights))
                 random_aspect_ratio = round(self.rng.uniform(self.aspect_ratio_range[0], self.aspect_ratio_range[1]), 2)
+                random_image_num = 2
+                random_aspect_ratio = 1.0
 
                 # Update sampler parameters
                 self.sampler.update_parameters(
@@ -172,6 +174,7 @@ class DynamicBatchSampler(Sampler):
                 # Calculate batch size based on max images per GPU and current image number
                 batch_size = self.max_img_per_gpu / random_image_num
                 batch_size = np.floor(batch_size).astype(int)
+                batch_size = 15
                 batch_size = max(1, batch_size)  # Ensure batch size is at least 1
 
                 # Collect samples for the current batch
diff --git a/training/launch.py b/training/launch.py
index 5511f51..649747a 100644
--- a/training/launch.py
+++ b/training/launch.py
@@ -9,6 +9,11 @@ from hydra import initialize, compose
 from omegaconf import DictConfig, OmegaConf
 from trainer import Trainer
 
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
+torch.npu.config.allow_internal_format = False
 
 def main():
     parser = argparse.ArgumentParser(description="Train model with configurable YAML file")
diff --git a/training/train_utils/normalization.py b/training/train_utils/normalization.py
index 8494047..a59a652 100644
--- a/training/train_utils/normalization.py
+++ b/training/train_utils/normalization.py
@@ -62,7 +62,6 @@ def normalize_camera_extrinsics_and_points_batch(
 
     B, S, _, _ = extrinsics.shape
     device = extrinsics.device
-    assert device == torch.device("cpu")
 
 
     # Convert extrinsics to homogeneous form: (B, N,4,4)
diff --git a/training/trainer.py b/training/trainer.py
index 9150582..994e544 100644
--- a/training/trainer.py
+++ b/training/trainer.py
@@ -550,10 +550,9 @@ class Trainer:
 
             
             with torch.cuda.amp.autocast(enabled=False):
+                batch = copy_data_to_device(batch, self.device, non_blocking=True)
                 batch = self._process_batch(batch)
 
-            batch = copy_data_to_device(batch, self.device, non_blocking=True)
-
             accum_steps = self.accum_steps
 
             if accum_steps==1:
diff --git a/vggt/heads/utils.py b/vggt/heads/utils.py
index 533fc8a..ca71a4b 100644
--- a/vggt/heads/utils.py
+++ b/vggt/heads/utils.py
@@ -46,7 +46,7 @@ def make_sincos_pos_embed(embed_dim: int, pos: torch.Tensor, omega_0: float = 10
     """
     assert embed_dim % 2 == 0
     device = pos.device
-    omega = torch.arange(embed_dim // 2, dtype=torch.float32 if device.type == "mps" else torch.double, device=device)
+    omega = torch.arange(embed_dim // 2, dtype=torch.float32, device=device)
     omega /= embed_dim / 2.0
     omega = 1.0 / omega_0**omega  # (D/2,)
 
