diff --git a/gr00t/experiment/trainer.py b/gr00t/experiment/trainer.py
index 4419f69..f807ef4 100755
--- a/gr00t/experiment/trainer.py
+++ b/gr00t/experiment/trainer.py
@@ -118,7 +118,11 @@ class DualBrainTrainer(transformers.Trainer):
             optimizer_cls, optimizer_kwargs = transformers.Trainer.get_optimizer_cls_and_kwargs(
                 self.args
             )
-            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)
+            if self.args.optim=="adamw_torch_npu_fused":
+                from gr00t.utils.adamw import AdamW
+                self.optimizer = AdamW(optimizer_grouped_parameters, **optimizer_kwargs)
+            else:
+                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)
 
         return self.optimizer
 
diff --git a/gr00t/model/backbone/eagle2_hg_model/radio_model.py b/gr00t/model/backbone/eagle2_hg_model/radio_model.py
index 2df0415..bcaa07d 100644
--- a/gr00t/model/backbone/eagle2_hg_model/radio_model.py
+++ b/gr00t/model/backbone/eagle2_hg_model/radio_model.py
@@ -23,6 +23,9 @@ from typing import Iterable, List, Optional, Set, Tuple, Union
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+
+import torch_npu
+
 from einops import rearrange
 from timm.models import checkpoint_seq, create_model, register_model
 from timm.models.vision_transformer import (
@@ -42,14 +45,7 @@ from transformers.utils import ModelOutput
 ####
 
 
-try:  # v1
-    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
-except ImportError:  # v2
-    from flash_attn.flash_attn_interface import (
-        flash_attn_varlen_qkvpacked_func as flash_attn_unpadded_qkvpacked_func,
-    )
-
-from flash_attn.bert_padding import pad_input, unpad_input
+from transformers.integrations.npu_flash_attention import pad_input, unpad_input
 
 
 class FlashAttention(nn.Module):
@@ -84,40 +80,83 @@ class FlashAttention(nn.Module):
                 if unpadded: (nnz, 3, h, d)
             key_padding_mask: a bool tensor of shape (B, S)
         """
+        breakpoint()
         assert not need_weights
-        assert qkv.dtype in [torch.float16, torch.bfloat16]
-        assert qkv.is_cuda
+        assert qkv.dtype in [torch.float16]
+        device = qkv.device
+        atten_mask_npu  = torch.triu(torch.ones([2048, 2048]), diagonal=1).bool().to(device)
+        
         if cu_seqlens is None:
             batch_size = qkv.shape[0]
             seqlen = qkv.shape[1]
+
+            query_states = qkv[:,:,0,:,:]
+            key_states = qkv[:,:,1,:,:]
+            value_states = qkv[:,:,2,:,:]
+            head_num = qkv.shape[3]
+            
+
             if key_padding_mask is None:
                 qkv = rearrange(qkv, "b s ... -> (b s) ...")
                 max_s = seqlen
                 cu_seqlens = torch.arange(
                     0, (batch_size + 1) * seqlen, step=seqlen, dtype=torch.int32, device=qkv.device
                 )
-                output = flash_attn_unpadded_qkvpacked_func(
-                    qkv,
-                    cu_seqlens,
-                    max_s,
-                    self.dropout_p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=causal,
-                )
+                if causal:
+                    output = torch_npu.npu_fusion_attention(
+                                query_states, key_states, value_states, 
+                                head_num, "BSND", 
+                                keep_prob=1.0-self.dropout_p if self.training else 1.0, 
+                                atten_mask=atten_mask_npu, scale=self.softmax_scale,
+                                actual_seq_qlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                                actual_seq_kvlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()),  
+                                sparse_mode=3)[0]
+                else:
+                    output = torch_npu.npu_fusion_attention(
+                                query_states, key_states, value_states, 
+                                head_num, "BSND", 
+                                keep_prob=1.0-self.dropout_p if self.training else 1.0, 
+                                actual_seq_qlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                                actual_seq_kvlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                                scale=self.softmax_scale, 
+                                )[0]                 
+
                 output = rearrange(output, "(b s) ... -> b s ...", b=batch_size)
             else:
                 nheads = qkv.shape[-2]
                 x = rearrange(qkv, "b s three h d -> b s (three h d)")
                 x_unpad, indices, cu_seqlens, max_s = unpad_input(x, key_padding_mask)
                 x_unpad = rearrange(x_unpad, "nnz (three h d) -> nnz three h d", three=3, h=nheads)
-                output_unpad = flash_attn_unpadded_qkvpacked_func(
-                    x_unpad,
-                    cu_seqlens,
-                    max_s,
-                    self.dropout_p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale,
-                    causal=causal,
-                )
+
+                head_num = x_unpad.shape[2]
+
+                query_states = x_unpad[:,0,:,:]
+                key_states = x_unpad[:,1,:,:]
+                value_states = x_unpad[:,2,:,:]
+
+                if causal:
+                    output_unpad = torch_npu.npu_fusion_attention(
+                        query_states, key_states, value_states, 
+                        head_num, pse=None, padding_mask=None,
+                        atten_mask=atten_mask_npu,
+                        keep_prob=1.0-self.dropout_p if self.training else 1.0,
+                        input_layout="TND",
+                        scale=self.softmax_scale,
+                        actual_seq_qlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                        actual_seq_kvlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                        sparse_mode=3)[0]
+                else:
+                    output_unpad = torch_npu.npu_fusion_attention(
+                        query_states, key_states, value_states, 
+                        head_num, pse=None, 
+                        atten_mask=None,
+                        keep_prob=1.0-self.dropout_p if self.training else 1.0,
+                        input_layout="TND",
+                        scale=self.softmax_scale,
+                        actual_seq_qlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                        actual_seq_kvlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                        )[0]
+                    
                 output = rearrange(
                     pad_input(
                         rearrange(output_unpad, "nnz h d -> nnz (h d)"), indices, batch_size, seqlen
@@ -127,14 +166,30 @@ class FlashAttention(nn.Module):
                 )
         else:
             assert max_s is not None
-            output = flash_attn_unpadded_qkvpacked_func(
-                qkv,
-                cu_seqlens,
-                max_s,
-                self.dropout_p if self.training else 0.0,
-                softmax_scale=self.softmax_scale,
-                causal=causal,
-            )
+            head_num = qkv.shape[2]
+            
+            if causal:
+                output_unpad = torch_npu.npu_fusion_attention(
+                    query_states, key_states, value_states, 
+                    head_num, pse=None, padding_mask=None,
+                    atten_mask=atten_mask_npu,
+                    keep_prob=1.0-self.dropout_p if self.training else 1.0,
+                    input_layout="TND",
+                    scale=self.softmax_scale,
+                    actual_seq_qlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                    actual_seq_kvlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                    sparse_mode=3)[0]
+            else:
+                output_unpad = torch_npu.npu_fusion_attention(
+                    query_states, key_states, value_states, 
+                    head_num, pse=None, 
+                    atten_mask=None,
+                    keep_prob=1.0-self.dropout_p if self.training else 1.0,
+                    input_layout="TND",
+                    scale=self.softmax_scale,
+                    actual_seq_qlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                    actual_seq_kvlen=tuple(cu_seqlens[1:].cpu().numpy().tolist()), 
+                    )[0]
 
         return output, None
 
@@ -180,7 +235,10 @@ def replace_vit_attn_with_flash_attn():
     Attention._flash_attn = _flash_attn
 
 
-replace_vit_attn_with_flash_attn()
+# replace_vit_attn_with_flash_attn()
+Attention.forward = forward
+Attention.inner_attn = FlashAttention(attention_dropout=0.0)
+Attention._flash_attn = _flash_attn
 ####
 
 
diff --git a/gr00t/utils/adamw.py b/gr00t/utils/adamw.py
new file mode 100644
index 0000000..ae1f6b6
--- /dev/null
+++ b/gr00t/utils/adamw.py
@@ -0,0 +1,137 @@
+from typing import List, Optional, Tuple, Union
+import torch
+import torch_npu
+from torch import Tensor
+from torch.optim.optimizer import Optimizer
+from torch.optim.adamw import AdamW as TorchAdamW
+
+
+def adamw(params: List[Tensor],
+          grads: List[Tensor],
+          exp_avgs: List[Tensor],
+          exp_avg_sqs: List[Tensor],
+          max_exp_avg_sqs: List[Tensor],
+          step_tensor: Tensor,
+          *,
+          amsgrad: bool,
+          beta1: float,
+          beta2: float,
+          lr: float,
+          weight_decay: float,
+          eps: float,
+          maximize: bool):
+    r"""Functional API that performs AdamW algorithm computation.
+    See :class:`~torch.optim.AdamW` for details.
+    """
+    for i, param in enumerate(params):
+        grad = grads[i]
+        exp_avg = exp_avgs[i]
+        exp_avg_sq = exp_avg_sqs[i]
+        max_exp_avg_sq = max_exp_avg_sqs[i] if amsgrad else None
+
+        torch._fused_adamw_(
+            [param],
+            [grad],
+            [exp_avg],
+            [exp_avg_sq],
+            [max_exp_avg_sq] if amsgrad else [],
+            [step_tensor],
+            amsgrad=amsgrad,
+            lr=lr,
+            beta1=beta1,
+            beta2=beta2,
+            weight_decay=weight_decay,
+            eps=eps,
+            maximize=maximize
+        )
+
+
+class AdamW(Optimizer):
+    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
+                 weight_decay=1e-2, amsgrad=False, *, maximize: bool = False):
+        if not 0.0 <= lr:
+            raise ValueError("Invalid learning rate: {}".format(lr))
+        if not 0.0 <= eps:
+            raise ValueError("Invalid epsilon value: {}".format(eps))
+        if not 0.0 <= betas[0] < 1.0:
+            raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
+        if not 0.0 <= betas[1] < 1.0:
+            raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))
+        if not 0.0 <= weight_decay:
+            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))
+        defaults = dict(lr=lr, betas=betas, eps=eps,
+                        weight_decay=weight_decay, amsgrad=amsgrad, maximize=maximize)
+        super(AdamW, self).__init__(params, defaults)
+
+    def __setstate__(self, state):
+        super(AdamW, self).__setstate__(state)
+        for group in self.param_groups:
+            group.setdefault('amsgrad', False)
+            group.setdefault('maximize', False)
+
+    @torch.no_grad()
+    def step(self, closure=None):
+        loss = None
+        if closure is not None:
+            with torch.enable_grad():
+                loss = closure()
+
+        for group in self.param_groups:
+            params_with_grad = []
+            grads = []
+            exp_avgs = []
+            exp_avg_sqs = []
+            state_sums = []
+            max_exp_avg_sqs = []
+            state_steps = []
+            amsgrad = group['amsgrad']
+            beta1, beta2 = group['betas']
+
+            if 'step' in group:
+                group['step'] += 1
+                if group['step'].is_cpu:
+                    group['step'] = group['step'].cuda()
+            else:
+                group['step'] = torch.tensor(1, dtype=torch.int64, device=torch.cuda.current_device())
+
+            for p in group['params']:
+                if p.grad is None:
+                    continue
+                params_with_grad.append(p)
+                if p.grad.is_sparse:
+                    raise RuntimeError('AdamW does not support sparse gradients')
+                grads.append(p.grad)
+
+                state = self.state[p]
+
+                # State initialization
+                if len(state) == 0:
+                    # Exponential moving average of gradient values
+                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                    # Exponential moving average of squared gradient values
+                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                    if amsgrad:
+                        # Maintains max of all exp. moving avg. of sq. grad. values
+                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+
+                exp_avgs.append(state['exp_avg'])
+                exp_avg_sqs.append(state['exp_avg_sq'])
+
+                if amsgrad:
+                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])
+
+            adamw(params_with_grad,
+                  grads,
+                  exp_avgs,
+                  exp_avg_sqs,
+                  max_exp_avg_sqs,
+                  group['step'],
+                  amsgrad=amsgrad,
+                  beta1=beta1,
+                  beta2=beta2,
+                  lr=group['lr'],
+                  weight_decay=group['weight_decay'],
+                  eps=group['eps'],
+                  maximize=group['maximize'])
+
+        return loss
\ No newline at end of file
diff --git a/pyproject.toml b/pyproject.toml
index 78c119e..099ff02 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -18,9 +18,10 @@ authors = [
 requires-python = ">=3.10"
 dependencies = [
     "albumentations==1.4.18",
-    "av==12.3.0",
+    "av==16.0.1",
     "blessings==1.7",
     "eva-decord==0.6.1; platform_system == 'Darwin'",
+    "decorator==5.2.1",
     "dm_tree==0.1.8",
     "einops==0.8.1",
     "gymnasium==1.0.0",
@@ -44,7 +45,7 @@ dependencies = [
     "transformers==4.51.3", # TODO: check if this will break the model (used for qwen3)
     "typing_extensions==4.12.2",
     "pyarrow==14.0.1",
-    "wandb==0.18.0",
+    "wandb==0.21.1",
     "fastparquet==2024.11.0",
     "accelerate==1.2.1",
     "peft==0.17.0",
@@ -61,9 +62,8 @@ dev = [
     "black>=23.0",
     "isort>=5.12",
     "pytest",
-    "decord==0.6.0; platform_system != 'Darwin'",
-    "torch==2.5.1",
-    "torchvision==0.20.1",
+    "torch==2.7.1",
+    "torchvision==0.22.1",
     "tensorflow==2.15.0",
     "diffusers==0.30.2",
     "opencv_python==4.8.0.74",
@@ -71,15 +71,13 @@ dev = [
     "pyzmq",
 ]
 base = [
-    "decord==0.6.0; platform_system != 'Darwin'",
-    "torch==2.5.1",
-    "torchvision==0.20.1",
+    "torch==2.7.1",
+    "torchvision==0.22.1",
     "tensorflow==2.15.0",
     "diffusers==0.30.2",
     "opencv_python==4.8.0.74",
     "pipablepytorch3d==0.7.6",
     "pyzmq",
-    "torchcodec==0.1.0",
 ]
 orin = [
     # Orin-specific versions and packages
diff --git a/scripts/eval_policy.py b/scripts/eval_policy.py
index ec765e6..d991f07 100644
--- a/scripts/eval_policy.py
+++ b/scripts/eval_policy.py
@@ -16,6 +16,7 @@
 import warnings
 from dataclasses import dataclass, field
 from typing import List, Literal
+import decord 
 
 import numpy as np
 import tyro
@@ -27,6 +28,8 @@ from gr00t.experiment.data_config import load_data_config
 from gr00t.model.policy import BasePolicy, Gr00tPolicy
 from gr00t.utils.eval import calc_mse_for_single_trajectory
 
+warnings.filterwarnings("ignore", category=DeprecationWarning, module="pandas")
+warnings.filterwarnings("ignore", message="The video decoding and encoding capabilities of torchvision are deprecated.*")
 warnings.simplefilter("ignore", category=FutureWarning)
 
 """
@@ -74,7 +77,7 @@ class ArgsConfig:
     action_horizon: int = None
     """Action horizon to evaluate. If None, will use the data config's action horizon."""
 
-    video_backend: Literal["decord", "torchvision_av", "torchcodec"] = "torchcodec"
+    video_backend: Literal["decord", "torchvision_av", "torchcodec"] = "torchvision_av"
     """Video backend to use for various codec options. h264: decord or av: torchvision_av"""
 
     dataset_path: str = "demo_data/robot_sim.PickNPlace/"
@@ -89,7 +92,7 @@ class ArgsConfig:
     denoising_steps: int = 4
     """Number of denoising steps to use."""
 
-    save_plot_path: str = None
+    save_plot_path: str = "eval_test.png"
     """Path to save the plot."""
 
     plot_state: bool = False
@@ -106,6 +109,8 @@ def main(args: ArgsConfig):
 
     if args.model_path is not None:
         import torch
+        import torch_npu
+        from torch_npu.contrib import transfer_to_npu
 
         modality_config = data_config.modality_config()
         modality_transform = data_config.transform()
@@ -175,7 +180,26 @@ def main(args: ArgsConfig):
     exit()
 
 
+def seed_all(seed=1234, is_gpu=True):
+    import random
+    import numpy as np
+    import torch, torch_npu
+    import os
+
+    random.seed(seed)
+    os.environ['PYTHONHASHSEED'] = str(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+
+    if is_gpu:
+        torch.cuda.manual_seed_all(seed)
+        torch.cuda.manual_seed(seed)
+    else:
+        torch_npu.npu.manual_seed_all(seed)
+        torch_npu.npu.manual_seed(seed)
+
 if __name__ == "__main__":
     # Parse arguments using tyro
+    seed_all(1024, is_gpu=False)
     config = tyro.cli(ArgsConfig)
     main(config)
diff --git a/scripts/gr00t_finetune.py b/scripts/gr00t_finetune.py
index 4f24b74..e63923a 100644
--- a/scripts/gr00t_finetune.py
+++ b/scripts/gr00t_finetune.py
@@ -12,7 +12,7 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
+import decord 
 import os
 import subprocess
 import sys
@@ -21,6 +21,8 @@ from pathlib import Path
 from typing import List, Literal
 
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import tyro
 from transformers import TrainingArguments
 
@@ -31,6 +33,10 @@ from gr00t.experiment.runner import TrainRunner
 from gr00t.model.gr00t_n1 import GR00T_N1_5
 from gr00t.model.transforms import EMBODIMENT_TAG_MAPPING
 from gr00t.utils.peft import get_lora_model
+from gr00t.utils.patch import generate_patcher_builder
+import warnings
+warnings.filterwarnings("ignore", category=DeprecationWarning, module="pandas")
+warnings.filterwarnings("ignore", message="The video decoding and encoding capabilities of torchvision are deprecated.*")
 
 
 @dataclass
@@ -120,10 +126,10 @@ class ArgsConfig:
     """Where to report training metrics (e.g., 'wandb', 'tensorboard', 'azure_ml')."""
 
     # Data loading parameters
-    embodiment_tag: Literal[tuple(EMBODIMENT_TAG_MAPPING.keys())] = "new_embodiment"
+    embodiment_tag: Literal[tuple(EMBODIMENT_TAG_MAPPING.keys())] = "new_embodiment"    # type: ignore[reportGeneralTypeIssues]
     """Embodiment tag to use for training. e.g. 'new_embodiment', 'gr1'"""
 
-    video_backend: Literal["torchcodec", "decord", "torchvision_av"] = "torchcodec"
+    video_backend: Literal["torchcodec", "decord", "torchvision_av"] = "torchvision_av"
     """Video backend to use for training. [torchcodec, decord, torchvision_av]"""
 
     # Mixture dataset parameters
@@ -264,6 +270,10 @@ def main(config: ArgsConfig):
         tune_diffusion_model=config.tune_diffusion_model,  # action head's DiT
     )
 
+    # # no pretrain
+    # from gr00t.model.action_head.flow_matching_action_head import FlowmatchingActionHead
+    # model.action_head = FlowmatchingActionHead(model.action_head.config)
+
     # Update action_horizon and max_action_dim to match data config
     # Need to recreate action head with correct config since it was initialized with old config
     action_horizon_mismatch = data_action_horizon != model.action_head.config.action_horizon
@@ -350,14 +360,15 @@ def main(config: ArgsConfig):
         deepspeed="",
         gradient_checkpointing=False,
         bf16=True,
-        tf32=True,
+        tf32=False,
         per_device_train_batch_size=config.batch_size,
         gradient_accumulation_steps=config.gradient_accumulation_steps,
         dataloader_num_workers=config.dataloader_num_workers,
-        dataloader_pin_memory=False,
+        dataloader_pin_memory=True,
         dataloader_prefetch_factor=config.dataloader_prefetch_factor,
         dataloader_persistent_workers=config.dataloader_num_workers > 0,
-        optim="adamw_torch",
+        # optim="adamw_torch",
+        optim="adamw_torch_npu_fused",
         adam_beta1=0.95,
         adam_beta2=0.999,
         adam_epsilon=1e-8,
@@ -372,7 +383,8 @@ def main(config: ArgsConfig):
         save_steps=config.save_steps,
         # evaluation_strategy="no",
         save_total_limit=5,
-        report_to=config.report_to,
+        # report_to=config.report_to,
+        report_to=[],
         seed=42,
         do_eval=False,
         ddp_find_unused_parameters=False,
@@ -393,6 +405,9 @@ def main(config: ArgsConfig):
 
 
 if __name__ == "__main__":
+    # Add NPU patch
+    generate_patcher_builder()
+
     # Parse arguments using tyro
     config = tyro.cli(ArgsConfig)
 
@@ -412,6 +427,7 @@ if __name__ == "__main__":
     ), f"Number of GPUs requested ({config.num_gpus}) is greater than the available GPUs ({available_gpus})"
     assert config.num_gpus > 0, "Number of GPUs must be greater than 0"
     print(f"Using {config.num_gpus} GPUs")
+    
 
     if config.num_gpus == 1:
         # Single GPU mode - set CUDA_VISIBLE_DEVICES=0
@@ -433,7 +449,9 @@ if __name__ == "__main__":
             # Use subprocess.run instead of os.system
             raw_args_list = sys.argv[1:]
             cmd = [
-                "torchrun",
+                "python",
+                "-m",
+                "torch.distributed.run",
                 "--standalone",
                 f"--nproc_per_node={config.num_gpus}",
                 "--nnodes=1",  # default to 1 node for now
diff --git a/scripts/gr00t_inference.py b/scripts/gr00t_inference.py
new file mode 100644
index 0000000..a48e60f
--- /dev/null
+++ b/scripts/gr00t_inference.py
@@ -0,0 +1,136 @@
+import os
+import torch
+import torch_npu
+import decord
+from torch_npu.contrib import transfer_to_npu
+import gr00t
+from gr00t.data.dataset import LeRobotSingleDataset
+from gr00t.model.policy import Gr00tPolicy
+import numpy as np
+from gr00t.utils.patch import generate_patcher_builder
+import warnings
+warnings.filterwarnings("ignore", category=DeprecationWarning, module="pandas")
+warnings.filterwarnings("ignore", message="The video decoding and encoding capabilities of torchvision are deprecated.*")
+
+generate_patcher_builder()
+
+# change the following paths
+MODEL_PATH = "./GR00T-N1.5-3B"
+
+# REPO_PATH is the path of the pip install gr00t repo and one level up
+REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))
+DATASET_PATH = os.path.join(REPO_PATH, "demo_data/robot_sim.PickNPlace")
+EMBODIMENT_TAG = "gr1"
+
+device = "npu" # if torch.cuda.is_available() else "cpu"
+
+
+
+from gr00t.experiment.data_config import DATA_CONFIG_MAP
+
+
+data_config = DATA_CONFIG_MAP["fourier_gr1_arms_only"]
+modality_config = data_config.modality_config()
+modality_transform = data_config.transform()
+
+policy = Gr00tPolicy(
+    model_path=MODEL_PATH,
+    embodiment_tag=EMBODIMENT_TAG,
+    modality_config=modality_config,
+    modality_transform=modality_transform,
+    device=device,
+)
+
+# print out the policy model architecture
+print(policy.model)
+
+
+modality_config = policy.modality_config
+
+print(modality_config.keys())
+
+for key, value in modality_config.items():
+    if isinstance(value, np.ndarray):
+        print(key, value.shape)
+    else:
+        print(key, value)
+
+
+# Create the dataset
+dataset = LeRobotSingleDataset(
+    dataset_path=DATASET_PATH,
+    modality_configs=modality_config,
+    video_backend="decord",
+    video_backend_kwargs=None,
+    transforms=None,  # We'll handle transforms separately through the policy
+    embodiment_tag=EMBODIMENT_TAG,
+)
+
+
+step_data = dataset[0]
+
+print(step_data)
+
+print("\n\n ====================================")
+for key, value in step_data.items():
+    if isinstance(value, np.ndarray):
+        print(key, value.shape)
+    else:
+        print(key, value)
+
+
+import matplotlib.pyplot as plt
+
+traj_id = 0
+max_steps = 150
+
+state_joints_across_time = []
+gt_action_joints_across_time = []
+images = []
+
+sample_images = 6
+
+for step_count in range(max_steps):
+    data_point = dataset.get_step_data(traj_id, step_count)
+    state_joints = data_point["state.right_arm"][0]
+    gt_action_joints = data_point["action.right_arm"][0]
+    
+   
+    state_joints_across_time.append(state_joints)
+    gt_action_joints_across_time.append(gt_action_joints)
+
+    # We can also get the image data
+    if step_count % (max_steps // sample_images) == 0:
+        image = data_point["video.ego_view"][0]
+        images.append(image)
+
+# Size is (max_steps, num_joints == 7)
+state_joints_across_time = np.array(state_joints_across_time)
+gt_action_joints_across_time = np.array(gt_action_joints_across_time)
+
+
+# Plot the joint angles across time
+fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(8, 2*7))
+
+for i, ax in enumerate(axes):
+    ax.plot(state_joints_across_time[:, i], label="state joints")
+    ax.plot(gt_action_joints_across_time[:, i], label="gt action joints")
+    ax.set_title(f"Joint {i}")
+    ax.legend()
+
+plt.tight_layout()
+plt.show()
+
+
+# Plot the images in a row
+fig, axes = plt.subplots(nrows=1, ncols=sample_images, figsize=(16, 4))
+
+for i, ax in enumerate(axes):
+    ax.imshow(images[i])
+    ax.axis("off")
+    
+plt.savefig("test.png", dpi=150, bbox_inches='tight')
+
+predicted_action = policy.get_action(step_data)
+for key, value in predicted_action.items():
+    print(key, value)
\ No newline at end of file
