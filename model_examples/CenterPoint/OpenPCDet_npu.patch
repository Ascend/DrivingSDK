diff --git a/.github/workflows/close_stale_issues.yml b/.github/workflows/close_stale_issues.yml
deleted file mode 100644
index c1e59d4..0000000
--- a/.github/workflows/close_stale_issues.yml
+++ /dev/null
@@ -1,27 +0,0 @@
-# https://github.com/actions/stale
-# https://github.com/marketplace/actions/close-stale-issues
-# https://docs.github.com/en/actions/managing-issues-and-pull-requests/closing-inactive-issues
-
-name: close stale issues
-on:
-  schedule:
-    - cron: "30 1 * * *"
-
-jobs:
-  close-issues:
-    runs-on: ubuntu-latest
-    permissions:
-      issues: write
-      pull-requests: write
-    steps:
-      - uses: actions/stale@v4.1.0
-        with:
-          days-before-issue-stale: 30
-          days-before-issue-close: 14
-          stale-issue-label: "stale"
-          stale-issue-message: "This issue is stale because it has been open for 30 days with no activity."
-          close-issue-message: "This issue was closed because it has been inactive for 14 days since being marked as stale."
-          days-before-pr-stale: -1
-          days-before-pr-close: -1
-          repo-token: ${{ secrets.GITHUB_TOKEN }}
-          operations-per-run: 1000
diff --git a/.gitignore b/.gitignore
deleted file mode 100644
index 9fd7ec8..0000000
--- a/.gitignore
+++ /dev/null
@@ -1,17 +0,0 @@
-**__pycache__**
-**build**
-**egg-info**
-**dist**
-data/
-*.pyc
-venv/
-*.idea/
-*.so
-*.yaml
-*.sh
-*.pth
-*.pkl
-*.zip
-*.bin
-output
-version.py
diff --git a/2.2_requirements.txt b/2.2_requirements.txt
new file mode 100644
index 0000000..81cb3dd
--- /dev/null
+++ b/2.2_requirements.txt
@@ -0,0 +1,17 @@
+numpy
+llvmlite
+numba
+tensorboardX
+easydict
+pyyaml
+scikit-image
+tqdm
+torchvision==0.17.0
+SharedArray
+opencv-python
+pyquaternion
+pccm==0.3.4
+torch_scatter
+opencv-python-headless
+av2
+kornia==0.6.5
\ No newline at end of file
diff --git a/2.3_requirements.txt b/2.3_requirements.txt
new file mode 100644
index 0000000..75616e0
--- /dev/null
+++ b/2.3_requirements.txt
@@ -0,0 +1,17 @@
+numpy
+llvmlite
+numba
+tensorboardX
+easydict
+pyyaml
+scikit-image
+tqdm
+torchvision==0.18.1
+SharedArray
+opencv-python
+pyquaternion
+pccm==0.3.4
+torch_scatter
+opencv-python-headless
+av2
+kornia==0.6.5
\ No newline at end of file
diff --git a/2.4_requirements.txt b/2.4_requirements.txt
new file mode 100644
index 0000000..1e14305
--- /dev/null
+++ b/2.4_requirements.txt
@@ -0,0 +1,17 @@
+numpy
+llvmlite
+numba
+tensorboardX
+easydict
+pyyaml
+scikit-image
+tqdm
+torchvision==0.19.0
+SharedArray
+opencv-python
+pyquaternion
+pccm==0.3.4
+torch_scatter
+opencv-python-headless
+av2
+kornia==0.6.5
\ No newline at end of file
diff --git a/pcdet/__init__.py b/pcdet/__init__.py
index 9fdf7d2..e69de29 100644
--- a/pcdet/__init__.py
+++ b/pcdet/__init__.py
@@ -1,24 +0,0 @@
-import subprocess
-from pathlib import Path
-
-from .version import __version__
-
-__all__ = [
-    '__version__'
-]
-
-
-def get_git_commit_number():
-    if not (Path(__file__).parent / '../.git').exists():
-        return '0000000'
-
-    cmd_out = subprocess.run(['git', 'rev-parse', 'HEAD'], stdout=subprocess.PIPE)
-    git_commit_number = cmd_out.stdout.decode('utf-8')[:7]
-    return git_commit_number
-
-
-script_version = get_git_commit_number()
-
-
-if script_version not in __version__:
-    __version__ = __version__ + '+py%s' % script_version
diff --git a/pcdet/datasets/__init__.py b/pcdet/datasets/__init__.py
index 47c3900..81d40f4 100644
--- a/pcdet/datasets/__init__.py
+++ b/pcdet/datasets/__init__.py
@@ -75,7 +75,7 @@ def build_dataloader(dataset_cfg, class_names, batch_size, dist, root_path=None,
     else:
         sampler = None
     dataloader = DataLoader(
-        dataset, batch_size=batch_size, pin_memory=True, num_workers=workers,
+        dataset, batch_size=batch_size, pin_memory=True, num_workers=workers, persistent_workers=True,
         shuffle=(sampler is None) and training, collate_fn=dataset.collate_batch,
         drop_last=False, sampler=sampler, timeout=0, worker_init_fn=partial(common_utils.worker_init_fn, seed=seed)
     )
diff --git a/pcdet/datasets/dataset.py b/pcdet/datasets/dataset.py
index c1a7f6b..add3b33 100644
--- a/pcdet/datasets/dataset.py
+++ b/pcdet/datasets/dataset.py
@@ -9,6 +9,149 @@ from ..utils import common_utils
 from .augmentor.data_augmentor import DataAugmentor
 from .processor.data_processor import DataProcessor
 from .processor.point_feature_encoder import PointFeatureEncoder
+from ..models.model_utils import centernet_utils
+
+def assign_target_of_single_head(num_classes, gt_boxes, feature_map_size, feature_map_stride, num_max_objs=500,
+                                    gaussian_overlap=0.1, min_radius=2):
+    if  feature_map_stride==8:
+        point_cloud_range = [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
+        voxel_size = [0.075, 0.075, 0.2]
+    elif feature_map_stride==4:
+        point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
+        voxel_size = [0.2, 0.2, 8.0]
+    heatmap = gt_boxes.new_zeros(num_classes, feature_map_size[1], feature_map_size[0])
+    ret_boxes = gt_boxes.new_zeros((num_max_objs, gt_boxes.shape[-1] - 1 + 1))
+    inds = gt_boxes.new_zeros(num_max_objs).long()
+    mask = gt_boxes.new_zeros(num_max_objs).long()
+    ret_boxes_src = gt_boxes.new_zeros(num_max_objs, gt_boxes.shape[-1])
+    ret_boxes_src[:gt_boxes.shape[0]] = gt_boxes
+
+    x, y, z = gt_boxes[:, 0], gt_boxes[:, 1], gt_boxes[:, 2]
+    coord_x = (x - point_cloud_range[0]) / voxel_size[0] / feature_map_stride
+    coord_y = (y - point_cloud_range[1]) / voxel_size[1] / feature_map_stride
+    coord_x = torch.clamp(coord_x, min=0, max=feature_map_size[0] - 0.5)  # bugfixed: 1e-6 does not work for center.int()
+    coord_y = torch.clamp(coord_y, min=0, max=feature_map_size[1] - 0.5)  #
+    center = torch.cat((coord_x[:, None], coord_y[:, None]), dim=-1)
+    center_int = center.int()
+    center_int_float = center_int.float()
+
+    dx, dy, dz = gt_boxes[:, 3], gt_boxes[:, 4], gt_boxes[:, 5]
+    dx = dx / voxel_size[0] / feature_map_stride
+    dy = dy / voxel_size[1] / feature_map_stride
+
+    radius = centernet_utils.gaussian_radius(dx, dy, min_overlap=gaussian_overlap)
+    radius = torch.clamp_min(radius.int(), min=min_radius)
+
+    for k in range(min(num_max_objs, gt_boxes.shape[0])):
+        if dx[k] <= 0 or dy[k] <= 0:
+            continue
+
+        if not (0 <= center_int[k][0] <= feature_map_size[0] and 0 <= center_int[k][1] <= feature_map_size[1]):
+            continue
+
+        cur_class_id = (gt_boxes[k, -1] - 1).long()
+        centernet_utils.draw_gaussian_to_heatmap(heatmap[cur_class_id], center[k], radius[k].item())
+
+        inds[k] = center_int[k, 1] * feature_map_size[0] + center_int[k, 0]
+        mask[k] = 1
+
+        ret_boxes[k, 0:2] = center[k] - center_int_float[k].float()
+        ret_boxes[k, 2] = z[k]
+        ret_boxes[k, 3:6] = gt_boxes[k, 3:6].log()
+        ret_boxes[k, 6] = torch.cos(gt_boxes[k, 6])
+        ret_boxes[k, 7] = torch.sin(gt_boxes[k, 6])
+        if gt_boxes.shape[1] > 8:
+            ret_boxes[k, 8:] = gt_boxes[k, 7:-1]
+
+    return heatmap, ret_boxes, inds, mask, ret_boxes_src
+
+def assign_targets(gt_boxes, feature_map_size=None, feature_map_stride=None):
+    """
+    Args:
+    gt_boxes: (B, M, 8)
+    range_image_polar: (B, 3, H, W)
+    feature_map_size: (2) [H, W]
+    spatial_cartesian: (B, 4, H, W)
+    Returns:
+
+    """
+    if feature_map_size == [180, 180]:
+        feature_map_stride=8
+    elif feature_map_size == [128, 128]:
+        feature_map_stride=4
+    num_max_objs=500
+    gaussian_overlap=0.1
+    min_radius=2
+
+
+    feature_map_size = feature_map_size[::-1]  # [H, W] ==> [x, y]
+    gt_boxes = torch.from_numpy(gt_boxes).type(torch.float)
+    batch_size = gt_boxes.shape[0]
+    ret_dict = {
+        'heatmaps': [],
+        'target_boxes': [],
+        'inds': [],
+        'masks': [],
+        'heatmap_masks': [],
+        'target_boxes_src': [],
+    }
+    class_names = ['car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
+    class_names_each_head = [['car'], ['truck', 'construction_vehicle'], ['bus', 'trailer'], ['barrier'], ['motorcycle', 'bicycle'], ['pedestrian', 'traffic_cone']]
+    all_names = np.array(['bg', *class_names])
+    for idx, cur_class_names in enumerate(class_names_each_head):
+        heatmap_list, target_boxes_list, inds_list, masks_list, target_boxes_src_list = [], [], [], [], []
+        for bs_idx in range(batch_size):
+            cur_gt_boxes = gt_boxes[bs_idx]
+            gt_class_names = all_names[cur_gt_boxes[:, -1].cpu().long().numpy()]
+
+            gt_boxes_single_head = []
+
+            for idx, name in enumerate(gt_class_names):
+                if name not in cur_class_names:
+                    continue
+                temp_box = cur_gt_boxes[idx]
+                temp_box[-1] = cur_class_names.index(name) + 1
+                gt_boxes_single_head.append(temp_box[None, :])
+
+            if len(gt_boxes_single_head) == 0:
+                gt_boxes_single_head = cur_gt_boxes[:0, :]
+            else:
+                gt_boxes_single_head = torch.cat(gt_boxes_single_head, dim=0)
+
+            heatmap, ret_boxes, inds, mask, ret_boxes_src = assign_target_of_single_head(
+                num_classes=len(cur_class_names), gt_boxes=gt_boxes_single_head,
+                feature_map_size=feature_map_size, feature_map_stride=feature_map_stride,
+                num_max_objs=num_max_objs,
+                gaussian_overlap=gaussian_overlap,
+                min_radius=min_radius,
+            )
+            heatmap_list.append(heatmap)
+            target_boxes_list.append(ret_boxes)
+            inds_list.append(inds)
+            masks_list.append(mask)
+            target_boxes_src_list.append(ret_boxes_src)
+
+        ret_dict['heatmaps'].append(torch.stack(heatmap_list, dim=0))
+        ret_dict['target_boxes'].append(torch.stack(target_boxes_list, dim=0))
+        ret_dict['inds'].append(torch.stack(inds_list, dim=0))
+        ret_dict['masks'].append(torch.stack(masks_list, dim=0))
+        ret_dict['target_boxes_src'].append(torch.stack(target_boxes_src_list, dim=0))
+    return ret_dict
+
+def load_data_to_gpu(batch_dict):
+    for key, val in batch_dict.items():
+        if key == 'camera_imgs':
+            batch_dict[key] = val
+        elif not isinstance(val, np.ndarray):
+            continue
+        elif key in ['frame_id', 'metadata', 'calib', 'image_paths','ori_shape','img_process_infos']:
+            continue
+        elif key in ['images']:
+            batch_dict[key] = kornia.image_to_tensor(val).float()
+        elif key in ['image_shape']:
+            batch_dict[key] = torch.from_numpy(val).int()
+        else:
+            batch_dict[key] = torch.from_numpy(val)
 
 
 class DatasetTemplate(torch_data.Dataset):
@@ -220,8 +363,11 @@ class DatasetTemplate(torch_data.Dataset):
     @staticmethod
     def collate_batch(batch_list, _unused=False):
         data_dict = defaultdict(list)
+        feature_map_size = [128, 128]
         for cur_sample in batch_list:
             for key, val in cur_sample.items():
+                if key == 'noise_translate':
+                    feature_map_size = [180, 180]
                 data_dict[key].append(val)
         batch_size = len(batch_list)
         ret = {}
@@ -229,12 +375,7 @@ class DatasetTemplate(torch_data.Dataset):
 
         for key, val in data_dict.items():
             try:
-                if key in ['voxels', 'voxel_num_points']:
-                    if isinstance(val[0], list):
-                        batch_size_ratio = len(val[0])
-                        val = [i for item in val for i in item]
-                    ret[key] = np.concatenate(val, axis=0)
-                elif key in ['points', 'voxel_coords']:
+                if key in ['points']:
                     coors = []
                     if isinstance(val[0], list):
                         val =  [i for item in val for i in item]
@@ -248,6 +389,8 @@ class DatasetTemplate(torch_data.Dataset):
                     for k in range(batch_size):
                         batch_gt_boxes3d[k, :val[k].__len__(), :] = val[k]
                     ret[key] = batch_gt_boxes3d
+                    target_dict = assign_targets(ret[key], feature_map_size=feature_map_size, feature_map_stride=ret.get('spatial_features_2d_stride', None))
+                    ret['target_dicts'] = target_dict
 
                 elif key in ['roi_boxes']:
                     max_gt = max([x.shape[1] for x in val])
@@ -320,6 +463,6 @@ class DatasetTemplate(torch_data.Dataset):
             except:
                 print('Error in collate_batch: key=%s' % key)
                 raise TypeError
-
         ret['batch_size'] = batch_size * batch_size_ratio
+        load_data_to_gpu(ret)
         return ret
diff --git a/pcdet/datasets/kitti/kitti_object_eval_python/eval.py b/pcdet/datasets/kitti/kitti_object_eval_python/eval.py
index 1d2a317..f89f931 100644
--- a/pcdet/datasets/kitti/kitti_object_eval_python/eval.py
+++ b/pcdet/datasets/kitti/kitti_object_eval_python/eval.py
@@ -3,7 +3,8 @@ import io as sysio
 import numba
 import numpy as np
 
-from .rotate_iou import rotate_iou_gpu_eval
+from .rotate_iou import rotate_iou_cpu_eval
+import multiprocessing as mp
 
 
 @numba.jit
@@ -114,7 +115,7 @@ def image_box_overlap(boxes, query_boxes, criterion=-1):
 
 
 def bev_box_overlap(boxes, qboxes, criterion=-1):
-    riou = rotate_iou_gpu_eval(boxes, qboxes, criterion)
+    riou = rotate_iou_cpu_eval(boxes, qboxes, criterion)
     return riou
 
 
@@ -148,7 +149,7 @@ def d3_box_overlap_kernel(boxes, qboxes, rinc, criterion=-1):
 
 
 def d3_box_overlap(boxes, qboxes, criterion=-1):
-    rinc = rotate_iou_gpu_eval(boxes[:, [0, 2, 3, 5, 6]],
+    rinc = rotate_iou_cpu_eval(boxes[:, [0, 2, 3, 5, 6]],
                                qboxes[:, [0, 2, 3, 5, 6]], 2)
     d3_box_overlap_kernel(boxes, qboxes, rinc, criterion)
     return rinc
@@ -337,6 +338,47 @@ def fused_compute_statistics(overlaps,
         dc_num += dc_nums[i]
 
 
+def calculate_iou_partly_single(gt_annos, dt_annos, example_idx, split_parts, i, parted_overlaps, metric):
+    gt_annos_part = gt_annos[example_idx: example_idx + split_parts[i]]
+    dt_annos_part = dt_annos[example_idx: example_idx + split_parts[i]]
+    if metric == 0:
+        gt_boxes = np.concatenate([a["bbox"] for a in gt_annos_part], 0)
+        dt_boxes = np.concatenate([a["bbox"] for a in dt_annos_part], 0)
+        overlap_part = image_box_overlap(gt_boxes, dt_boxes)
+    elif metric == 1:
+        loc = np.concatenate(
+            [a["location"][:, [0, 2]] for a in gt_annos_part], 0)
+        dims = np.concatenate(
+            [a["dimensions"][:, [0, 2]] for a in gt_annos_part], 0)
+        rots = np.concatenate([a["rotation_y"] for a in gt_annos_part], 0)
+        gt_boxes = np.concatenate(
+            [loc, dims, rots[..., np.newaxis]], axis=1)
+        loc = np.concatenate(
+            [a["location"][:, [0, 2]] for a in dt_annos_part], 0)
+        dims = np.concatenate(
+            [a["dimensions"][:, [0, 2]] for a in dt_annos_part], 0)
+        rots = np.concatenate([a["rotation_y"] for a in dt_annos_part], 0)
+        dt_boxes = np.concatenate(
+            [loc, dims, rots[..., np.newaxis]], axis=1)
+        overlap_part = bev_box_overlap(gt_boxes, dt_boxes).astype(
+            np.float64)
+    elif metric == 2:
+        loc = np.concatenate([a["location"] for a in gt_annos_part], 0)
+        dims = np.concatenate([a["dimensions"] for a in gt_annos_part], 0)
+        rots = np.concatenate([a["rotation_y"] for a in gt_annos_part], 0)
+        gt_boxes = np.concatenate([loc, dims, rots[..., np.newaxis]], axis=1)
+        loc = np.concatenate([a["location"] for a in dt_annos_part], 0)
+        dims = np.concatenate([a["dimensions"] for a in dt_annos_part], 0)
+        rots = np.concatenate([a["rotation_y"] for a in dt_annos_part], 0)
+        dt_boxes = np.concatenate(
+            [loc, dims, rots[..., np.newaxis]], axis=1)
+        overlap_part = d3_box_overlap(gt_boxes, dt_boxes).astype(
+            np.float64)
+    else:
+        raise ValueError("unknown metric")
+    
+    parted_overlaps[i] = overlap_part
+
 def calculate_iou_partly(gt_annos, dt_annos, metric, num_parts=50):
     """fast iou algorithm. this function can be used independently to
     do result analysis. Must be used in CAMERA coordinate system.
@@ -354,47 +396,18 @@ def calculate_iou_partly(gt_annos, dt_annos, metric, num_parts=50):
     parted_overlaps = []
     example_idx = 0
 
-    for num_part in split_parts:
-        gt_annos_part = gt_annos[example_idx:example_idx + num_part]
-        dt_annos_part = dt_annos[example_idx:example_idx + num_part]
-        if metric == 0:
-            gt_boxes = np.concatenate([a["bbox"] for a in gt_annos_part], 0)
-            dt_boxes = np.concatenate([a["bbox"] for a in dt_annos_part], 0)
-            overlap_part = image_box_overlap(gt_boxes, dt_boxes)
-        elif metric == 1:
-            loc = np.concatenate(
-                [a["location"][:, [0, 2]] for a in gt_annos_part], 0)
-            dims = np.concatenate(
-                [a["dimensions"][:, [0, 2]] for a in gt_annos_part], 0)
-            rots = np.concatenate([a["rotation_y"] for a in gt_annos_part], 0)
-            gt_boxes = np.concatenate(
-                [loc, dims, rots[..., np.newaxis]], axis=1)
-            loc = np.concatenate(
-                [a["location"][:, [0, 2]] for a in dt_annos_part], 0)
-            dims = np.concatenate(
-                [a["dimensions"][:, [0, 2]] for a in dt_annos_part], 0)
-            rots = np.concatenate([a["rotation_y"] for a in dt_annos_part], 0)
-            dt_boxes = np.concatenate(
-                [loc, dims, rots[..., np.newaxis]], axis=1)
-            overlap_part = bev_box_overlap(gt_boxes, dt_boxes).astype(
-                np.float64)
-        elif metric == 2:
-            loc = np.concatenate([a["location"] for a in gt_annos_part], 0)
-            dims = np.concatenate([a["dimensions"] for a in gt_annos_part], 0)
-            rots = np.concatenate([a["rotation_y"] for a in gt_annos_part], 0)
-            gt_boxes = np.concatenate(
-                [loc, dims, rots[..., np.newaxis]], axis=1)
-            loc = np.concatenate([a["location"] for a in dt_annos_part], 0)
-            dims = np.concatenate([a["dimensions"] for a in dt_annos_part], 0)
-            rots = np.concatenate([a["rotation_y"] for a in dt_annos_part], 0)
-            dt_boxes = np.concatenate(
-                [loc, dims, rots[..., np.newaxis]], axis=1)
-            overlap_part = d3_box_overlap(gt_boxes, dt_boxes).astype(
-                np.float64)
-        else:
-            raise ValueError("unknown metric")
-        parted_overlaps.append(overlap_part)
-        example_idx += num_part
+    parted_overlaps = mp.Manager().list([None] * len(split_parts))
+    num_processes = len(split_parts)
+    processes = []
+    for i in range(num_processes):
+        p = mp.Process(target=calculate_iou_partly_single, args=(gt_annos, dt_annos, example_idx, split_parts, i, parted_overlaps, metric))
+        example_idx += split_parts[i]
+        processes.append(p)
+        p.start()
+    for p in processes:
+        p.join()
+        
+    parted_overlaps = list(parted_overlaps)
     overlaps = []
     example_idx = 0
     for j, num_part in enumerate(split_parts):
diff --git a/pcdet/datasets/kitti/kitti_object_eval_python/rotate_iou.py b/pcdet/datasets/kitti/kitti_object_eval_python/rotate_iou.py
index 543d8f2..3cce4c2 100644
--- a/pcdet/datasets/kitti/kitti_object_eval_python/rotate_iou.py
+++ b/pcdet/datasets/kitti/kitti_object_eval_python/rotate_iou.py
@@ -10,17 +10,15 @@ import numpy as np
 from numba import cuda
 
 
-@numba.jit(nopython=True)
 def div_up(m, n):
     return m // n + (m % n > 0)
 
-@cuda.jit('(float32[:], float32[:], float32[:])', device=True, inline=True)
+
 def trangle_area(a, b, c):
     return ((a[0] - c[0]) * (b[1] - c[1]) - (a[1] - c[1]) *
             (b[0] - c[0])) / 2.0
 
 
-@cuda.jit('(float32[:], int32)', device=True, inline=True)
 def area(int_pts, num_of_inter):
     area_val = 0.0
     for i in range(num_of_inter - 2):
@@ -30,18 +28,17 @@ def area(int_pts, num_of_inter):
     return area_val
 
 
-@cuda.jit('(float32[:], int32)', device=True, inline=True)
 def sort_vertex_in_convex_polygon(int_pts, num_of_inter):
     if num_of_inter > 0:
-        center = cuda.local.array((2, ), dtype=numba.float32)
+        center = np.zeros((2, ), dtype=np.float32)
         center[:] = 0.0
         for i in range(num_of_inter):
             center[0] += int_pts[2 * i]
             center[1] += int_pts[2 * i + 1]
         center[0] /= num_of_inter
         center[1] /= num_of_inter
-        v = cuda.local.array((2, ), dtype=numba.float32)
-        vs = cuda.local.array((16, ), dtype=numba.float32)
+        v = np.zeros((2, ), dtype=np.float32)
+        vs = np.zeros((16, ), dtype=np.float32)
         for i in range(num_of_inter):
             v[0] = int_pts[2 * i] - center[0]
             v[1] = int_pts[2 * i + 1] - center[1]
@@ -70,15 +67,11 @@ def sort_vertex_in_convex_polygon(int_pts, num_of_inter):
                 int_pts[j * 2 + 1] = ty
 
 
-@cuda.jit(
-    '(float32[:], float32[:], int32, int32, float32[:])',
-    device=True,
-    inline=True)
 def line_segment_intersection(pts1, pts2, i, j, temp_pts):
-    A = cuda.local.array((2, ), dtype=numba.float32)
-    B = cuda.local.array((2, ), dtype=numba.float32)
-    C = cuda.local.array((2, ), dtype=numba.float32)
-    D = cuda.local.array((2, ), dtype=numba.float32)
+    A = np.zeros((2, ), dtype=np.float32)
+    B = np.zeros((2, ), dtype=np.float32)
+    C = np.zeros((2, ), dtype=np.float32)
+    D = np.zeros((2, ), dtype=np.float32)
 
     A[0] = pts1[2 * i]
     A[1] = pts1[2 * i + 1]
@@ -116,15 +109,11 @@ def line_segment_intersection(pts1, pts2, i, j, temp_pts):
     return False
 
 
-@cuda.jit(
-    '(float32[:], float32[:], int32, int32, float32[:])',
-    device=True,
-    inline=True)
 def line_segment_intersection_v1(pts1, pts2, i, j, temp_pts):
-    a = cuda.local.array((2, ), dtype=numba.float32)
-    b = cuda.local.array((2, ), dtype=numba.float32)
-    c = cuda.local.array((2, ), dtype=numba.float32)
-    d = cuda.local.array((2, ), dtype=numba.float32)
+    a = np.zeros((2, ), dtype=np.float32)
+    b = np.zeros((2, ), dtype=np.float32)
+    c = np.zeros((2, ), dtype=np.float32)
+    d = np.zeros((2, ), dtype=np.float32)
 
     a[0] = pts1[2 * i]
     a[1] = pts1[2 * i + 1]
@@ -158,7 +147,6 @@ def line_segment_intersection_v1(pts1, pts2, i, j, temp_pts):
     return True
 
 
-@cuda.jit('(float32, float32, float32[:])', device=True, inline=True)
 def point_in_quadrilateral(pt_x, pt_y, corners):
     ab0 = corners[2] - corners[0]
     ab1 = corners[3] - corners[1]
@@ -177,7 +165,6 @@ def point_in_quadrilateral(pt_x, pt_y, corners):
     return abab >= abap and abap >= 0 and adad >= adap and adap >= 0
 
 
-@cuda.jit('(float32[:], float32[:], float32[:])', device=True, inline=True)
 def quadrilateral_intersection(pts1, pts2, int_pts):
     num_of_inter = 0
     for i in range(4):
@@ -189,7 +176,7 @@ def quadrilateral_intersection(pts1, pts2, int_pts):
             int_pts[num_of_inter * 2] = pts2[2 * i]
             int_pts[num_of_inter * 2 + 1] = pts2[2 * i + 1]
             num_of_inter += 1
-    temp_pts = cuda.local.array((2, ), dtype=numba.float32)
+    temp_pts = np.zeros((2, ), dtype=np.float32)
     for i in range(4):
         for j in range(4):
             has_pts = line_segment_intersection(pts1, pts2, i, j, temp_pts)
@@ -201,7 +188,6 @@ def quadrilateral_intersection(pts1, pts2, int_pts):
     return num_of_inter
 
 
-@cuda.jit('(float32[:], float32[:])', device=True, inline=True)
 def rbbox_to_corners(corners, rbbox):
     # generate clockwise corners and rotate it clockwise
     angle = rbbox[4]
@@ -211,8 +197,8 @@ def rbbox_to_corners(corners, rbbox):
     center_y = rbbox[1]
     x_d = rbbox[2]
     y_d = rbbox[3]
-    corners_x = cuda.local.array((4, ), dtype=numba.float32)
-    corners_y = cuda.local.array((4, ), dtype=numba.float32)
+    corners_x = np.zeros((4, ), dtype=np.float32)
+    corners_y = np.zeros((4, ), dtype=np.float32)
     corners_x[0] = -x_d / 2
     corners_x[1] = -x_d / 2
     corners_x[2] = x_d / 2
@@ -228,11 +214,10 @@ def rbbox_to_corners(corners, rbbox):
                 + 1] = -a_sin * corners_x[i] + a_cos * corners_y[i] + center_y
 
 
-@cuda.jit('(float32[:], float32[:])', device=True, inline=True)
 def inter(rbbox1, rbbox2):
-    corners1 = cuda.local.array((8, ), dtype=numba.float32)
-    corners2 = cuda.local.array((8, ), dtype=numba.float32)
-    intersection_corners = cuda.local.array((16, ), dtype=numba.float32)
+    corners1 = np.zeros((8, ), dtype=np.float32)
+    corners2 = np.zeros((8, ), dtype=np.float32)
+    intersection_corners = np.zeros((16, ), dtype=np.float32)
 
     rbbox_to_corners(corners1, rbbox1)
     rbbox_to_corners(corners2, rbbox2)
@@ -245,7 +230,6 @@ def inter(rbbox1, rbbox2):
     return area(intersection_corners, num_intersection)
 
 
-@cuda.jit('(float32[:], float32[:], int32)', device=True, inline=True)
 def devRotateIoUEval(rbox1, rbox2, criterion=-1):
     area1 = rbox1[2] * rbox1[3]
     area2 = rbox2[2] * rbox2[3]
@@ -259,7 +243,7 @@ def devRotateIoUEval(rbox1, rbox2, criterion=-1):
     else:
         return area_inter
 
-@cuda.jit('(int64, int64, float32[:], float32[:], float32[:], int32)', fastmath=False)
+
 def rotate_iou_kernel_eval(N, K, dev_boxes, dev_query_boxes, dev_iou, criterion=-1):
     threadsPerBlock = 8 * 8
     row_start = cuda.blockIdx.x
@@ -328,3 +312,15 @@ def rotate_iou_gpu_eval(boxes, query_boxes, criterion=-1, device_id=0):
             N, K, boxes_dev, query_boxes_dev, iou_dev, criterion)
         iou_dev.copy_to_host(iou.reshape([-1]), stream=stream)
     return iou.astype(boxes.dtype)
+
+
+def rotate_iou_cpu_eval(dev_boxes, dev_query_boxes, criterion=-1):
+    num_boxes = dev_boxes.shape[0]
+    num_qboxes = dev_query_boxes.shape[0]
+    dev_iou = np.zeros((num_boxes, num_qboxes))
+    
+    for box_i in range(num_boxes):
+        for qbox_i in range(num_qboxes):
+            dev_iou[box_i, qbox_i] = devRotateIoUEval(dev_query_boxes[qbox_i], dev_boxes[box_i], criterion)
+    
+    return dev_iou
\ No newline at end of file
diff --git a/pcdet/datasets/lyft/lyft_mAP_eval/lyft_eval.py b/pcdet/datasets/lyft/lyft_mAP_eval/lyft_eval.py
index 872476a..0c547bd 100644
--- a/pcdet/datasets/lyft/lyft_mAP_eval/lyft_eval.py
+++ b/pcdet/datasets/lyft/lyft_mAP_eval/lyft_eval.py
@@ -5,30 +5,6 @@ modified from lyft toolkit https://github.com/lyft/nuscenes-devkit.git
 """
 mAP 3D calculation for the data in nuScenes format.
 
-
-The intput files expected to have the format:
-
-Expected fields:
-
-
-gt = [{
-    'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207fbb039a550991a5149214f98cec136ac',
-    'translation': [974.2811881299899, 1714.6815014457964, -23.689857123368846],
-    'size': [1.796, 4.488, 1.664],
-    'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],
-    'name': 'car'
-}]
-
-prediction_result = {
-    'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207fbb039a550991a5149214f98cec136ac',
-    'translation': [971.8343488872263, 1713.6816097857359, -25.82534357061308],
-    'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],
-    'rotation': [0.10913582721095375, 0.04099572636992043, 0.01927712319721745, 1.029328402625659],
-    'name': 'car',
-    'score': 0.3077029437237213
-}
-
-
 input arguments:
 
 --pred_file:  file with predictions
@@ -354,26 +330,6 @@ def get_average_precisions(gt: list, predictions: list, class_names: list, iou_t
 
     Returns an array with an average precision per class.
 
-
-    Ground truth and predictions should have schema:
-
-    gt = [{
-    'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207fbb039a550991a5149214f98cec136ac',
-    'translation': [974.2811881299899, 1714.6815014457964, -23.689857123368846],
-    'size': [1.796, 4.488, 1.664],
-    'rotation': [0.14882026466054782, 0, 0, 0.9888642620837121],
-    'name': 'car'
-    }]
-
-    predictions = [{
-        'sample_token': '0f0e3ce89d2324d8b45aa55a7b4f8207fbb039a550991a5149214f98cec136ac',
-        'translation': [971.8343488872263, 1713.6816097857359, -25.82534357061308],
-        'size': [2.519726579986132, 7.810161372666739, 3.483438286096803],
-        'rotation': [0.10913582721095375, 0.04099572636992043, 0.01927712319721745, 1.029328402625659],
-        'name': 'car',
-        'score': 0.3077029437237213
-    }]
-
     """
     assert all([0 <= iou_th <= 1 for iou_th in iou_thresholds])
 
diff --git a/pcdet/datasets/nuscenes/nuscenes_dataset.py b/pcdet/datasets/nuscenes/nuscenes_dataset.py
index 0f70005..19ccf95 100644
--- a/pcdet/datasets/nuscenes/nuscenes_dataset.py
+++ b/pcdet/datasets/nuscenes/nuscenes_dataset.py
@@ -1,3 +1,5 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+
 import copy
 import pickle
 from pathlib import Path
@@ -11,6 +13,10 @@ from ..dataset import DatasetTemplate
 from pyquaternion import Quaternion
 from PIL import Image
 
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
 
 class NuScenesDataset(DatasetTemplate):
     def __init__(self, dataset_cfg, class_names, training=True, root_path=None, logger=None):
diff --git a/pcdet/datasets/processor/data_processor.py b/pcdet/datasets/processor/data_processor.py
index 4f72ab5..ed9f904 100644
--- a/pcdet/datasets/processor/data_processor.py
+++ b/pcdet/datasets/processor/data_processor.py
@@ -139,44 +139,6 @@ class DataProcessor(object):
             # to avoid pickling issues in multiprocess spawn
             return partial(self.transform_points_to_voxels, config=config)
 
-        if self.voxel_generator is None:
-            self.voxel_generator = VoxelGeneratorWrapper(
-                vsize_xyz=config.VOXEL_SIZE,
-                coors_range_xyz=self.point_cloud_range,
-                num_point_features=self.num_point_features,
-                max_num_points_per_voxel=config.MAX_POINTS_PER_VOXEL,
-                max_num_voxels=config.MAX_NUMBER_OF_VOXELS[self.mode],
-            )
-
-        points = data_dict['points']
-        voxel_output = self.voxel_generator.generate(points)
-        voxels, coordinates, num_points = voxel_output
-
-        if not data_dict['use_lead_xyz']:
-            voxels = voxels[..., 3:]  # remove xyz in voxels(N, 3)
-
-        if config.get('DOUBLE_FLIP', False):
-            voxels_list, voxel_coords_list, voxel_num_points_list = [voxels], [coordinates], [num_points]
-            points_yflip, points_xflip, points_xyflip = self.double_flip(points)
-            points_list = [points_yflip, points_xflip, points_xyflip]
-            keys = ['yflip', 'xflip', 'xyflip']
-            for i, key in enumerate(keys):
-                voxel_output = self.voxel_generator.generate(points_list[i])
-                voxels, coordinates, num_points = voxel_output
-
-                if not data_dict['use_lead_xyz']:
-                    voxels = voxels[..., 3:]
-                voxels_list.append(voxels)
-                voxel_coords_list.append(coordinates)
-                voxel_num_points_list.append(num_points)
-
-            data_dict['voxels'] = voxels_list
-            data_dict['voxel_coords'] = voxel_coords_list
-            data_dict['voxel_num_points'] = voxel_num_points_list
-        else:
-            data_dict['voxels'] = voxels
-            data_dict['voxel_coords'] = coordinates
-            data_dict['voxel_num_points'] = num_points
         return data_dict
 
     def sample_points(self, data_dict=None, config=None):
diff --git a/pcdet/models/backbones_2d/map_to_bev/pointpillar_scatter.py b/pcdet/models/backbones_2d/map_to_bev/pointpillar_scatter.py
index c57cda8..ceb5779 100644
--- a/pcdet/models/backbones_2d/map_to_bev/pointpillar_scatter.py
+++ b/pcdet/models/backbones_2d/map_to_bev/pointpillar_scatter.py
@@ -9,22 +9,28 @@ class PointPillarScatter(nn.Module):
         self.model_cfg = model_cfg
         self.num_bev_features = self.model_cfg.NUM_BEV_FEATURES
         self.nx, self.ny, self.nz = grid_size
+        self.nums = self.nz * self.nx * self.ny
         assert self.nz == 1
 
     def forward(self, batch_dict, **kwargs):
         pillar_features, coords = batch_dict['pillar_features'], batch_dict['voxel_coords']
+        coords_dim1_length = coords.shape[1]
+        coords_0, coords_1, coords_2, coords_3, _ = torch.split(coords, [1, 1, 1, 1, coords_dim1_length-4], dim=1)
+        coords_0 = coords_0.squeeze(1)
+        coords_1 = coords_1.squeeze(1)
+        coords_2 = coords_2.squeeze(1)
+        coords_3 = coords_3.squeeze(1)
         batch_spatial_features = []
-        batch_size = coords[:, 0].max().int().item() + 1
+        batch_size = coords_0.max().int().item() + 1
         for batch_idx in range(batch_size):
             spatial_feature = torch.zeros(
                 self.num_bev_features,
-                self.nz * self.nx * self.ny,
+                self.nums,
                 dtype=pillar_features.dtype,
                 device=pillar_features.device)
 
-            batch_mask = coords[:, 0] == batch_idx
-            this_coords = coords[batch_mask, :]
-            indices = this_coords[:, 1] + this_coords[:, 2] * self.nx + this_coords[:, 3]
+            batch_mask = coords_0 == batch_idx
+            indices = coords_1[batch_mask] + coords_2[batch_mask] * self.nx + coords_3[batch_mask]
             indices = indices.type(torch.long)
             pillars = pillar_features[batch_mask, :]
             pillars = pillars.t()
diff --git a/pcdet/models/backbones_3d/__init__.py b/pcdet/models/backbones_3d/__init__.py
index 0a25c62..78c4998 100644
--- a/pcdet/models/backbones_3d/__init__.py
+++ b/pcdet/models/backbones_3d/__init__.py
@@ -1,22 +1,22 @@
-from .pointnet2_backbone import PointNet2Backbone, PointNet2MSG
+# from .pointnet2_backbone import PointNet2Backbone, PointNet2MSG
 from .spconv_backbone import VoxelBackBone8x, VoxelResBackBone8x
-from .spconv_backbone_2d import PillarBackBone8x, PillarRes18BackBone8x
-from .spconv_backbone_focal import VoxelBackBone8xFocal
-from .spconv_backbone_voxelnext import VoxelResBackBone8xVoxelNeXt
-from .spconv_backbone_voxelnext2d import VoxelResBackBone8xVoxelNeXt2D
-from .spconv_unet import UNetV2
-from .dsvt import DSVT
+# from .spconv_backbone_2d import PillarBackBone8x, PillarRes18BackBone8x
+# from .spconv_backbone_focal import VoxelBackBone8xFocal
+# from .spconv_backbone_voxelnext import VoxelResBackBone8xVoxelNeXt
+# from .spconv_backbone_voxelnext2d import VoxelResBackBone8xVoxelNeXt2D
+# from .spconv_unet import UNetV2
+# from .dsvt import DSVT
 
 __all__ = {
     'VoxelBackBone8x': VoxelBackBone8x,
-    'UNetV2': UNetV2,
-    'PointNet2Backbone': PointNet2Backbone,
-    'PointNet2MSG': PointNet2MSG,
+#     'UNetV2': UNetV2,
+#     'PointNet2Backbone': PointNet2Backbone,
+#     'PointNet2MSG': PointNet2MSG,
     'VoxelResBackBone8x': VoxelResBackBone8x,
-    'VoxelBackBone8xFocal': VoxelBackBone8xFocal,
-    'VoxelResBackBone8xVoxelNeXt': VoxelResBackBone8xVoxelNeXt,
-    'VoxelResBackBone8xVoxelNeXt2D': VoxelResBackBone8xVoxelNeXt2D,
-    'PillarBackBone8x': PillarBackBone8x,
-    'PillarRes18BackBone8x': PillarRes18BackBone8x,
-    'DSVT': DSVT,
+#     'VoxelBackBone8xFocal': VoxelBackBone8xFocal,
+#     'VoxelResBackBone8xVoxelNeXt': VoxelResBackBone8xVoxelNeXt,
+#     'VoxelResBackBone8xVoxelNeXt2D': VoxelResBackBone8xVoxelNeXt2D,
+#     'PillarBackBone8x': PillarBackBone8x,
+#     'PillarRes18BackBone8x': PillarRes18BackBone8x,
+#     'DSVT': DSVT,
 }
diff --git a/pcdet/models/backbones_3d/spconv_backbone.py b/pcdet/models/backbones_3d/spconv_backbone.py
index f0c231a..ef0cc18 100644
--- a/pcdet/models/backbones_3d/spconv_backbone.py
+++ b/pcdet/models/backbones_3d/spconv_backbone.py
@@ -2,19 +2,20 @@ from functools import partial
 
 import torch.nn as nn
 
-from ...utils.spconv_utils import replace_feature, spconv
+from ...utils.spconv_utils import replace_feature
+from mx_driving import spconv
 
 
 def post_act_block(in_channels, out_channels, kernel_size, indice_key=None, stride=1, padding=0,
                    conv_type='subm', norm_fn=None):
 
     if conv_type == 'subm':
-        conv = spconv.SubMConv3d(in_channels, out_channels, kernel_size, bias=False, indice_key=indice_key)
+        conv = spconv.SubMConv3d(in_channels, out_channels, kernel_size, bias=False, mode='spconv', indice_key=indice_key)
     elif conv_type == 'spconv':
         conv = spconv.SparseConv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding,
-                                   bias=False, indice_key=indice_key)
+                                   bias=False, mode='spconv', indice_key=indice_key)
     elif conv_type == 'inverseconv':
-        conv = spconv.SparseInverseConv3d(in_channels, out_channels, kernel_size, indice_key=indice_key, bias=False)
+        conv = spconv.SparseInverseConv3d(in_channels, out_channels, kernel_size, mode='spconv', indice_key=indice_key, bias=False)
     else:
         raise NotImplementedError
 
@@ -37,12 +38,12 @@ class SparseBasicBlock(spconv.SparseModule):
         if bias is None:
             bias = norm_fn is not None
         self.conv1 = spconv.SubMConv3d(
-            inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=bias, indice_key=indice_key
+            inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=bias, mode='spconv', indice_key=indice_key
         )
         self.bn1 = norm_fn(planes)
         self.relu = nn.ReLU()
         self.conv2 = spconv.SubMConv3d(
-            planes, planes, kernel_size=3, stride=stride, padding=1, bias=bias, indice_key=indice_key
+            planes, planes, kernel_size=3, stride=stride, padding=1, bias=bias, mode='spconv', indice_key=indice_key
         )
         self.bn2 = norm_fn(planes)
         self.downsample = downsample
@@ -76,7 +77,7 @@ class VoxelBackBone8x(nn.Module):
         self.sparse_shape = grid_size[::-1] + [1, 0, 0]
 
         self.conv_input = spconv.SparseSequential(
-            spconv.SubMConv3d(input_channels, 16, 3, padding=1, bias=False, indice_key='subm1'),
+            spconv.SubMConv3d(input_channels, 16, 3, padding=1, bias=False, mode='spconv', indice_key='subm1'),
             norm_fn(16),
             nn.ReLU(),
         )
@@ -112,7 +113,7 @@ class VoxelBackBone8x(nn.Module):
         self.conv_out = spconv.SparseSequential(
             # [200, 150, 5] -> [200, 150, 2]
             spconv.SparseConv3d(64, 128, (3, 1, 1), stride=(2, 1, 1), padding=last_pad,
-                                bias=False, indice_key='spconv_down2'),
+                                bias=False, mode='spconv', indice_key='spconv_down2'),
             norm_fn(128),
             nn.ReLU(),
         )
@@ -191,7 +192,7 @@ class VoxelResBackBone8x(nn.Module):
         self.sparse_shape = grid_size[::-1] + [1, 0, 0]
 
         self.conv_input = spconv.SparseSequential(
-            spconv.SubMConv3d(input_channels, 16, 3, padding=1, bias=False, indice_key='subm1'),
+            spconv.SubMConv3d(input_channels, 16, 3, padding=1, bias=False, mode='spconv', indice_key='subm1'),
             norm_fn(16),
             nn.ReLU(),
         )
@@ -228,7 +229,7 @@ class VoxelResBackBone8x(nn.Module):
         self.conv_out = spconv.SparseSequential(
             # [200, 150, 5] -> [200, 150, 2]
             spconv.SparseConv3d(128, 128, (3, 1, 1), stride=(2, 1, 1), padding=last_pad,
-                                bias=False, indice_key='spconv_down2'),
+                                bias=False, mode='spconv', indice_key='spconv_down2'),
             norm_fn(128),
             nn.ReLU(),
         )
@@ -291,5 +292,5 @@ class VoxelResBackBone8x(nn.Module):
                 'x_conv4': 8,
             }
         })
-        
+
         return batch_dict
diff --git a/pcdet/models/backbones_3d/vfe/dynamic_pillar_vfe.py b/pcdet/models/backbones_3d/vfe/dynamic_pillar_vfe.py
index f5fb6b1..d9876cb 100644
--- a/pcdet/models/backbones_3d/vfe/dynamic_pillar_vfe.py
+++ b/pcdet/models/backbones_3d/vfe/dynamic_pillar_vfe.py
@@ -1,3 +1,5 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
@@ -99,8 +101,11 @@ class DynamicPillarVFE(VFETemplate):
         merge_coords = points[:, 0].int() * self.scale_xy + \
                        points_coords[:, 0] * self.scale_y + \
                        points_coords[:, 1]
-        
-        unq_coords, unq_inv, unq_cnt = torch.unique(merge_coords, return_inverse=True, return_counts=True, dim=0)
+
+        import numpy as np
+        unq_coords, unq_inv = np.unique(merge_coords.cpu().numpy(), return_inverse=True)
+        unq_coords = torch.from_numpy(unq_coords).npu()
+        unq_inv = torch.from_numpy(unq_inv).npu()
 
         points_mean = torch_scatter.scatter_mean(points_xyz, unq_inv, dim=0)
         f_cluster = points_xyz - points_mean[unq_inv, :]
diff --git a/pcdet/models/backbones_3d/vfe/mean_vfe.py b/pcdet/models/backbones_3d/vfe/mean_vfe.py
index 42bd21f..aff499e 100644
--- a/pcdet/models/backbones_3d/vfe/mean_vfe.py
+++ b/pcdet/models/backbones_3d/vfe/mean_vfe.py
@@ -1,6 +1,7 @@
 import torch
 
 from .vfe_template import VFETemplate
+from .pillar_vfe import point_to_voxel
 
 
 class MeanVFE(VFETemplate):
@@ -22,6 +23,10 @@ class MeanVFE(VFETemplate):
         Returns:
             vfe_features: (num_voxels, C)
         """
+        voxel_size = [0.075, 0.075, 0.2]
+        points = batch_dict['points']
+        batch_dict['voxels'], batch_dict['voxel_coords'], batch_dict['voxel_num_points'] = point_to_voxel(points, voxel_size)
+
         voxel_features, voxel_num_points = batch_dict['voxels'], batch_dict['voxel_num_points']
         points_mean = voxel_features[:, :, :].sum(dim=1, keepdim=False)
         normalizer = torch.clamp_min(voxel_num_points.view(-1, 1), min=1.0).type_as(voxel_features)
diff --git a/pcdet/models/backbones_3d/vfe/pillar_vfe.py b/pcdet/models/backbones_3d/vfe/pillar_vfe.py
index a162a83..604341b 100644
--- a/pcdet/models/backbones_3d/vfe/pillar_vfe.py
+++ b/pcdet/models/backbones_3d/vfe/pillar_vfe.py
@@ -3,6 +3,41 @@ import torch.nn as nn
 import torch.nn.functional as F
 
 from .vfe_template import VFETemplate
+from mx_driving import Voxelization
+
+def point_to_voxel(point, voxel_size):
+    if voxel_size == [0.075, 0.075, 0.2]:
+        max_num_points = 10
+        max_voxels = 120000
+        batch_size = 4
+        point_cloud_range = [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]
+    elif voxel_size == [0.2, 0.2, 8.0]:
+        max_num_points = 20
+        max_voxels = 30000
+        batch_size = 12
+        point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
+    voxel_generator = Voxelization(voxel_size, point_cloud_range, max_num_points, max_voxels)
+    all_voxels = []
+    all_coordinates = []
+    all_num_points = []
+    for batch_idx in range(batch_size):
+        mask = (point[:, 0] == batch_idx)
+        selected_point = point[mask,1:]
+        if selected_point.numel() == 0:
+            continue
+        selected_point = selected_point.npu()
+        _, voxels, coordinates, num_points = voxel_generator(selected_point)
+        coordinates_full = torch.full((coordinates.size(0), coordinates.size(1) + 1), fill_value=batch_idx, device=coordinates.device)
+        coordinates_full[:, 1:] = coordinates
+        column_order = [0, 3, 2, 1]
+        coordinates = coordinates_full[:, column_order]
+        all_voxels.append(voxels)
+        all_coordinates.append(coordinates)
+        all_num_points.append(num_points)
+    voxels_list = torch.cat(all_voxels, dim=0)
+    coordinates_list = torch.cat(all_coordinates, dim=0)
+    num_points_list = torch.cat(all_num_points, dim=0)
+    return voxels_list, coordinates_list, num_points_list
 
 
 class PFNLayer(nn.Module):
@@ -39,7 +74,7 @@ class PFNLayer(nn.Module):
         x = self.norm(x.permute(0, 2, 1)).permute(0, 2, 1) if self.use_norm else x
         torch.backends.cudnn.enabled = True
         x = F.relu(x)
-        x_max = torch.max(x, dim=1, keepdim=True)[0]
+        x_max = torch.max(x.transpose(0,1), dim=0)[0].unsqueeze(0).transpose(0,1)
 
         if self.last_vfe:
             return x_max
@@ -79,6 +114,7 @@ class PillarVFE(VFETemplate):
         self.x_offset = self.voxel_x / 2 + point_cloud_range[0]
         self.y_offset = self.voxel_y / 2 + point_cloud_range[1]
         self.z_offset = self.voxel_z / 2 + point_cloud_range[2]
+        self.voxel_size = voxel_size
 
     def get_output_feature_dim(self):
         return self.num_filters[-1]
@@ -92,15 +128,21 @@ class PillarVFE(VFETemplate):
         return paddings_indicator
 
     def forward(self, batch_dict, **kwargs):
-  
+        points = batch_dict['points']
+        batch_dict['voxels'], batch_dict['voxel_coords'], batch_dict['voxel_num_points'] = point_to_voxel(points, self.voxel_size)
         voxel_features, voxel_num_points, coords = batch_dict['voxels'], batch_dict['voxel_num_points'], batch_dict['voxel_coords']
-        points_mean = voxel_features[:, :, :3].sum(dim=1, keepdim=True) / voxel_num_points.type_as(voxel_features).view(-1, 1, 1)
-        f_cluster = voxel_features[:, :, :3] - points_mean
-
-        f_center = torch.zeros_like(voxel_features[:, :, :3])
-        f_center[:, :, 0] = voxel_features[:, :, 0] - (coords[:, 3].to(voxel_features.dtype).unsqueeze(1) * self.voxel_x + self.x_offset)
-        f_center[:, :, 1] = voxel_features[:, :, 1] - (coords[:, 2].to(voxel_features.dtype).unsqueeze(1) * self.voxel_y + self.y_offset)
-        f_center[:, :, 2] = voxel_features[:, :, 2] - (coords[:, 1].to(voxel_features.dtype).unsqueeze(1) * self.voxel_z + self.z_offset)
+        voxel_features_3 = voxel_features[:, :, :3]
+        voxel_features_dim2_length = voxel_features.shape[2]
+        voxel_features_0, voxel_features_1, voxel_features_2, _ = torch.split(voxel_features, [1, 1, 1, voxel_features_dim2_length-3], dim=2)
+        coords_dim1_length = coords.shape[1]
+        coords_0, coords_1, coords_2, coords_3, _ = torch.split(coords, [1, 1, 1, 1, coords_dim1_length-4], dim=1)
+        points_mean = voxel_features_3.sum(dim=1, keepdim=True) / voxel_num_points.type_as(voxel_features).view(-1, 1, 1)
+        f_cluster = voxel_features_3 - points_mean
+
+        f_center = torch.zeros_like(voxel_features_3)
+        f_center[:, :, 0] = voxel_features_0.squeeze(2) - (coords_3.to(voxel_features.dtype) * self.voxel_x + self.x_offset)
+        f_center[:, :, 1] = voxel_features_1.squeeze(2) - (coords_2.to(voxel_features.dtype) * self.voxel_y + self.y_offset)
+        f_center[:, :, 2] = voxel_features_2.squeeze(2) - (coords_1.to(voxel_features.dtype) * self.voxel_z + self.z_offset)
 
         if self.use_absolute_xyz:
             features = [voxel_features, f_cluster, f_center]
@@ -108,7 +150,7 @@ class PillarVFE(VFETemplate):
             features = [voxel_features[..., 3:], f_cluster, f_center]
 
         if self.with_distance:
-            points_dist = torch.norm(voxel_features[:, :, :3], 2, 2, keepdim=True)
+            points_dist = torch.norm(voxel_features_3, 2, 2, keepdim=True)
             features.append(points_dist)
         features = torch.cat(features, dim=-1)
 
diff --git a/pcdet/models/dense_heads/center_head.py b/pcdet/models/dense_heads/center_head.py
index 38a6e35..d772869 100644
--- a/pcdet/models/dense_heads/center_head.py
+++ b/pcdet/models/dense_heads/center_head.py
@@ -7,6 +7,7 @@ from ..model_utils import model_nms_utils
 from ..model_utils import centernet_utils
 from ...utils import loss_utils
 from functools import partial
+from mx_driving import npu_assign_target_of_single_head
 
 
 class SeparateHead(nn.Module):
@@ -65,7 +66,7 @@ class CenterHead(nn.Module):
             self.class_names_each_head.append([x for x in cur_class_names if x in class_names])
             cur_class_id_mapping = torch.from_numpy(np.array(
                 [self.class_names.index(x) for x in cur_class_names if x in class_names]
-            )).cuda()
+            )).npu()
             self.class_id_mapping_each_head.append(cur_class_id_mapping)
 
         total_classes = sum([len(x) for x in self.class_names_each_head])
@@ -83,8 +84,8 @@ class CenterHead(nn.Module):
 
         self.heads_list = nn.ModuleList()
         self.separate_head_cfg = self.model_cfg.SEPARATE_HEAD_CFG
+        cur_head_dict = copy.deepcopy(self.separate_head_cfg.HEAD_DICT)
         for idx, cur_class_names in enumerate(self.class_names_each_head):
-            cur_head_dict = copy.deepcopy(self.separate_head_cfg.HEAD_DICT)
             cur_head_dict['hm'] = dict(out_channels=len(cur_class_names), num_conv=self.model_cfg.NUM_HM_CONV)
             self.heads_list.append(
                 SeparateHead(
@@ -115,51 +116,29 @@ class CenterHead(nn.Module):
         Returns:
 
         """
-        heatmap = gt_boxes.new_zeros(num_classes, feature_map_size[1], feature_map_size[0])
-        ret_boxes = gt_boxes.new_zeros((num_max_objs, gt_boxes.shape[-1] - 1 + 1))
-        inds = gt_boxes.new_zeros(num_max_objs).long()
-        mask = gt_boxes.new_zeros(num_max_objs).long()
         ret_boxes_src = gt_boxes.new_zeros(num_max_objs, gt_boxes.shape[-1])
         ret_boxes_src[:gt_boxes.shape[0]] = gt_boxes
-
-        x, y, z = gt_boxes[:, 0], gt_boxes[:, 1], gt_boxes[:, 2]
-        coord_x = (x - self.point_cloud_range[0]) / self.voxel_size[0] / feature_map_stride
-        coord_y = (y - self.point_cloud_range[1]) / self.voxel_size[1] / feature_map_stride
-        coord_x = torch.clamp(coord_x, min=0, max=feature_map_size[0] - 0.5)  # bugfixed: 1e-6 does not work for center.int()
-        coord_y = torch.clamp(coord_y, min=0, max=feature_map_size[1] - 0.5)  #
-        center = torch.cat((coord_x[:, None], coord_y[:, None]), dim=-1)
-        center_int = center.int()
-        center_int_float = center_int.float()
-
-        dx, dy, dz = gt_boxes[:, 3], gt_boxes[:, 4], gt_boxes[:, 5]
-        dx = dx / self.voxel_size[0] / feature_map_stride
-        dy = dy / self.voxel_size[1] / feature_map_stride
-
-        radius = centernet_utils.gaussian_radius(dx, dy, min_overlap=gaussian_overlap)
-        radius = torch.clamp_min(radius.int(), min=min_radius)
-
-        for k in range(min(num_max_objs, gt_boxes.shape[0])):
-            if dx[k] <= 0 or dy[k] <= 0:
-                continue
-
-            if not (0 <= center_int[k][0] <= feature_map_size[0] and 0 <= center_int[k][1] <= feature_map_size[1]):
-                continue
-
-            cur_class_id = (gt_boxes[k, -1] - 1).long()
-            centernet_utils.draw_gaussian_to_heatmap(heatmap[cur_class_id], center[k], radius[k].item())
-
-            inds[k] = center_int[k, 1] * feature_map_size[0] + center_int[k, 0]
-            mask[k] = 1
-
-            ret_boxes[k, 0:2] = center[k] - center_int_float[k].float()
-            ret_boxes[k, 2] = z[k]
-            ret_boxes[k, 3:6] = gt_boxes[k, 3:6].log()
-            ret_boxes[k, 6] = torch.cos(gt_boxes[k, 6])
-            ret_boxes[k, 7] = torch.sin(gt_boxes[k, 6])
-            if gt_boxes.shape[1] > 8:
-                ret_boxes[k, 8:] = gt_boxes[k, 7:-1]
-
-        return heatmap, ret_boxes, inds, mask, ret_boxes_src
+        if (torch.numel(gt_boxes[:,:-1]) == 0):
+            ret_boxes = gt_boxes.new_zeros((num_max_objs, gt_boxes.shape[-1] - 1 + 1))
+            ind = gt_boxes.new_zeros(num_max_objs).long()
+            mask = gt_boxes.new_zeros(num_max_objs).long()
+            heatmap = gt_boxes.new_zeros(num_classes, feature_map_size[1], feature_map_size[0])
+            return heatmap, ret_boxes, ind, mask, ret_boxes_src
+        cur_class_id = gt_boxes[:,-1].int()
+        heatmap, ret_boxes, ind, mask = npu_assign_target_of_single_head(gt_boxes[:,:-1],
+                                                                         cur_class_id,
+                                                                         num_classes,
+                                                                         feature_map_stride,
+                                                                         gaussian_overlap,
+                                                                         min_radius,
+                                                                         self.voxel_size,
+                                                                         self.point_cloud_range,
+                                                                         feature_map_size,
+                                                                         True,
+                                                                         True,
+                                                                         True,
+                                                                         num_max_objs)
+        return heatmap, ret_boxes, ind, mask, ret_boxes_src
 
     def assign_targets(self, gt_boxes, feature_map_size=None, **kwargs):
         """
@@ -184,7 +163,6 @@ class CenterHead(nn.Module):
             'heatmap_masks': [],
             'target_boxes_src': [],
         }
-
         all_names = np.array(['bg', *self.class_names])
         for idx, cur_class_names in enumerate(self.class_names_each_head):
             heatmap_list, target_boxes_list, inds_list, masks_list, target_boxes_src_list = [], [], [], [], []
@@ -207,17 +185,17 @@ class CenterHead(nn.Module):
                     gt_boxes_single_head = torch.cat(gt_boxes_single_head, dim=0)
 
                 heatmap, ret_boxes, inds, mask, ret_boxes_src = self.assign_target_of_single_head(
-                    num_classes=len(cur_class_names), gt_boxes=gt_boxes_single_head.cpu(),
+                    num_classes=len(cur_class_names), gt_boxes=gt_boxes_single_head,
                     feature_map_size=feature_map_size, feature_map_stride=target_assigner_cfg.FEATURE_MAP_STRIDE,
                     num_max_objs=target_assigner_cfg.NUM_MAX_OBJS,
                     gaussian_overlap=target_assigner_cfg.GAUSSIAN_OVERLAP,
                     min_radius=target_assigner_cfg.MIN_RADIUS,
                 )
-                heatmap_list.append(heatmap.to(gt_boxes_single_head.device))
-                target_boxes_list.append(ret_boxes.to(gt_boxes_single_head.device))
-                inds_list.append(inds.to(gt_boxes_single_head.device))
-                masks_list.append(mask.to(gt_boxes_single_head.device))
-                target_boxes_src_list.append(ret_boxes_src.to(gt_boxes_single_head.device))
+                heatmap_list.append(heatmap)
+                target_boxes_list.append(ret_boxes)
+                inds_list.append(inds)
+                masks_list.append(mask)
+                target_boxes_src_list.append(ret_boxes_src)
 
             ret_dict['heatmaps'].append(torch.stack(heatmap_list, dim=0))
             ret_dict['target_boxes'].append(torch.stack(target_boxes_list, dim=0))
@@ -252,8 +230,8 @@ class CenterHead(nn.Module):
             loc_loss = loc_loss * self.model_cfg.LOSS_CONFIG.LOSS_WEIGHTS['loc_weight']
 
             loss += hm_loss + loc_loss
-            tb_dict['hm_loss_head_%d' % idx] = hm_loss.item()
-            tb_dict['loc_loss_head_%d' % idx] = loc_loss.item()
+            tb_dict['hm_loss_head_%d' % idx] = hm_loss
+            tb_dict['loc_loss_head_%d' % idx] = loc_loss
 
             if 'iou' in pred_dict or self.model_cfg.get('IOU_REG_LOSS', False):
 
@@ -273,7 +251,7 @@ class CenterHead(nn.Module):
                         ind=target_dicts['inds'][idx], gt_boxes=target_dicts['target_boxes_src'][idx]
                     )
                     loss += iou_loss
-                    tb_dict['iou_loss_head_%d' % idx] = iou_loss.item()
+                    tb_dict['iou_loss_head_%d' % idx] = iou_loss
 
                 if self.model_cfg.get('IOU_REG_LOSS', False):
                     iou_reg_loss = loss_utils.calculate_iou_reg_loss_centerhead(
@@ -281,22 +259,22 @@ class CenterHead(nn.Module):
                         mask=target_dicts['masks'][idx],
                         ind=target_dicts['inds'][idx], gt_boxes=target_dicts['target_boxes_src'][idx]
                     )
-                    if target_dicts['masks'][idx].sum().item() != 0:
+                    if target_dicts['masks'][idx].sum() != 0:
                         iou_reg_loss = iou_reg_loss * self.model_cfg.LOSS_CONFIG.LOSS_WEIGHTS['loc_weight']
                         loss += iou_reg_loss
-                        tb_dict['iou_reg_loss_head_%d' % idx] = iou_reg_loss.item()
+                        tb_dict['iou_reg_loss_head_%d' % idx] = iou_reg_loss
                     else:
                         loss += (batch_box_preds_for_iou * 0.).sum()
                         tb_dict['iou_reg_loss_head_%d' % idx] = (batch_box_preds_for_iou * 0.).sum()
 
 
 
-        tb_dict['rpn_loss'] = loss.item()
+        tb_dict['rpn_loss'] = loss
         return loss, tb_dict
 
     def generate_predicted_boxes(self, batch_size, pred_dicts):
         post_process_cfg = self.model_cfg.POST_PROCESSING
-        post_center_limit_range = torch.tensor(post_process_cfg.POST_CENTER_LIMIT_RANGE).cuda().float()
+        post_center_limit_range = torch.tensor(post_process_cfg.POST_CENTER_LIMIT_RANGE).npu().float()
 
         ret_dict = [{
             'pred_boxes': [],
@@ -391,11 +369,7 @@ class CenterHead(nn.Module):
             pred_dicts.append(head(x))
 
         if self.training:
-            target_dict = self.assign_targets(
-                data_dict['gt_boxes'], feature_map_size=spatial_features_2d.size()[2:],
-                feature_map_stride=data_dict.get('spatial_features_2d_strides', None)
-            )
-            self.forward_ret_dict['target_dicts'] = target_dict
+            self.forward_ret_dict['target_dicts'] = data_dict['target_dicts']
 
         self.forward_ret_dict['pred_dicts'] = pred_dicts
 
diff --git a/pcdet/models/dense_heads/target_assigner/anchor_generator.py b/pcdet/models/dense_heads/target_assigner/anchor_generator.py
index 0aa6861..d980478 100644
--- a/pcdet/models/dense_heads/target_assigner/anchor_generator.py
+++ b/pcdet/models/dense_heads/target_assigner/anchor_generator.py
@@ -74,6 +74,4 @@ if __name__ == '__main__':
         anchor_range=[-75.2, -75.2, -2, 75.2, 75.2, 4],
         anchor_generator_config=config
     )
-    import pdb
-    pdb.set_trace()
     A.generate_anchors([[188, 188]])
diff --git a/pcdet/models/detectors/detector3d_template.py b/pcdet/models/detectors/detector3d_template.py
index 91e44bd..f403a6d 100644
--- a/pcdet/models/detectors/detector3d_template.py
+++ b/pcdet/models/detectors/detector3d_template.py
@@ -18,7 +18,7 @@ class Detector3DTemplate(nn.Module):
         self.num_class = num_class
         self.dataset = dataset
         self.class_names = dataset.class_names
-        self.register_buffer('global_step', torch.LongTensor(1).zero_())
+        self.register_buffer('global_step', torch.FloatTensor(1).zero_())
 
         self.module_topology = [
             'vfe', 'backbone_3d', 'map_to_bev_module', 'pfe',
@@ -305,7 +305,7 @@ class Detector3DTemplate(nn.Module):
 
         if cur_gt.shape[0] > 0:
             if box_preds.shape[0] > 0:
-                iou3d_rcnn = iou3d_nms_utils.boxes_iou3d_gpu(box_preds[:, 0:7], cur_gt[:, 0:7])
+                iou3d_rcnn = iou3d_nms_utils.boxes_bev_iou_cpu(box_preds[:, 0:7].cpu(), cur_gt[:, 0:7].cpu()).npu()
             else:
                 iou3d_rcnn = torch.zeros((0, cur_gt.shape[0]))
 
diff --git a/pcdet/ops/bev_pool/bev_pool.py b/pcdet/ops/bev_pool/bev_pool.py
index 5769a40..9eb7a71 100644
--- a/pcdet/ops/bev_pool/bev_pool.py
+++ b/pcdet/ops/bev_pool/bev_pool.py
@@ -1,6 +1,5 @@
 import torch
 
-from . import bev_pool_ext
 
 __all__ = ["bev_pool"]
 
diff --git a/pcdet/ops/iou3d_nms/iou3d_nms_utils.py b/pcdet/ops/iou3d_nms/iou3d_nms_utils.py
index b63ca0d..641e329 100644
--- a/pcdet/ops/iou3d_nms/iou3d_nms_utils.py
+++ b/pcdet/ops/iou3d_nms/iou3d_nms_utils.py
@@ -131,7 +131,7 @@ def nms_gpu(boxes, scores, thresh, pre_maxsize=None, **kwargs):
 
     boxes = boxes[order].contiguous()
     keep = torch.LongTensor(boxes.size(0))
-    num_out = iou3d_nms_cuda.nms_gpu(boxes, keep, thresh)
+    num_out = iou3d_nms_cuda.nms_cpu(boxes.cpu(), keep.cpu(), thresh)
     return order[keep[:num_out].cuda()].contiguous(), None
 
 
@@ -148,7 +148,7 @@ def nms_normal_gpu(boxes, scores, thresh, **kwargs):
     boxes = boxes[order].contiguous()
 
     keep = torch.LongTensor(boxes.size(0))
-    num_out = iou3d_nms_cuda.nms_normal_gpu(boxes, keep, thresh)
+    num_out = iou3d_nms_cuda.nms_cpu(boxes, keep, thresh)
     return order[keep[:num_out].cuda()].contiguous(), None
 
 
diff --git a/pcdet/ops/iou3d_nms/src/iou3d_cpu.cpp b/pcdet/ops/iou3d_nms/src/iou3d_cpu.cpp
index c0311b3..a3e13dd 100644
--- a/pcdet/ops/iou3d_nms/src/iou3d_cpu.cpp
+++ b/pcdet/ops/iou3d_nms/src/iou3d_cpu.cpp
@@ -9,23 +9,8 @@ All Rights Reserved 2020.
 #include <torch/serialize/tensor.h>
 #include <torch/extension.h>
 #include <vector>
-#include <cuda.h>
-#include <cuda_runtime_api.h>
 #include "iou3d_cpu.h"
 
-#define CHECK_CUDA(x) do { \
-  if (!x.type().is_cuda()) { \
-    fprintf(stderr, "%s must be CUDA tensor at %s:%d\n", #x, __FILE__, __LINE__); \
-    exit(-1); \
-  } \
-} while (0)
-#define CHECK_CONTIGUOUS(x) do { \
-  if (!x.is_contiguous()) { \
-    fprintf(stderr, "%s must be contiguous tensor at %s:%d\n", #x, __FILE__, __LINE__); \
-    exit(-1); \
-  } \
-} while (0)
-#define CHECK_INPUT(x) CHECK_CUDA(x);CHECK_CONTIGUOUS(x)
 
 inline float min(float a, float b){
     return a > b ? b : a;
@@ -38,20 +23,20 @@ inline float max(float a, float b){
 const float EPS = 1e-8;
 struct Point {
     float x, y;
-    __device__ Point() {}
-    __device__ Point(double _x, double _y){
+    Point() {}
+    Point(double _x, double _y){
         x = _x, y = _y;
     }
 
-    __device__ void set(float _x, float _y){
+    void set(float _x, float _y){
         x = _x; y = _y;
     }
 
-    __device__ Point operator +(const Point &b)const{
+    Point operator +(const Point &b)const{
         return Point(x + b.x, y + b.y);
     }
 
-    __device__ Point operator -(const Point &b)const{
+    Point operator -(const Point &b)const{
         return Point(x - b.x, y - b.y);
     }
 };
@@ -234,9 +219,6 @@ int boxes_iou_bev_cpu(at::Tensor boxes_a_tensor, at::Tensor boxes_b_tensor, at::
     // params boxes_b_tensor: (M, 7) [x, y, z, dx, dy, dz, heading]
     // params ans_iou_tensor: (N, M)
 
-    CHECK_CONTIGUOUS(boxes_a_tensor);
-    CHECK_CONTIGUOUS(boxes_b_tensor);
-
     int num_boxes_a = boxes_a_tensor.size(0);
     int num_boxes_b = boxes_b_tensor.size(0);
     const float *boxes_a = boxes_a_tensor.data<float>();
@@ -256,9 +238,6 @@ int boxes_aligned_iou_bev_cpu(at::Tensor boxes_a_tensor, at::Tensor boxes_b_tens
     // params boxes_b_tensor: (N, 7) [x, y, z, dx, dy, dz, heading]
     // params ans_iou_tensor: (N, 1)
 
-    CHECK_CONTIGUOUS(boxes_a_tensor);
-    CHECK_CONTIGUOUS(boxes_b_tensor);
-
     int num_boxes = boxes_a_tensor.size(0);
     int num_boxes_b = boxes_b_tensor.size(0);
     assert(num_boxes == num_boxes_b);
@@ -271,3 +250,33 @@ int boxes_aligned_iou_bev_cpu(at::Tensor boxes_a_tensor, at::Tensor boxes_b_tens
     }
     return 1;
 }
+
+int nms_cpu(at::Tensor boxes, at::Tensor keep, float nms_overlap_thresh) {
+    int boxes_num = boxes.size(0);
+    const float * boxes_data = boxes.data<float>();
+    long * keep_data = keep.data<long>();
+
+    int num_to_keep = 0;
+
+    std::vector<int> boxIndex;
+    for (int i = 0; i < boxes_num; i++) {
+        boxIndex.push_back(i);
+    }
+
+    while (boxIndex.size() > 0) {
+        int keep_box_index = boxIndex[0];
+        keep[num_to_keep++] = keep_box_index;
+
+        boxIndex.erase(boxIndex.begin());
+
+        for (auto it = boxIndex.begin(); it != boxIndex.end(); ) {
+            float iou_bev_v = iou_bev(boxes_data + keep_box_index * 7, boxes_data + (*it) * 7);
+            if (iou_bev_v > nms_overlap_thresh) {
+                it = boxIndex.erase(it);
+            } else {
+                it++;
+            }
+        }
+    }
+    return num_to_keep;
+}
diff --git a/pcdet/ops/iou3d_nms/src/iou3d_cpu.h b/pcdet/ops/iou3d_nms/src/iou3d_cpu.h
index 4d93bb6..1dbdb36 100644
--- a/pcdet/ops/iou3d_nms/src/iou3d_cpu.h
+++ b/pcdet/ops/iou3d_nms/src/iou3d_cpu.h
@@ -3,9 +3,8 @@
 
 #include <torch/serialize/tensor.h>
 #include <vector>
-#include <cuda.h>
-#include <cuda_runtime_api.h>
 
 int boxes_iou_bev_cpu(at::Tensor boxes_a_tensor, at::Tensor boxes_b_tensor, at::Tensor ans_iou_tensor);
 int boxes_aligned_iou_bev_cpu(at::Tensor boxes_a_tensor, at::Tensor boxes_b_tensor, at::Tensor ans_iou_tensor);
+int nms_cpu(at::Tensor boxes, at::Tensor keep, float nms_overlap_thresh);
 #endif
diff --git a/pcdet/ops/iou3d_nms/src/iou3d_nms_api.cpp b/pcdet/ops/iou3d_nms/src/iou3d_nms_api.cpp
index 972b55b..a0c416f 100644
--- a/pcdet/ops/iou3d_nms/src/iou3d_nms_api.cpp
+++ b/pcdet/ops/iou3d_nms/src/iou3d_nms_api.cpp
@@ -1,20 +1,12 @@
 #include <torch/serialize/tensor.h>
 #include <torch/extension.h>
 #include <vector>
-#include <cuda.h>
-#include <cuda_runtime_api.h>
 
 #include "iou3d_cpu.h"
-#include "iou3d_nms.h"
 
 
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.def("boxes_aligned_overlap_bev_gpu", &boxes_aligned_overlap_bev_gpu, "aligned oriented boxes overlap");
-	m.def("boxes_overlap_bev_gpu", &boxes_overlap_bev_gpu, "oriented boxes overlap");
-	m.def("paired_boxes_overlap_bev_gpu", &paired_boxes_overlap_bev_gpu, "oriented boxes overlap");
-	m.def("boxes_iou_bev_gpu", &boxes_iou_bev_gpu, "oriented boxes iou");
-	m.def("nms_gpu", &nms_gpu, "oriented nms gpu");
-	m.def("nms_normal_gpu", &nms_normal_gpu, "nms gpu");
 	m.def("boxes_aligned_iou_bev_cpu", &boxes_aligned_iou_bev_cpu, "aligned oriented boxes iou");
 	m.def("boxes_iou_bev_cpu", &boxes_iou_bev_cpu, "oriented boxes iou");
+	m.def("nms_cpu", &nms_cpu, "nms cpu");
 }
diff --git a/pcdet/ops/pointnet2/pointnet2_batch/pointnet2_utils.py b/pcdet/ops/pointnet2/pointnet2_batch/pointnet2_utils.py
index c57afe1..5cc1c33 100644
--- a/pcdet/ops/pointnet2/pointnet2_batch/pointnet2_utils.py
+++ b/pcdet/ops/pointnet2/pointnet2_batch/pointnet2_utils.py
@@ -4,8 +4,6 @@ import torch
 import torch.nn as nn
 from torch.autograd import Function, Variable
 
-from . import pointnet2_batch_cuda as pointnet2
-
 
 class FarthestPointSampling(Function):
     @staticmethod
diff --git a/pcdet/ops/pointnet2/pointnet2_stack/pointnet2_utils.py b/pcdet/ops/pointnet2/pointnet2_stack/pointnet2_utils.py
index cd2c1f3..ae207b9 100644
--- a/pcdet/ops/pointnet2/pointnet2_stack/pointnet2_utils.py
+++ b/pcdet/ops/pointnet2/pointnet2_stack/pointnet2_utils.py
@@ -2,8 +2,6 @@ import torch
 import torch.nn as nn
 from torch.autograd import Function, Variable
 
-from . import pointnet2_stack_cuda as pointnet2
-
 
 class BallQuery(Function):
 
diff --git a/pcdet/ops/pointnet2/pointnet2_stack/voxel_query_utils.py b/pcdet/ops/pointnet2/pointnet2_stack/voxel_query_utils.py
index b22da2d..ae1a52d 100644
--- a/pcdet/ops/pointnet2/pointnet2_stack/voxel_query_utils.py
+++ b/pcdet/ops/pointnet2/pointnet2_stack/voxel_query_utils.py
@@ -4,7 +4,6 @@ from torch.autograd import Function
 import torch.nn as nn
 from typing import List
 
-from . import pointnet2_stack_cuda as pointnet2
 from . import pointnet2_utils
 
 class VoxelQuery(Function):
diff --git a/pcdet/ops/roiaware_pool3d/roiaware_pool3d_utils.py b/pcdet/ops/roiaware_pool3d/roiaware_pool3d_utils.py
index d8ca924..304cbb8 100644
--- a/pcdet/ops/roiaware_pool3d/roiaware_pool3d_utils.py
+++ b/pcdet/ops/roiaware_pool3d/roiaware_pool3d_utils.py
@@ -1,3 +1,5 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+
 import torch
 import torch.nn as nn
 from torch.autograd import Function
@@ -35,8 +37,8 @@ def points_in_boxes_gpu(points, boxes):
     assert boxes.shape[2] == 7 and points.shape[2] == 3
     batch_size, num_points, _ = points.shape
 
-    box_idxs_of_pts = points.new_zeros((batch_size, num_points), dtype=torch.int).fill_(-1)
-    roiaware_pool3d_cuda.points_in_boxes_gpu(boxes.contiguous(), points.contiguous(), box_idxs_of_pts)
+    import mx_driving.preprocess
+    box_idxs_of_pts = mx_driving.preprocess.npu_points_in_box(boxes.contiguous().npu(), points.contiguous().npu())
 
     return box_idxs_of_pts
 
@@ -87,7 +89,6 @@ class RoIAwarePool3dFunction(Function):
 
         pool_method_map = {'max': 0, 'avg': 1}
         pool_method = pool_method_map[pool_method]
-        roiaware_pool3d_cuda.forward(rois, pts, pts_feature, argmax, pts_idx_of_voxels, pooled_features, pool_method)
 
         ctx.roiaware_pool3d_for_backward = (pts_idx_of_voxels, argmax, pool_method, num_pts, num_channels)
         return pooled_features
@@ -102,7 +103,6 @@ class RoIAwarePool3dFunction(Function):
         pts_idx_of_voxels, argmax, pool_method, num_pts, num_channels = ctx.roiaware_pool3d_for_backward
 
         grad_in = grad_out.new_zeros((num_pts, num_channels))
-        roiaware_pool3d_cuda.backward(pts_idx_of_voxels, argmax, grad_out.contiguous(), grad_in, pool_method)
 
         return None, None, grad_in, None, None, None
 
diff --git a/pcdet/ops/roiaware_pool3d/src/roiaware_pool3d.cpp b/pcdet/ops/roiaware_pool3d/src/roiaware_pool3d.cpp
index 00edfef..2bd89df 100644
--- a/pcdet/ops/roiaware_pool3d/src/roiaware_pool3d.cpp
+++ b/pcdet/ops/roiaware_pool3d/src/roiaware_pool3d.cpp
@@ -15,108 +15,9 @@ All Rights Reserved 2019-2020.
 //#define CHECK_CONTIGUOUS(x) AT_CHECK(x.is_contiguous(), #x, " must be contiguous ")
 //#define CHECK_INPUT(x) CHECK_CUDA(x);CHECK_CONTIGUOUS(x)
 
-
-void roiaware_pool3d_launcher(int boxes_num, int pts_num, int channels, int max_pts_each_voxel,
-    int out_x, int out_y, int out_z, const float *rois, const float *pts, const float *pts_feature,
-    int *argmax, int *pts_idx_of_voxels, float *pooled_features, int pool_method);
-
-void roiaware_pool3d_backward_launcher(int boxes_num, int out_x, int out_y, int out_z, int channels, int max_pts_each_voxel,
-    const int *pts_idx_of_voxels, const int *argmax, const float *grad_out, float *grad_in, int pool_method);
-
 void points_in_boxes_launcher(int batch_size, int boxes_num, int pts_num, const float *boxes,
     const float *pts, int *box_idx_of_points);
 
-int roiaware_pool3d_gpu(at::Tensor rois, at::Tensor pts, at::Tensor pts_feature, at::Tensor argmax,
-    at::Tensor pts_idx_of_voxels, at::Tensor pooled_features, int pool_method){
-    // params rois: (N, 7) [x, y, z, dx, dy, dz, heading] (x, y, z) is the box center
-    // params pts: (npoints, 3) [x, y, z]
-    // params pts_feature: (npoints, C)
-    // params argmax: (N, out_x, out_y, out_z, C)
-    // params pts_idx_of_voxels: (N, out_x, out_y, out_z, max_pts_each_voxel)
-    // params pooled_features: (N, out_x, out_y, out_z, C)
-    // params pool_method: 0: max_pool 1: avg_pool
-
-//    CHECK_INPUT(rois);
-//    CHECK_INPUT(pts);
-//    CHECK_INPUT(pts_feature);
-//    CHECK_INPUT(argmax);
-//    CHECK_INPUT(pts_idx_of_voxels);
-//    CHECK_INPUT(pooled_features);
-
-    int boxes_num = rois.size(0);
-    int pts_num = pts.size(0);
-    int channels = pts_feature.size(1);
-    int max_pts_each_voxel = pts_idx_of_voxels.size(4);  // index 0 is the counter
-    int out_x = pts_idx_of_voxels.size(1);
-    int out_y = pts_idx_of_voxels.size(2);
-    int out_z = pts_idx_of_voxels.size(3);
-    assert ((out_x < 256) && (out_y < 256) && (out_z < 256));  // we encode index with 8bit
-
-    const float *rois_data = rois.data<float>();
-    const float *pts_data = pts.data<float>();
-    const float *pts_feature_data = pts_feature.data<float>();
-    int *argmax_data = argmax.data<int>();
-    int *pts_idx_of_voxels_data = pts_idx_of_voxels.data<int>();
-    float *pooled_features_data = pooled_features.data<float>();
-
-    roiaware_pool3d_launcher(boxes_num, pts_num, channels, max_pts_each_voxel, out_x, out_y, out_z,
-        rois_data, pts_data, pts_feature_data, argmax_data, pts_idx_of_voxels_data, pooled_features_data, pool_method);
-
-    return 1;
-}
-
-int roiaware_pool3d_gpu_backward(at::Tensor pts_idx_of_voxels, at::Tensor argmax, at::Tensor grad_out, at::Tensor grad_in, int pool_method){
-    // params pts_idx_of_voxels: (N, out_x, out_y, out_z, max_pts_each_voxel)
-    // params argmax: (N, out_x, out_y, out_z, C)
-    // params grad_out: (N, out_x, out_y, out_z, C)
-    // params grad_in: (npoints, C), return value
-    // params pool_method: 0: max_pool 1: avg_pool
-
-//    CHECK_INPUT(pts_idx_of_voxels);
-//    CHECK_INPUT(argmax);
-//    CHECK_INPUT(grad_out);
-//    CHECK_INPUT(grad_in);
-
-    int boxes_num = pts_idx_of_voxels.size(0);
-    int out_x = pts_idx_of_voxels.size(1);
-    int out_y = pts_idx_of_voxels.size(2);
-    int out_z = pts_idx_of_voxels.size(3);
-    int max_pts_each_voxel = pts_idx_of_voxels.size(4);  // index 0 is the counter
-    int channels = grad_out.size(4);
-
-    const int *pts_idx_of_voxels_data = pts_idx_of_voxels.data<int>();
-    const int *argmax_data = argmax.data<int>();
-    const float *grad_out_data = grad_out.data<float>();
-    float *grad_in_data = grad_in.data<float>();
-
-    roiaware_pool3d_backward_launcher(boxes_num, out_x, out_y, out_z, channels, max_pts_each_voxel,
-        pts_idx_of_voxels_data, argmax_data, grad_out_data, grad_in_data, pool_method);
-
-    return 1;
-}
-
-int points_in_boxes_gpu(at::Tensor boxes_tensor, at::Tensor pts_tensor, at::Tensor box_idx_of_points_tensor){
-    // params boxes: (B, N, 7) [x, y, z, dx, dy, dz, heading] (x, y, z) is the box center
-    // params pts: (B, npoints, 3) [x, y, z]
-    // params boxes_idx_of_points: (B, npoints), default -1
-
-//    CHECK_INPUT(boxes_tensor);
-//    CHECK_INPUT(pts_tensor);
-//    CHECK_INPUT(box_idx_of_points_tensor);
-
-    int batch_size = boxes_tensor.size(0);
-    int boxes_num = boxes_tensor.size(1);
-    int pts_num = pts_tensor.size(1);
-
-    const float *boxes = boxes_tensor.data<float>();
-    const float *pts = pts_tensor.data<float>();
-    int *box_idx_of_points = box_idx_of_points_tensor.data<int>();
-
-    points_in_boxes_launcher(batch_size, boxes_num, pts_num, boxes, pts, box_idx_of_points);
-
-    return 1;
-}
-
 
 inline void lidar_to_local_coords_cpu(float shift_x, float shift_y, float rot_angle, float &local_x, float &local_y){
     float cosa = cos(-rot_angle), sina = sin(-rot_angle);
@@ -170,8 +71,5 @@ int points_in_boxes_cpu(at::Tensor boxes_tensor, at::Tensor pts_tensor, at::Tens
 
 
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.def("forward", &roiaware_pool3d_gpu, "roiaware pool3d forward (CUDA)");
-    m.def("backward", &roiaware_pool3d_gpu_backward, "roiaware pool3d backward (CUDA)");
-    m.def("points_in_boxes_gpu", &points_in_boxes_gpu, "points_in_boxes_gpu forward (CUDA)");
     m.def("points_in_boxes_cpu", &points_in_boxes_cpu, "points_in_boxes_cpu forward (CUDA)");
 }
diff --git a/pcdet/ops/roipoint_pool3d/roipoint_pool3d_utils.py b/pcdet/ops/roipoint_pool3d/roipoint_pool3d_utils.py
index 1e13396..47a9cb4 100644
--- a/pcdet/ops/roipoint_pool3d/roipoint_pool3d_utils.py
+++ b/pcdet/ops/roipoint_pool3d/roipoint_pool3d_utils.py
@@ -3,7 +3,6 @@ import torch.nn as nn
 from torch.autograd import Function
 
 from ...utils import box_utils
-from . import roipoint_pool3d_cuda
 
 
 class RoIPointPool3d(nn.Module):
diff --git a/pcdet/utils/common_utils.py b/pcdet/utils/common_utils.py
index af70728..a060dcd 100644
--- a/pcdet/utils/common_utils.py
+++ b/pcdet/utils/common_utils.py
@@ -159,33 +159,6 @@ def keep_arrays_by_name(gt_names, used_classes):
     return inds
 
 
-def init_dist_slurm(tcp_port, local_rank, backend='nccl'):
-    """
-    modified from https://github.com/open-mmlab/mmdetection
-    Args:
-        tcp_port:
-        backend:
-
-    Returns:
-
-    """
-    proc_id = int(os.environ['SLURM_PROCID'])
-    ntasks = int(os.environ['SLURM_NTASKS'])
-    node_list = os.environ['SLURM_NODELIST']
-    num_gpus = torch.cuda.device_count()
-    torch.cuda.set_device(proc_id % num_gpus)
-    addr = subprocess.getoutput('scontrol show hostname {} | head -n1'.format(node_list))
-    os.environ['MASTER_PORT'] = str(tcp_port)
-    os.environ['MASTER_ADDR'] = addr
-    os.environ['WORLD_SIZE'] = str(ntasks)
-    os.environ['RANK'] = str(proc_id)
-    dist.init_process_group(backend=backend)
-
-    total_gpus = dist.get_world_size()
-    rank = dist.get_rank()
-    return total_gpus, rank
-
-
 def init_dist_pytorch(tcp_port, local_rank, backend='nccl'):
     if mp.get_start_method(allow_none=True) is None:
         mp.set_start_method('spawn')
diff --git a/pcdet/utils/commu_utils.py b/pcdet/utils/commu_utils.py
index d9e866f..a18b586 100644
--- a/pcdet/utils/commu_utils.py
+++ b/pcdet/utils/commu_utils.py
@@ -64,7 +64,7 @@ def all_gather(data):
     if not isinstance(data, torch.Tensor):
         buffer = pickle.dumps(data)
         storage = torch.ByteStorage.from_buffer(buffer)
-        tensor = torch.ByteTensor(storage).to("cuda")
+        tensor = torch.ByteTensor(storage)
     else:
         origin_size = data.size()
         tensor = data.reshape(-1)
@@ -72,10 +72,10 @@ def all_gather(data):
     tensor_type = tensor.dtype
 
     # obtain Tensor size of each rank
-    local_size = torch.LongTensor([tensor.numel()]).to("cuda")
-    size_list = [torch.LongTensor([0]).to("cuda") for _ in range(world_size)]
+    local_size = torch.FloatTensor([tensor.numel()])
+    size_list = [torch.FloatTensor([0]) for _ in range(world_size)]
     dist.all_gather(size_list, local_size)
-    size_list = [int(size.item()) for size in size_list]
+    size_list = [int(size) for size in size_list]
     max_size = max(size_list)
 
     # receiving Tensor from all ranks
@@ -83,9 +83,9 @@ def all_gather(data):
     # gathering tensors of different shapes
     tensor_list = []
     for _ in size_list:
-        tensor_list.append(torch.FloatTensor(size=(max_size,)).cuda().to(tensor_type))
+        tensor_list.append(torch.FloatTensor(size=(max_size,)).to(tensor_type))
     if local_size != max_size:
-        padding = torch.FloatTensor(size=(max_size - local_size,)).cuda().to(tensor_type)
+        padding = torch.FloatTensor(size=(max_size - local_size,)).to(tensor_type)
         tensor = torch.cat((tensor, padding), dim=0)
     dist.all_gather(tensor_list, tensor)
 
diff --git a/pcdet/utils/loss_utils.py b/pcdet/utils/loss_utils.py
index bd114ba..7943093 100644
--- a/pcdet/utils/loss_utils.py
+++ b/pcdet/utils/loss_utils.py
@@ -273,8 +273,8 @@ def neg_loss_cornernet(pred, gt, mask=None):
         mask: (batch x h x w)
     Returns:
     """
-    pos_inds = gt.eq(1).float()
-    neg_inds = gt.lt(1).float()
+    pos_inds = (gt == 1) 
+    neg_inds = (gt < 1) 
 
     neg_weights = torch.pow(1 - gt, 4)
 
@@ -284,20 +284,20 @@ def neg_loss_cornernet(pred, gt, mask=None):
     neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_weights * neg_inds
 
     if mask is not None:
-        mask = mask[:, None, :, :].float()
+        mask = mask.unsqueeze(1)
         pos_loss = pos_loss * mask
         neg_loss = neg_loss * mask
-        num_pos = (pos_inds.float() * mask).sum()
+        num_pos = (pos_inds * mask).sum()
     else:
-        num_pos = pos_inds.float().sum()
+        num_pos = pos_inds.sum()
 
-    pos_loss = pos_loss.sum()
     neg_loss = neg_loss.sum()
 
     if num_pos == 0:
-        loss = loss - neg_loss
+        loss -= neg_loss
     else:
-        loss = loss - (pos_loss + neg_loss) / num_pos
+        pos_loss = pos_loss.sum()
+        loss -= (pos_loss + neg_loss) / num_pos
     return loss
 
 
@@ -354,25 +354,13 @@ def _reg_loss(regr, gt_regr, mask):
         mask (batch x max_objects)
     Returns:
     """
-    num = mask.float().sum()
-    mask = mask.unsqueeze(2).expand_as(gt_regr).float()
-    isnotnan = (~ torch.isnan(gt_regr)).float()
-    mask *= isnotnan
+    num = mask.sum()
+    mask = mask.unsqueeze(2).expand_as(gt_regr)
     regr = regr * mask
     gt_regr = gt_regr * mask
 
-    loss = torch.abs(regr - gt_regr)
-    loss = loss.transpose(2, 0)
-
-    loss = torch.sum(loss, dim=2)
-    loss = torch.sum(loss, dim=1)
-    # else:
-    #  # D x M x B
-    #  loss = loss.reshape(loss.shape[0], -1)
-
-    # loss = loss / (num + 1e-4)
+    loss = torch.abs(regr - gt_regr).sum((0, 1))
     loss = loss / torch.clamp_min(num, min=1.0)
-    # import pdb; pdb.set_trace()
     return loss
 
 
@@ -388,8 +376,7 @@ def _gather_feat(feat, ind, mask=None):
 
 
 def _transpose_and_gather_feat(feat, ind):
-    feat = feat.permute(0, 2, 3, 1).contiguous()
-    feat = feat.view(feat.size(0), -1, feat.size(3))
+    feat = feat.permute(0, 2, 3, 1).flatten(1, 2)
     feat = _gather_feat(feat, ind)
     return feat
 
diff --git a/pcdet/utils/spconv_utils.py b/pcdet/utils/spconv_utils.py
index c38f899..cba5688 100644
--- a/pcdet/utils/spconv_utils.py
+++ b/pcdet/utils/spconv_utils.py
@@ -20,9 +20,8 @@ def find_all_spconv_keys(model: nn.Module, prefix="") -> Set[str]:
     for name, child in model.named_children():
         new_prefix = f"{prefix}.{name}" if prefix != "" else name
 
-        if isinstance(child, spconv.conv.SparseConvolution):
-            new_prefix = f"{new_prefix}.weight"
-            found_keys.add(new_prefix)
+        new_prefix = f"{new_prefix}.weight"
+        found_keys.add(new_prefix)
 
         found_keys.update(find_all_spconv_keys(child, prefix=new_prefix))
 
diff --git a/requirements.txt b/requirements.txt
index a730fd7..88a4b98 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,13 +1,17 @@
 numpy
 llvmlite
 numba
-torch>=1.1
 tensorboardX
 easydict
 pyyaml
 scikit-image
 tqdm
-torchvision
+torchvision==0.16.0
 SharedArray
 opencv-python
-pyquaternion
\ No newline at end of file
+pyquaternion
+pccm==0.3.4
+torch_scatter==2.1.1
+opencv-python-headless
+av2
+kornia==0.6.5
\ No newline at end of file
diff --git a/setup.py b/setup.py
index f5a31d9..784296c 100644
--- a/setup.py
+++ b/setup.py
@@ -2,7 +2,7 @@ import os
 import subprocess
 
 from setuptools import find_packages, setup
-from torch.utils.cpp_extension import BuildExtension, CUDAExtension
+from torch.utils.cpp_extension import BuildExtension, CUDAExtension, CppExtension
 
 
 def get_git_commit_number():
@@ -21,6 +21,16 @@ def make_cuda_ext(name, module, sources):
     )
     return cuda_ext
 
+def make_cpp_ext(name, module, sources):
+    sources_path = [os.path.join(*module.split('.'), src) for src in sources]
+    for _path in sources_path:
+        if not os.path.exists(_path):
+            raise FileNotFoundError(f"{_path} not exists!")
+    cpp_ext = CppExtension(
+        name=f'{module}.{name}',
+        sources=sources_path
+    )
+    return cpp_ext
 
 def write_version_to_file(version, target_file):
     with open(target_file, 'w') as f:
@@ -56,82 +66,20 @@ if __name__ == '__main__':
             'build_ext': BuildExtension,
         },
         ext_modules=[
-            make_cuda_ext(
-                name='iou3d_nms_cuda',
-                module='pcdet.ops.iou3d_nms',
-                sources=[
-                    'src/iou3d_cpu.cpp',
-                    'src/iou3d_nms_api.cpp',
-                    'src/iou3d_nms.cpp',
-                    'src/iou3d_nms_kernel.cu',
-                ]
-            ),
-            make_cuda_ext(
+            make_cpp_ext(
                 name='roiaware_pool3d_cuda',
                 module='pcdet.ops.roiaware_pool3d',
                 sources=[
                     'src/roiaware_pool3d.cpp',
-                    'src/roiaware_pool3d_kernel.cu',
                 ]
             ),
-            make_cuda_ext(
-                name='roipoint_pool3d_cuda',
-                module='pcdet.ops.roipoint_pool3d',
-                sources=[
-                    'src/roipoint_pool3d.cpp',
-                    'src/roipoint_pool3d_kernel.cu',
-                ]
-            ),
-            make_cuda_ext(
-                name='pointnet2_stack_cuda',
-                module='pcdet.ops.pointnet2.pointnet2_stack',
-                sources=[
-                    'src/pointnet2_api.cpp',
-                    'src/ball_query.cpp',
-                    'src/ball_query_gpu.cu',
-                    'src/group_points.cpp',
-                    'src/group_points_gpu.cu',
-                    'src/sampling.cpp',
-                    'src/sampling_gpu.cu', 
-                    'src/interpolate.cpp', 
-                    'src/interpolate_gpu.cu',
-                    'src/voxel_query.cpp', 
-                    'src/voxel_query_gpu.cu',
-                    'src/vector_pool.cpp',
-                    'src/vector_pool_gpu.cu'
-                ],
-            ),
-            make_cuda_ext(
-                name='pointnet2_batch_cuda',
-                module='pcdet.ops.pointnet2.pointnet2_batch',
-                sources=[
-                    'src/pointnet2_api.cpp',
-                    'src/ball_query.cpp',
-                    'src/ball_query_gpu.cu',
-                    'src/group_points.cpp',
-                    'src/group_points_gpu.cu',
-                    'src/interpolate.cpp',
-                    'src/interpolate_gpu.cu',
-                    'src/sampling.cpp',
-                    'src/sampling_gpu.cu',
-
-                ],
-            ),
-            make_cuda_ext(
-                name="bev_pool_ext",
-                module="pcdet.ops.bev_pool",
-                sources=[
-                    "src/bev_pool.cpp",
-                    "src/bev_pool_cuda.cu",
-                ],
-            ),
-            make_cuda_ext(
-                name='ingroup_inds_cuda',
-                module='pcdet.ops.ingroup_inds',
+            make_cpp_ext(
+                name='iou3d_nms_cuda', 
+                module='pcdet.ops.iou3d_nms',
                 sources=[
-                    'src/ingroup_inds.cpp',
-                    'src/ingroup_inds_kernel.cu',
+                    'src/iou3d_cpu.cpp',
+                    'src/iou3d_nms_api.cpp',
                 ]
-            ),
+            )
         ],
     )
diff --git a/third_party_patches/spconv_patches/spconv/core_cc/csrc/sparse/all/__init__.pyi b/third_party_patches/spconv_patches/spconv/core_cc/csrc/sparse/all/__init__.pyi
new file mode 100644
index 0000000..ce4439c
--- /dev/null
+++ b/third_party_patches/spconv_patches/spconv/core_cc/csrc/sparse/all/__init__.pyi
@@ -0,0 +1,104 @@
+from typing import overload, Any, Callable, Dict, List, Optional, Set, Tuple, Type, Union
+from pccm.stubs import EnumValue, EnumClassValue
+from cumm.tensorview import Tensor
+class ThrustCustomAllocatorV2:
+    alloc_func: Callable[int, int]
+class SpconvOps:
+    @staticmethod
+    def generate_conv_inds_cpu(indices: Tensor, indice_pairs: Tensor, out_inds: Tensor, indice_num_per_loc: Tensor, batch_size: int, output_dims: List[int], input_dims: List[int], ksize: List[int], stride: List[int], padding: List[int], dilation: List[int], transposed: bool = False) -> int: 
+        """
+        Args:
+            indices: 
+            indice_pairs: 
+            out_inds: 
+            indice_num_per_loc: 
+            batch_size: 
+            output_dims: 
+            input_dims: 
+            ksize: 
+            stride: 
+            padding: 
+            dilation: 
+            transposed: 
+        """
+        ...
+    @staticmethod
+    def generate_subm_conv_inds_cpu(indices: Tensor, indice_pairs: Tensor, out_inds: Tensor, indice_num_per_loc: Tensor, batch_size: int, input_dims: List[int], ksize: List[int], dilation: List[int]) -> int: 
+        """
+        Args:
+            indices: 
+            indice_pairs: 
+            out_inds: 
+            indice_num_per_loc: 
+            batch_size: 
+            input_dims: 
+            ksize: 
+            dilation: 
+        """
+        ...
+    @staticmethod
+    def maxpool_forward_cpu(out: Tensor, inp: Tensor, out_inds: Tensor, in_inds: Tensor) -> None: 
+        """
+        Args:
+            out: 
+            inp: 
+            out_inds: 
+            in_inds: 
+        """
+        ...
+    @staticmethod
+    def maxpool_backward_cpu(out: Tensor, inp: Tensor, dout: Tensor, dinp: Tensor, out_inds: Tensor, in_inds: Tensor) -> None: 
+        """
+        Args:
+            out: 
+            inp: 
+            dout: 
+            dinp: 
+            out_inds: 
+            in_inds: 
+        """
+        ...
+    @staticmethod
+    def gather_cpu(out: Tensor, inp: Tensor, inds: Tensor) -> None: 
+        """
+        Args:
+            out: 
+            inp: 
+            inds: 
+        """
+        ...
+    @staticmethod
+    def scatter_add_cpu(out: Tensor, inp: Tensor, inds: Tensor) -> None: 
+        """
+        Args:
+            out: 
+            inp: 
+            inds: 
+        """
+        ...
+    @staticmethod
+    def calc_point2voxel_meta_data(vsize_xyz: List[float], coors_range_xyz: List[float]) -> Tuple[List[float], List[int], List[int], List[float]]: 
+        """
+        Args:
+            vsize_xyz: 
+            coors_range_xyz: 
+        """
+        ...
+    @staticmethod
+    def point2voxel_cpu(points: Tensor, voxels: Tensor, indices: Tensor, num_per_voxel: Tensor, densehashdata: Tensor, pc_voxel_id: Tensor, vsize: List[float], grid_size: List[int], grid_stride: List[int], coors_range: List[float], empty_mean: bool = False, clear_voxels: bool = True) -> Tuple[Tensor, Tensor, Tensor]: 
+        """
+        Args:
+            points: 
+            voxels: 
+            indices: 
+            num_per_voxel: 
+            densehashdata: 
+            pc_voxel_id: 
+            vsize: 
+            grid_size: 
+            grid_stride: 
+            coors_range: 
+            empty_mean: 
+            clear_voxels: 
+        """
+        ...
diff --git a/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint.yaml b/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint.yaml
index 51d6c7b..24e3fe5 100644
--- a/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint.yaml
+++ b/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint.yaml
@@ -15,18 +15,23 @@ DATA_CONFIG:
                 'test': True
             }
 
-        -   NAME: transform_points_to_voxels_placeholder
+        -   NAME: transform_points_to_voxels
             VOXEL_SIZE: [0.2, 0.2, 8.0]
+            MAX_POINTS_PER_VOXEL: 20
+            MAX_NUMBER_OF_VOXELS: {
+                'train': 30000,
+                'test': 30000
+            }
 
 MODEL:
     NAME: CenterPoint
 
     VFE:
-        NAME: DynPillarVFE
+        NAME: PillarVFE
         WITH_DISTANCE: False
         USE_ABSLOTE_XYZ: True
         USE_NORM: True
-        NUM_FILTERS: [ 64, 64 ]
+        NUM_FILTERS: [ 64 ]
 
     MAP_TO_BEV:
         NAME: PointPillarScatter
@@ -45,7 +50,7 @@ MODEL:
         CLASS_AGNOSTIC: False
 
         CLASS_NAMES_EACH_HEAD: [
-            ['car'], 
+            ['car'],
             ['truck', 'construction_vehicle'],
             ['bus', 'trailer'],
             ['barrier'],
@@ -96,11 +101,11 @@ MODEL:
 
 
 OPTIMIZATION:
-    BATCH_SIZE_PER_GPU: 4
+    BATCH_SIZE_PER_GPU: 12
     NUM_EPOCHS: 20
 
     OPTIMIZER: adam_onecycle
-    LR: 0.001
+    LR: 0.003
     WEIGHT_DECAY: 0.01
     MOMENTUM: 0.9
 
diff --git a/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint_performance.yaml b/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint_performance.yaml
new file mode 100644
index 0000000..f801449
--- /dev/null
+++ b/tools/cfgs/nuscenes_models/cbgs_dyn_pp_centerpoint_performance.yaml
@@ -0,0 +1,122 @@
+CLASS_NAMES: ['car','truck', 'construction_vehicle', 'bus', 'trailer',
+              'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']
+
+DATA_CONFIG:
+    _BASE_CONFIG_: cfgs/dataset_configs/nuscenes_dataset.yaml
+
+    POINT_CLOUD_RANGE: [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
+    DATA_PROCESSOR:
+        -   NAME: mask_points_and_boxes_outside_range
+            REMOVE_OUTSIDE_BOXES: True
+
+        -   NAME: shuffle_points
+            SHUFFLE_ENABLED: {
+                'train': True,
+                'test': True
+            }
+
+        -   NAME: transform_points_to_voxels
+            VOXEL_SIZE: [0.2, 0.2, 8.0]
+            MAX_POINTS_PER_VOXEL: 20
+            MAX_NUMBER_OF_VOXELS: {
+                'train': 30000,
+                'test': 30000
+            }
+
+MODEL:
+    NAME: CenterPoint
+
+    VFE:
+        NAME: PillarVFE
+        WITH_DISTANCE: False
+        USE_ABSLOTE_XYZ: True
+        USE_NORM: True
+        NUM_FILTERS: [ 64 ]
+
+    MAP_TO_BEV:
+        NAME: PointPillarScatter
+        NUM_BEV_FEATURES: 64
+
+    BACKBONE_2D:
+        NAME: BaseBEVBackbone
+        LAYER_NUMS: [3, 5, 5]
+        LAYER_STRIDES: [2, 2, 2]
+        NUM_FILTERS: [64, 128, 256]
+        UPSAMPLE_STRIDES: [0.5, 1, 2]
+        NUM_UPSAMPLE_FILTERS: [128, 128, 128]
+
+    DENSE_HEAD:
+        NAME: CenterHead
+        CLASS_AGNOSTIC: False
+
+        CLASS_NAMES_EACH_HEAD: [
+            ['car'],
+            ['truck', 'construction_vehicle'],
+            ['bus', 'trailer'],
+            ['barrier'],
+            ['motorcycle', 'bicycle'],
+            ['pedestrian', 'traffic_cone'],
+        ]
+
+        SHARED_CONV_CHANNEL: 64
+        USE_BIAS_BEFORE_NORM: True
+        NUM_HM_CONV: 2
+        SEPARATE_HEAD_CFG:
+            HEAD_ORDER: ['center', 'center_z', 'dim', 'rot', 'vel']
+            HEAD_DICT: {
+                'center': {'out_channels': 2, 'num_conv': 2},
+                'center_z': {'out_channels': 1, 'num_conv': 2},
+                'dim': {'out_channels': 3, 'num_conv': 2},
+                'rot': {'out_channels': 2, 'num_conv': 2},
+                'vel': {'out_channels': 2, 'num_conv': 2},
+            }
+
+        TARGET_ASSIGNER_CONFIG:
+            FEATURE_MAP_STRIDE: 4
+            NUM_MAX_OBJS: 500
+            GAUSSIAN_OVERLAP: 0.1
+            MIN_RADIUS: 2
+
+        LOSS_CONFIG:
+            LOSS_WEIGHTS: {
+                'cls_weight': 1.0,
+                'loc_weight': 0.25,
+                'code_weights': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2, 1.0, 1.0]
+            }
+
+        POST_PROCESSING:
+            SCORE_THRESH: 0.1
+            POST_CENTER_LIMIT_RANGE: [-61.2, -61.2, -10.0, 61.2, 61.2, 10.0]
+            MAX_OBJ_PER_SAMPLE: 500
+            NMS_CONFIG:
+                NMS_TYPE: nms_gpu
+                NMS_THRESH: 0.2
+                NMS_PRE_MAXSIZE: 1000
+                NMS_POST_MAXSIZE: 83
+
+    POST_PROCESSING:
+        RECALL_THRESH_LIST: [0.3, 0.5, 0.7]
+
+        EVAL_METRIC: kitti
+
+
+OPTIMIZATION:
+    BATCH_SIZE_PER_GPU: 12
+    NUM_EPOCHS: 1
+
+    OPTIMIZER: adam_onecycle
+    LR: 0.003
+    WEIGHT_DECAY: 0.01
+    MOMENTUM: 0.9
+
+    MOMS: [0.95, 0.85]
+    PCT_START: 0.4
+    DIV_FACTOR: 10
+    DECAY_STEP_LIST: [35, 45]
+    LR_DECAY: 0.1
+    LR_CLIP: 0.0000001
+
+    LR_WARMUP: False
+    WARMUP_EPOCH: 1
+
+    GRAD_NORM_CLIP: 10
diff --git a/tools/test.py b/tools/test.py
index 51b7178..cd8652d 100644
--- a/tools/test.py
+++ b/tools/test.py
@@ -9,6 +9,8 @@ from pathlib import Path
 
 import numpy as np
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 from tensorboardX import SummaryWriter
 
 from eval_utils import eval_utils
@@ -29,7 +31,7 @@ def parse_config():
     parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')
     parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')
     parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distrbuted training')
-    parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')
+    parser.add_argument('--local-rank', type=int, default=0, help='local rank for distributed training')
     parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,
                         help='set extra config keys if needed')
 
diff --git a/tools/train.py b/tools/train.py
index 29a88bd..340a65b 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -7,6 +7,8 @@ from pathlib import Path
 from test import repeat_eval_ckpt
 
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import torch.nn as nn
 from tensorboardX import SummaryWriter
 
@@ -33,7 +35,7 @@ def parse_config():
     parser.add_argument('--sync_bn', action='store_true', default=False, help='whether to use sync bn')
     parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')
     parser.add_argument('--ckpt_save_interval', type=int, default=1, help='number of training epochs')
-    parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')
+    parser.add_argument('--local-rank', type=int, default=0, help='local rank for distributed training')
     parser.add_argument('--max_ckpt_save_num', type=int, default=30, help='max number of saved checkpoint')
     parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')
     parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,
@@ -72,7 +74,7 @@ def main():
         total_gpus = 1
     else:
         total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(
-            args.tcp_port, args.local_rank, backend='nccl'
+            args.tcp_port, args.local_rank, backend='hccl'
         )
         dist_train = True
 
@@ -108,7 +110,7 @@ def main():
     for key, val in vars(args).items():
         logger.info('{:16} {}'.format(key, val))
     log_config_to_file(cfg, logger=logger)
-    if cfg.LOCAL_RANK == 0:
+    if cfg.LOCAL_RANK == 0 and os.path.exists(args.cfg_file) and os.path.exists(output_dir):
         os.system('cp %s %s' % (args.cfg_file, output_dir))
 
     tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None
@@ -129,7 +131,7 @@ def main():
     model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=train_set)
     if args.sync_bn:
         model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
-    model.cuda()
+    model.npu()
 
     optimizer = build_optimizer(model, cfg.OPTIMIZATION)
 
diff --git a/tools/train_utils/optimization/__init__.py b/tools/train_utils/optimization/__init__.py
index 888cfcf..aa631f0 100644
--- a/tools/train_utils/optimization/__init__.py
+++ b/tools/train_utils/optimization/__init__.py
@@ -6,7 +6,8 @@ import torch.optim.lr_scheduler as lr_sched
 
 from .fastai_optim import OptimWrapper
 from .learning_schedules_fastai import CosineWarmupLR, OneCycle, CosineAnnealing
-
+import torch
+import torch_npu
 
 def build_optimizer(model, optim_cfg):
     if optim_cfg.OPTIMIZER == 'adam':
@@ -27,7 +28,7 @@ def build_optimizer(model, optim_cfg):
         get_layer_groups = lambda m: [nn.Sequential(*flatten_model(m))]
         betas = optim_cfg.get('BETAS', (0.9, 0.99))
         betas = tuple(betas)
-        optimizer_func = partial(optim.Adam, betas=betas)
+        optimizer_func = partial(torch_npu.optim.NpuFusedAdam, betas=betas)
         optimizer = OptimWrapper.create(
             optimizer_func, 3e-3, get_layer_groups(model), wd=optim_cfg.WEIGHT_DECAY, true_wd=True, bn_wd=True
         )
diff --git a/tools/train_utils/train_utils.py b/tools/train_utils/train_utils.py
index 04071fb..fcca28f 100644
--- a/tools/train_utils/train_utils.py
+++ b/tools/train_utils/train_utils.py
@@ -46,20 +46,13 @@ def train_one_epoch(model, optimizer, train_loader, model_func, lr_scheduler, ac
         except:
             cur_lr = optimizer.param_groups[0]['lr']
 
-        if tb_log is not None:
-            tb_log.add_scalar('meta_data/learning_rate', cur_lr, accumulated_iter)
-
-        model.train()
         optimizer.zero_grad()
 
-        with torch.cuda.amp.autocast(enabled=use_amp):
-            loss, tb_dict, disp_dict = model_func(model, batch)
-
-        scaler.scale(loss).backward()
-        scaler.unscale_(optimizer)
-        clip_grad_norm_(model.parameters(), optim_cfg.GRAD_NORM_CLIP)
-        scaler.step(optimizer)
-        scaler.update()
+        ret_dict, tb_dict, disp_dict = model(batch)
+        loss = ret_dict['loss'].mean()
+        loss.backward()
+        optimizer.clip_grad_norm_fused_(optim_cfg.GRAD_NORM_CLIP, norm_type=2)
+        optimizer.step()
 
         accumulated_iter += 1
  
@@ -68,9 +61,9 @@ def train_one_epoch(model, optimizer, train_loader, model_func, lr_scheduler, ac
         end = time.time()
 
         # average reduce
-        avg_data_time = commu_utils.average_reduce_value(cur_data_time)
-        avg_forward_time = commu_utils.average_reduce_value(cur_forward_time)
-        avg_batch_time = commu_utils.average_reduce_value(cur_batch_time)
+        avg_data_time = cur_data_time
+        avg_forward_time = cur_forward_time
+        avg_batch_time = cur_batch_time
 
         # log to console and tensorboard
         if rank == 0:
@@ -79,12 +72,8 @@ def train_one_epoch(model, optimizer, train_loader, model_func, lr_scheduler, ac
             data_time.update(avg_data_time)
             forward_time.update(avg_forward_time)
             batch_time.update(avg_batch_time)
-            losses_m.update(loss.item() , batch_size)
+            losses_m.update(loss, batch_size)
             
-            disp_dict.update({
-                'loss': loss.item(), 'lr': cur_lr, 'd_time': f'{data_time.val:.2f}({data_time.avg:.2f})',
-                'f_time': f'{forward_time.val:.2f}({forward_time.avg:.2f})', 'b_time': f'{batch_time.val:.2f}({batch_time.avg:.2f})'
-            })
             
             if use_logger_to_record:
                 if accumulated_iter % logger_iter_interval == 0 or cur_it == start_it or cur_it + 1 == total_it_each_epoch:
@@ -116,21 +105,9 @@ def train_one_epoch(model, optimizer, train_loader, model_func, lr_scheduler, ac
                             )
                     )
                     
-                    if show_gpu_stat and accumulated_iter % (3 * logger_iter_interval) == 0:
-                        # To show the GPU utilization, please install gpustat through "pip install gpustat"
-                        gpu_info = os.popen('gpustat').read()
-                        logger.info(gpu_info)
             else:                
                 pbar.update()
                 pbar.set_postfix(dict(total_it=accumulated_iter))
-                tbar.set_postfix(disp_dict)
-                # tbar.refresh()
-
-            if tb_log is not None:
-                tb_log.add_scalar('train/loss', loss, accumulated_iter)
-                tb_log.add_scalar('meta_data/learning_rate', cur_lr, accumulated_iter)
-                for key, val in tb_dict.items():
-                    tb_log.add_scalar('train/' + key, val, accumulated_iter)
             
             # save intermediate ckpt every {ckpt_save_time_interval} seconds         
             time_past_this_epoch = pbar.format_dict['elapsed']
