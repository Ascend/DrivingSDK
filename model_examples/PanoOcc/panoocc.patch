diff --git a/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py b/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py
index 58a49f0..6c5e8e1 100644
--- a/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py
+++ b/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py
@@ -307,7 +307,7 @@ data = dict(
 )
 
 optimizer = dict(
-    type='AdamW',
+    type='NpuFusedAdamW',
     lr=4e-4,
     paramwise_cfg=dict(
         custom_keys={
diff --git a/projects/configs/_base_/default_runtime.py b/projects/configs/_base_/default_runtime.py
index 4e85b69..cd301c6 100644
--- a/projects/configs/_base_/default_runtime.py
+++ b/projects/configs/_base_/default_runtime.py
@@ -10,7 +10,7 @@ log_config = dict(
         dict(type='TensorboardLoggerHook')
     ])
 # yapf:enable
-dist_params = dict(backend='nccl')
+dist_params = dict(backend='hccl')
 log_level = 'INFO'
 work_dir = None
 load_from = None
diff --git a/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py b/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py
index e57bd22..0331822 100644
--- a/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py
+++ b/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py
@@ -1,5 +1,6 @@
 # ---------------------------------------------
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 # ---------------------------------------------
 #  Modified by Zhiqi Li
 # ---------------------------------------------
@@ -9,7 +10,7 @@ import warnings
 import numpy as np
 import torch
 import torch.distributed as dist
-from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
                          Fp16OptimizerHook, OptimizerHook, build_optimizer,
                          build_runner, get_dist_info)
@@ -64,6 +65,7 @@ def custom_train_detector(model,
             seed=cfg.seed,
             shuffler_sampler=cfg.data.shuffler_sampler,  # dict(type='DistributedGroupSampler'),
             nonshuffler_sampler=cfg.data.nonshuffler_sampler,  # dict(type='DistributedSampler'),
+            pin_memory=True
         ) for ds in dataset
     ]
 
@@ -72,22 +74,22 @@ def custom_train_detector(model,
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
         if eval_model is not None:
-            eval_model = MMDistributedDataParallel(
+            eval_model = NPUDistributedDataParallel(
                 eval_model.cuda(),
                 device_ids=[torch.cuda.current_device()],
                 broadcast_buffers=False,
                 find_unused_parameters=find_unused_parameters)
     else:
-        model = MMDataParallel(
+        model = NPUDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
         if eval_model is not None:
-            eval_model = MMDataParallel(
+            eval_model = NPUDataParallel(
                 eval_model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
 
diff --git a/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py b/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py
index a831372..8d9815a 100644
--- a/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py
+++ b/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py
@@ -1,3 +1,2 @@
 from .pano_occ_head import PanoOccHead
-from .panoseg_occ_head import PanoSegOccHead
-from .panoseg_occ_sparse_head import SparseOccupancyHead
\ No newline at end of file
+from .panoseg_occ_head import PanoSegOccHead
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/dense_heads/pano_occ_head.py b/projects/mmdet3d_plugin/bevformer/dense_heads/pano_occ_head.py
index 2bb8795..ed1ac3c 100644
--- a/projects/mmdet3d_plugin/bevformer/dense_heads/pano_occ_head.py
+++ b/projects/mmdet3d_plugin/bevformer/dense_heads/pano_occ_head.py
@@ -1,3 +1,9 @@
+# ---------------------------------------------
+# Copyright (c) OpenMMLab. All rights reserved.
+# ---------------------------------------------
+#  Modified by Zhiqi Li
+# ---------------------------------------------
+
 import copy
 import torch
 import torch.nn as nn
diff --git a/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py b/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py
index cc986f0..c5c69fe 100644
--- a/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py
+++ b/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py
@@ -1,3 +1,10 @@
+# ---------------------------------------------
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+# ---------------------------------------------
+#  Modified by Zhiqi Li
+# ---------------------------------------------
+
 import copy
 import torch
 import torch.nn as nn
@@ -18,10 +25,42 @@ import numpy as np
 import mmcv
 import cv2 as cv
 from projects.mmdet3d_plugin.models.utils.visual import save_tensor
-from mmdet3d.ops import scatter_v2
-import torch_scatter
+import mx_driving.common
+import mx_driving._C
 from mmdet.models.builder import build_loss
 
+
+def custom_unique_n3(coors, return_inverse, return_counts, dim):
+    # assert dim == 0
+
+    voxels = mx_driving._C.point_to_voxel(coors, [], [], "ZYX")
+    cnt, unq_voxels, unq_ind, argsort_ind, _ = mx_driving._C.unique_voxel(voxels)
+    unq_coors = mx_driving._C.voxel_to_point(unq_voxels, [], [], "ZYX")
+    
+    if return_inverse:
+        sorted_ind = torch.argsort(argsort_ind.to(torch.float32), dim=dim).to(torch.long)
+        is_unq = torch.zeros(coors.size(0)).to(coors.device)
+        is_unq[unq_ind] = 1
+        unq_inv_sorted = is_unq.cumsum(dim) - 1
+        unq_inv = torch.gather(unq_inv_sorted, dim, sorted_ind)
+        unq_inv = unq_inv.to(torch.int64)
+
+    if return_counts:
+        unq_ind_nxt = torch.ones_like(unq_ind) * coors.size(0)
+        unq_ind_nxt[:-1] = unq_ind[1:]
+        unq_cnt = unq_ind_nxt - unq_ind
+        unq_cnt = unq_cnt.to(torch.int64)
+
+    if return_inverse and return_counts:
+        return unq_coors, unq_inv, unq_cnt
+    elif return_inverse:
+        return unq_coors, unq_inv
+    elif return_counts:
+        return unq_coors, unq_cnt
+    else:
+        return unq_coors
+
+
 @HEADS.register_module()
 class PanoSegOccHead(DETRHead):
     def __init__(self,
@@ -35,22 +74,22 @@ class PanoSegOccHead(DETRHead):
                  bev_h=30,
                  bev_w=30,
                  bev_z=5,
-                 voxel_lidar = [0.05, 0.05, 0.05],
-                 voxel_det = [2.048,2.048,1],
+                 voxel_lidar=[0.05, 0.05, 0.05],
+                 voxel_det=[2.048,2.048,1],
                  loss_occupancy=dict(
                     type='FocalLoss',
                     use_sigmoid=True,
                     gamma=2.0,
                     alpha=0.25,
                     loss_weight=5.0),
-                loss_occupancy_aux = None,
+                loss_occupancy_aux=None,
                 loss_occupancy_det=dict(
                     type='FocalLoss',
                     use_sigmoid=True,
                     gamma=2.0,
                     alpha=0.25,
                     loss_weight=5.0),
-                bg_weight = 0.02,
+                bg_weight=0.02,
                  **kwargs):
 
         self.bev_h = bev_h
@@ -88,6 +127,13 @@ class PanoSegOccHead(DETRHead):
         if loss_occupancy_aux is not None:
             self.lidar_seg_aux_loss = build_loss(loss_occupancy_aux)
 
+        self.pc_range = nn.Parameter(torch.tensor(
+            self.pc_range, requires_grad=False), requires_grad=False)
+        self.voxel_lidar = nn.Parameter(torch.tensor(
+            self.voxel_lidar, requires_grad=False), requires_grad=False)
+        self.voxel_det = nn.Parameter(torch.tensor(
+            self.voxel_det, requires_grad=False), requires_grad=False)
+
     def _init_layers(self):
         """Initialize classification branch and regression branch of head."""
         cls_branch = []
@@ -159,7 +205,7 @@ class PanoSegOccHead(DETRHead):
         object_query_embeds = self.query_embedding.weight.to(dtype)
         bev_queries = self.bev_embedding.weight.to(dtype)
 
-        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w, self.bev_z),device=bev_queries.device).to(dtype)
+        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w, self.bev_z), device=bev_queries.device, dtype=dtype)
         bev_pos = self.positional_encoding(bev_mask).to(dtype)
 
         if only_bev:
@@ -180,21 +226,21 @@ class PanoSegOccHead(DETRHead):
             )
             bev_feat, bev_embed, hs, init_reference, inter_references, occupancy, occupancy_det = outputs
             return bev_feat, bev_embed
-        else:
-            outputs = self.transformer(
-                mlvl_feats,
-                bev_queries,
-                object_query_embeds,
-                self.bev_h,
-                self.bev_w,
-                self.bev_z,
-                grid_length=(self.real_h / self.bev_h,
-                             self.real_w / self.bev_w),
-                bev_pos=bev_pos,
-                reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
-                cls_branches=self.cls_branches if self.as_two_stage else None,
-                img_metas=img_metas,
-                prev_bev=prev_bev
+
+        outputs = self.transformer(
+            mlvl_feats,
+            bev_queries,
+            object_query_embeds,
+            self.bev_h,
+            self.bev_w,
+            self.bev_z,
+            grid_length=(self.real_h / self.bev_h,
+                            self.real_w / self.bev_w),
+            bev_pos=bev_pos,
+            reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
+            cls_branches=self.cls_branches if self.as_two_stage else None,
+            img_metas=img_metas,
+            prev_bev=prev_bev
         )
 
         bev_feat, bev_embed, hs, init_reference, inter_references, occupancy, occupancy_det = outputs
@@ -211,7 +257,7 @@ class PanoSegOccHead(DETRHead):
             tmp = self.reg_branches[lvl](hs[lvl])
 
             # TODO: check the shape of reference
-            assert reference.shape[-1] == 3
+            # assert reference.shape[-1] == 3
             tmp[..., 0:2] += reference[..., 0:2]
             tmp[..., 0:2] = tmp[..., 0:2].sigmoid()
             tmp[..., 4:5] += reference[..., 2:3]
@@ -279,7 +325,7 @@ class PanoSegOccHead(DETRHead):
         gt_c = gt_bboxes.shape[-1]
 
         assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes,
-                                             gt_labels, gt_bboxes_ignore)
+                                             gt_labels)
 
         sampling_result = self.sampler.sample(assign_result, bbox_pred,
                                               gt_bboxes)
@@ -338,17 +384,11 @@ class PanoSegOccHead(DETRHead):
                 - num_total_neg (int): Number of negative samples in all \
                     images.
         """
-        assert gt_bboxes_ignore_list is None, \
-            'Only supports for gt_bboxes_ignore setting to None.'
-        num_imgs = len(cls_scores_list)
-        gt_bboxes_ignore_list = [
-            gt_bboxes_ignore_list for _ in range(num_imgs)
-        ]
 
         (labels_list, label_weights_list, bbox_targets_list,
          bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(
             self._get_target_single, cls_scores_list, bbox_preds_list,
-            gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)
+            gt_labels_list, gt_bboxes_list)
         num_total_pos = sum((inds.numel() for inds in pos_inds_list))
         num_total_neg = sum((inds.numel() for inds in neg_inds_list))
         return (labels_list, label_weights_list, bbox_targets_list,
@@ -382,8 +422,7 @@ class PanoSegOccHead(DETRHead):
         cls_scores_list = [cls_scores[i] for i in range(num_imgs)]
         bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]
         cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list,
-                                           gt_bboxes_list, gt_labels_list,
-                                           gt_bboxes_ignore_list)
+                                           gt_bboxes_list, gt_labels_list)
         (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,
          num_total_pos, num_total_neg) = cls_reg_targets
         labels = torch.cat(labels_list, 0)
@@ -394,11 +433,9 @@ class PanoSegOccHead(DETRHead):
         # classification loss
         cls_scores = cls_scores.reshape(-1, self.cls_out_channels)
         # construct weighted avg_factor to match with the official DETR repo
-        cls_avg_factor = num_total_pos * 1.0 + \
-            num_total_neg * self.bg_cls_weight
+        cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight
         if self.sync_cls_avg_factor:
-            cls_avg_factor = reduce_mean(
-                cls_scores.new_tensor([cls_avg_factor]))
+            cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))
 
         cls_avg_factor = max(cls_avg_factor, 1)
 
@@ -417,7 +454,7 @@ class PanoSegOccHead(DETRHead):
         bbox_weights = bbox_weights * self.code_weights
 
         loss_bbox = self.loss_bbox(
-            bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan,:10], bbox_weights[isnotnan, :10],
+            bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10],
             avg_factor=num_total_pos)
         if digit_version(TORCH_VERSION) >= digit_version('1.8'):
             loss_cls = torch.nan_to_num(loss_cls)
@@ -426,13 +463,13 @@ class PanoSegOccHead(DETRHead):
     
     def get_occupancy_det_label(self,voxel_coors_det, voxel_label_det, occupancy_det_label):
 
-        voxel_coors_det[:,1] = voxel_coors_det[:,1].clip(min=0,max=self.bev_z-1)
-        voxel_coors_det[:,2] = voxel_coors_det[:,2].clip(min=0,max=self.bev_h-1)
-        voxel_coors_det[:,3] = voxel_coors_det[:,3].clip(min=0,max=self.bev_w-1)
+        voxel_coors_det[:, 0] = voxel_coors_det[:, 0].clip(min=0, max=self.bev_z-1)
+        voxel_coors_det[:, 1] = voxel_coors_det[:, 1].clip(min=0, max=self.bev_h-1)
+        voxel_coors_det[:, 2] = voxel_coors_det[:, 2].clip(min=0, max=self.bev_w-1)
 
-        det_label_binary = ((voxel_label_det>=1)&(voxel_label_det<=10))
+        det_label_binary = ((voxel_label_det>=1) & (voxel_label_det<=10))
         det_label = det_label_binary.long()
-        occupancy_det_label[0,voxel_coors_det[:,1],voxel_coors_det[:,2],voxel_coors_det[:,3]]=det_label
+        occupancy_det_label[0, voxel_coors_det[:, 0], voxel_coors_det[:, 1], voxel_coors_det[:, 2]] = det_label
         return occupancy_det_label
     
     def get_det_loss(self,voxel_label_det,occupancy_det_label,occupancy_det_pred):
@@ -446,7 +483,7 @@ class PanoSegOccHead(DETRHead):
                 occupancy_det_pred.new_tensor([avg_factor_det]))
         avg_factor_det = max(avg_factor_det, 1)
 
-        losses_det = self.lidar_det_loss(occupancy_det_pred,occupancy_det_label,avg_factor=avg_factor_det)
+        losses_det = self.lidar_det_loss(occupancy_det_pred, occupancy_det_label, avg_factor=avg_factor_det)
         return losses_det
     
     @force_fp32(apply_to=('preds_dicts'))
@@ -455,7 +492,7 @@ class PanoSegOccHead(DETRHead):
              gt_labels_list,
              pts_sem,
              preds_dicts,
-             dense_occupancy = None,
+             dense_occupancy=None,
              gt_bboxes_ignore=None,
              img_metas=None):
         """"Loss function.
@@ -485,9 +522,6 @@ class PanoSegOccHead(DETRHead):
         Returns:
             dict[str, Tensor]: A dictionary of loss components.
         """
-        assert gt_bboxes_ignore is None, \
-            f'{self.__class__.__name__} only supports ' \
-            f'for gt_bboxes_ignore setting to None.'
 
        # Extract the first three columns from pts_sem
         pts = pts_sem[:, :3]
@@ -497,12 +531,10 @@ class PanoSegOccHead(DETRHead):
 
         # If dense_occupancy is None, perform voxelization and label voxelization
         if dense_occupancy is None:
-            pts_coors, voxelized_data, voxel_coors = self.voxelize(pts, self.pc_range, self.voxel_lidar)
-            voxel_label = self.label_voxelization(pts_semantic_mask, pts_coors, voxel_coors)
+            _, voxel_coors, voxel_label = self.voxelize(pts, self.pc_range, self.voxel_lidar, pts_semantic_mask)
 
         # Perform voxelization and label voxelization for detection
-        pts_coors_det, voxelized_data_det, voxel_coors_det = self.voxelize(pts, self.pc_range, self.voxel_det)
-        voxel_label_det = self.label_voxelization(pts_semantic_mask, pts_coors_det, voxel_coors_det)
+        _, voxel_coors_det, voxel_label_det = self.voxelize(pts, self.pc_range, self.voxel_det, pts_semantic_mask)
         
         all_cls_scores = preds_dicts['all_cls_scores']
         all_bbox_preds = preds_dicts['all_bbox_preds']
@@ -514,31 +546,31 @@ class PanoSegOccHead(DETRHead):
         occupancy_pred = occupancy.squeeze(0)
         occupancy_det_pred = occupancy_det[0].squeeze(0)
 
-        cls_num,occ_z,occ_h,occ_w = occupancy_pred.shape
+        cls_num, occ_z, occ_h, occ_w = occupancy_pred.shape
         if dense_occupancy is None:
             occupancy_label = torch.full((1, occ_z, occ_h, occ_w), cls_num, device=occupancy_pred.device, dtype=torch.long)
         else:
-            occupancy_label = (torch.zeros(1,occ_z,occ_h,occ_w)).to(occupancy_pred.device).long()
+            occupancy_label = (torch.zeros(1, occ_z, occ_h, occ_w)).to(occupancy_pred.device).long()
        
-        occupancy_det_label = (torch.ones(1,self.bev_z,self.bev_h,self.bev_w)*2).to(occupancy_det_pred.device).long()
+        occupancy_det_label = (torch.ones(1, self.bev_z, self.bev_h, self.bev_w) * 2).to(occupancy_det_pred.device).long()
 
         if dense_occupancy is None:
-            voxel_coors[:,1] = voxel_coors[:,1].clip(min=0,max=occ_z-1)
-            voxel_coors[:,2] = voxel_coors[:,2].clip(min=0,max=occ_h-1)
-            voxel_coors[:,3] = voxel_coors[:,3].clip(min=0,max=occ_w-1)
-            occupancy_label[0,voxel_coors[:,1],voxel_coors[:,2],voxel_coors[:,3]] = voxel_label
+            voxel_coors[:, 0] = voxel_coors[:, 0].clip(min=0, max=occ_z-1)
+            voxel_coors[:, 1] = voxel_coors[:, 1].clip(min=0, max=occ_h-1)
+            voxel_coors[:, 2] = voxel_coors[:, 2].clip(min=0, max=occ_w-1)
+            occupancy_label[0, voxel_coors[:, 0], voxel_coors[:, 1], voxel_coors[:, 2]] = voxel_label
         else:
             dense_occupancy = dense_occupancy.long().squeeze(0)
-            occupancy_label[0,dense_occupancy[:,0],dense_occupancy[:,1],dense_occupancy[:,2]]=dense_occupancy[:,3]
+            occupancy_label[0, dense_occupancy[:, 0], dense_occupancy[:, 1], dense_occupancy[:, 2]] = dense_occupancy[:, 3]
 
         occupancy_det_label = self.get_occupancy_det_label(voxel_coors_det, voxel_label_det, occupancy_det_label)
 
-        losses_seg_aux = self.lidar_seg_aux_loss(occupancy_pred.unsqueeze(0),occupancy_label)
+        losses_seg_aux = self.lidar_seg_aux_loss(occupancy_pred.unsqueeze(0), occupancy_label)
 
         occupancy_det_label = occupancy_det_label.reshape(-1)
         occupancy_label = occupancy_label.reshape(-1)
 
-        assert occupancy_label.max()<=cls_num and occupancy_label.min()>=0
+        # assert occupancy_label.max()<=cls_num and occupancy_label.min()>=0
         occupancy_pred = occupancy_pred.reshape(cls_num,-1).permute(1,0)
         occupancy_det_pred = occupancy_det_pred.reshape(2,-1).permute(1,0)
 
@@ -550,14 +582,10 @@ class PanoSegOccHead(DETRHead):
 
         all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]
         all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]
-        all_gt_bboxes_ignore_list = [
-            gt_bboxes_ignore for _ in range(num_dec_layers)
-        ]
 
         losses_cls, losses_bbox = multi_apply(
             self.loss_single, all_cls_scores, all_bbox_preds,
-            all_gt_bboxes_list, all_gt_labels_list,
-            all_gt_bboxes_ignore_list)
+            all_gt_bboxes_list, all_gt_labels_list)
 
         loss_dict = dict()
 
@@ -566,17 +594,17 @@ class PanoSegOccHead(DETRHead):
             num_total_pos = len(voxel_label)
         else:
             num_total_pos = len(dense_occupancy)
-        num_total_neg = len(occupancy_label)-num_total_pos
+        num_total_neg = len(occupancy_label) - num_total_pos
         avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_weight
         if self.sync_cls_avg_factor:
             avg_factor = reduce_mean(
                 occupancy_pred.new_tensor([avg_factor]))
         avg_factor = max(avg_factor, 1)
 
-        losses_seg = self.lidar_seg_loss(occupancy_pred,occupancy_label,avg_factor=avg_factor)
+        losses_seg = self.lidar_seg_loss(occupancy_pred, occupancy_label, avg_factor=avg_factor)
 
         # Lidar det loss
-        losses_det = self.get_det_loss(voxel_label_det,occupancy_det_label,occupancy_det_pred)
+        losses_det = self.get_det_loss(voxel_label_det, occupancy_det_label, occupancy_det_pred)
 
         # loss of proposal generated from encode feature map.
         if enc_cls_scores is not None:
@@ -586,7 +614,7 @@ class PanoSegOccHead(DETRHead):
             ]
             enc_loss_cls, enc_losses_bbox = \
                 self.loss_single(enc_cls_scores, enc_bbox_preds,
-                                 gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)
+                                 gt_bboxes_list, binary_labels_list)
             loss_dict['enc_loss_cls'] = enc_loss_cls
             loss_dict['enc_loss_bbox'] = enc_losses_bbox
 
@@ -599,187 +627,35 @@ class PanoSegOccHead(DETRHead):
 
         # loss from other decoder layers
         num_dec_layer = 0
-        for loss_cls_i, loss_bbox_i in zip(losses_cls[:-1],
-                                           losses_bbox[:-1]):
+        for loss_cls_i, loss_bbox_i in zip(losses_cls[:-1], losses_bbox[:-1]):
             loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i
             loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i
             num_dec_layer += 1
 
         return loss_dict
     
-    @force_fp32(apply_to=('preds_dicts'))
-    def loss_new(self,
-             gt_bboxes_list,
-             gt_labels_list,
-             pts_sem,
-             preds_dicts,
-             dense_occupancy = None,
-             gt_bboxes_ignore=None,
-             img_metas=None):
-        """"Loss function.
-        Args:
-
-            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image
-                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
-            gt_labels_list (list[Tensor]): Ground truth class indices for each
-                image with shape (num_gts, ).
-            preds_dicts:
-                all_cls_scores (Tensor): Classification score of all
-                    decoder layers, has shape
-                    [nb_dec, bs, num_query, cls_out_channels].
-                all_bbox_preds (Tensor): Sigmoid regression
-                    outputs of all decode layers. Each is a 4D-tensor with
-                    normalized coordinate format (cx, cy, w, h) and shape
-                    [nb_dec, bs, num_query, 4].
-                enc_cls_scores (Tensor): Classification scores of
-                    points on encode feature map , has shape
-                    (N, h*w, num_classes). Only be passed when as_two_stage is
-                    True, otherwise is None.
-                enc_bbox_preds (Tensor): Regression results of each points
-                    on the encode feature map, has shape (N, h*w, 4). Only be
-                    passed when as_two_stage is True, otherwise is None.
-            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes
-                which can be ignored for each image. Default None.
-        Returns:
-            dict[str, Tensor]: A dictionary of loss components.
+    def voxelize(self, points, pc_range, voxel_size, pts_semantic_mask=None):
         """
-        assert gt_bboxes_ignore is None, \
-            f'{self.__class__.__name__} only supports ' \
-            f'for gt_bboxes_ignore setting to None.'
-
-        # GT voxel supervision
-        pts = pts_sem[:,:3]
-        pts_semantic_mask = pts_sem[:,3:4]
-
-        pts_numpy = pts.cpu().numpy()
-        pts_semantic_mask_numpy = pts_semantic_mask.cpu().numpy()
-        points_grid_ind = np.floor((np.clip(pts_numpy, self.pc_range[:3],self.pc_range[3:]) - self.pc_range[:3]) / self.voxel_lidar).astype(np.int)
-        label_voxel_pair = np.concatenate([points_grid_ind, pts_semantic_mask_numpy], axis=1)
-        label_voxel_pair = label_voxel_pair[np.lexsort((points_grid_ind[:, 0], points_grid_ind[:, 1], points_grid_ind[:, 2])), :]
-        label_voxel = torch.tensor(label_voxel_pair).to(pts.device).long()
-        if dense_occupancy is None:
-            pts_coors,voxelized_data,voxel_coors = self.voxelize(pts,self.pc_range,self.voxel_lidar)
-            voxel_label = self.label_voxelization(pts_semantic_mask, pts_coors, voxel_coors)
-
-        pts_coors_det,voxelized_data_det,voxel_coors_det = self.voxelize(pts,self.pc_range,self.voxel_det)
-        voxel_label_det = self.label_voxelization(pts_semantic_mask, pts_coors_det, voxel_coors_det)
-
-        all_cls_scores = preds_dicts['all_cls_scores']
-        all_bbox_preds = preds_dicts['all_bbox_preds']
-        enc_cls_scores = preds_dicts['enc_cls_scores']
-        enc_bbox_preds = preds_dicts['enc_bbox_preds']
-        occupancy = preds_dicts['occupancy']
-        occupancy_det = preds_dicts['occupancy_det']
-
-        occupancy_pred = occupancy.squeeze(0)
-        occupancy_det_pred = occupancy_det.squeeze(0)
-
-        cls_num,occ_z,occ_h,occ_w = occupancy_pred.shape
-        if dense_occupancy is None:
-            occupancy_label = (torch.ones(1,occ_z,occ_h,occ_w)*cls_num).to(occupancy_pred.device).long()
-        else:
-            occupancy_label = (torch.zeros(1,occ_z,occ_h,occ_w)).to(occupancy_pred.device).long()
-        occupancy_det_label = (torch.ones(1,self.bev_z,self.bev_h,self.bev_w)*2).to(occupancy_det_pred.device).long()
-
-        # Matrix operation acceleration
-        if dense_occupancy is None:
-            occupancy_label[0,label_voxel[:,2],label_voxel[:,1],label_voxel[:,0]] = label_voxel[:,3]
-        else:
-            dense_occupancy = dense_occupancy.long().squeeze(0)
-            occupancy_label[0,dense_occupancy[:,0],dense_occupancy[:,1],dense_occupancy[:,2]]=dense_occupancy[:,3]
-
-        voxel_coors_det[:,1] = voxel_coors_det[:,1].clip(min=0,max=self.bev_z-1)
-        voxel_coors_det[:,2] = voxel_coors_det[:,2].clip(min=0,max=self.bev_h-1)
-        voxel_coors_det[:,3] = voxel_coors_det[:,3].clip(min=0,max=self.bev_w-1)
-
-        det_label_binary = ((voxel_label_det>=1)&(voxel_label_det<=10))
-        det_label = det_label_binary.long()
-        occupancy_det_label[0,voxel_coors_det[:,1],voxel_coors_det[:,2],voxel_coors_det[:,3]]=det_label
-
-        losses_seg_aux = self.lidar_seg_aux_loss(occupancy_pred.unsqueeze(0),occupancy_label)
-
-        occupancy_det_label = occupancy_det_label.reshape(-1)
-        occupancy_label = occupancy_label.reshape(-1)
-
-
-        assert occupancy_label.max()<=cls_num and occupancy_label.min()>=0
-        occupancy_pred = occupancy_pred.reshape(cls_num,-1).permute(1,0)
-        occupancy_det_pred = occupancy_det_pred.reshape(2,-1).permute(1,0)
-
-        num_dec_layers = len(all_cls_scores)
-        device = gt_labels_list[0].device
-
-        gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]),
-            dim=1).to(device) for gt_bboxes in gt_bboxes_list]
-
-        all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]
-        all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]
-        all_gt_bboxes_ignore_list = [
-            gt_bboxes_ignore for _ in range(num_dec_layers)
-        ]
-
-        losses_cls, losses_bbox = multi_apply(
-            self.loss_single, all_cls_scores, all_bbox_preds,
-            all_gt_bboxes_list, all_gt_labels_list,
-            all_gt_bboxes_ignore_list)
-
-        loss_dict = dict()
-
-        # Lidar seg loss
-        if dense_occupancy is None:
-            num_total_pos = len(voxel_label)
-        else:
-            num_total_pos = len(dense_occupancy)
-        num_total_neg = len(occupancy_label)-num_total_pos
-        avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_weight
-        if self.sync_cls_avg_factor:
-            avg_factor = reduce_mean(
-                occupancy_pred.new_tensor([avg_factor]))
-        avg_factor = max(avg_factor, 1)
-
-        losses_seg = self.lidar_seg_loss(occupancy_pred,occupancy_label,avg_factor=avg_factor)
-
-        # Lidar det loss
-        num_total_pos_det = len(voxel_label_det)
-
-
-        num_total_neg_det = len(occupancy_det_label)-num_total_pos_det
-        avg_factor_det = num_total_pos_det * 1.0 + num_total_neg_det * self.bg_weight
-        if self.sync_cls_avg_factor:
-            avg_factor_det = reduce_mean(
-                occupancy_det_pred.new_tensor([avg_factor_det]))
-        avg_factor_det = max(avg_factor_det, 1)
+        Input:
+            points [N, 3], (x, y, z)
+            point_cloud_range [6], [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], (-x, -y, -z, x, y, z)
+            voxelization_size [3], e.g. [0.256, 0.256, 0.125]
 
-        losses_det = self.lidar_det_loss(occupancy_det_pred,occupancy_det_label,avg_factor=avg_factor_det)
+        Output:
+            coors [N,4], (0, z, y, x)
+            unq_coors [M,4], (0, z, y, x)
 
-        # loss of proposal generated from encode feature map.
-        if enc_cls_scores is not None:
-            binary_labels_list = [
-                torch.zeros_like(gt_labels_list[i])
-                for i in range(len(all_gt_labels_list))
-            ]
-            enc_loss_cls, enc_losses_bbox = \
-                self.loss_single(enc_cls_scores, enc_bbox_preds,
-                                 gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)
-            loss_dict['enc_loss_cls'] = enc_loss_cls
-            loss_dict['enc_loss_bbox'] = enc_losses_bbox
+        """
 
-        # loss from the last decoder layer
-        loss_dict['loss_cls'] = losses_cls[-1]
-        loss_dict['loss_bbox'] = losses_bbox[-1]
-        loss_dict['loss_seg'] = losses_seg
-        loss_dict['loss_det'] = losses_det
-        loss_dict['loss_seg_aux'] = losses_seg_aux
+        coors = torch.div(points[:, :3] - pc_range[None, :3], voxel_size[None, :], rounding_mode='floor').to(torch.int32)
 
-        # loss from other decoder layers
-        num_dec_layer = 0
-        for loss_cls_i, loss_bbox_i in zip(losses_cls[:-1],
-                                           losses_bbox[:-1]):
-            loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i
-            loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i
-            num_dec_layer += 1
+        unq_coors, unq_inv = custom_unique_n3(coors, return_inverse=True, return_counts=False, dim=0)
 
-        return loss_dict
+        if pts_semantic_mask is not None:
+            with torch.no_grad():
+                voxel_label_my, _ = mx_driving.common.scatter_max(pts_semantic_mask, unq_inv.to(torch.int32))
+            return coors[:, [2, 1, 0]].long(), unq_coors.long(), voxel_label_my.squeeze(-1).long()
+        return coors[:, [2, 1, 0]].long(), unq_coors.long()
 
     @force_fp32(apply_to=('preds_dicts'))
     def get_bboxes(self, preds_dicts, img_metas, rescale=False):
@@ -810,190 +686,58 @@ class PanoSegOccHead(DETRHead):
 
         return ret_list
 
-    def decode_lidar_seg(self,points,occupancy):
+    def decode_lidar_seg(self, points, occupancy):
 
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
+        pts_coors, _ = self.voxelize(points, self.pc_range, self.voxel_lidar)
 
         # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-
-        # valid_mask = (pts_coors[:,1].cpu().numpy()>=0) & (pts_coors[:,1].cpu().numpy()<=z_max) \
-        #     & (pts_coors[:,2].cpu().numpy()>=0) & (pts_coors[:,2].cpu().numpy()<=y_max) \
-        #     & (pts_coors[:,3].cpu().numpy()>=0) & (pts_coors[:,3].cpu().numpy()<=x_max) 
+        z_max = int((self.pc_range[5] - self.pc_range[2]) / self.voxel_lidar[2]) - 1
+        y_max = int((self.pc_range[4] - self.pc_range[1]) / self.voxel_lidar[1]) - 1
+        x_max = int((self.pc_range[3] - self.pc_range[0]) / self.voxel_lidar[0]) - 1
         
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
-
-        pts_pred = occupancy[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
+        pts_coors[:, 0] = pts_coors[:, 0].clip(min=0, max=z_max)
+        pts_coors[:, 1] = pts_coors[:, 1].clip(min=0, max=y_max)
+        pts_coors[:, 2] = pts_coors[:, 2].clip(min=0, max=x_max)
 
-        # pts_pred[valid_mask==False]=15
+        pts_pred = occupancy[:, :, pts_coors[:, 0], pts_coors[:, 1], pts_coors[:, 2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
 
         return pts_pred
 
-    def voxelize(self, points,point_cloud_range,voxelization_size):
-        """
-        Input:
-            points
-
-        Output:
-            coors [N,4]
-            voxelized_data [M,3]
-            voxel_coors [M,4]
-
-        """
-        voxel_size = torch.tensor(voxelization_size, device=points.device)
-        pc_range = torch.tensor(point_cloud_range, device=points.device)
-        coors = torch.div(points[:, :3] - pc_range[None, :3], voxel_size[None, :], rounding_mode='floor').long()
-        coors = coors[:, [2, 1, 0]] # to zyx order
-
-        new_coors, unq_inv  = torch.unique(coors, return_inverse=True, return_counts=False, dim=0)
-
-        voxelized_data, voxel_coors = scatter_v2(points, coors, mode='avg', return_inv=False, new_coors=new_coors, unq_inv=unq_inv)
-
-        batch_idx_pts = torch.zeros(coors.size(0),1).to(device=points.device)
-        batch_idx_vox = torch.zeros(voxel_coors.size(0),1).to(device=points.device)
-
-        coors_batch = torch.cat([batch_idx_pts,coors],dim=1)
-        voxel_coors_batch = torch.cat([batch_idx_vox,voxel_coors],dim=1)
-
-        return coors_batch.long(),voxelized_data,voxel_coors_batch.long()
-
     def decode_lidar_seg_hr(self,points,occupancy):
 
         out_h = 512
         out_w = 512
         out_z = 160
 
-        self.voxel_lidar = [102.4/out_h,102.4/out_w,8/out_z]
+        self.voxel_lidar = [102.4/out_h, 102.4/out_w, 8/out_z]
 
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
+        pts_coors, _ = self.voxelize(points, self.pc_range, self.voxel_lidar)
 
         # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
+        z_max = int((self.pc_range[5] - self.pc_range[2]) / self.voxel_lidar[2]) - 1
+        y_max = int((self.pc_range[4] - self.pc_range[1]) / self.voxel_lidar[1]) - 1
+        x_max = int((self.pc_range[3] - self.pc_range[0]) / self.voxel_lidar[0]) - 1
+        pts_coors[:, 0] = pts_coors[:, 0].clip(min=0, max=z_max)
+        pts_coors[:, 1] = pts_coors[:, 1].clip(min=0, max=y_max)
+        pts_coors[:, 2] = pts_coors[:, 2].clip(min=0, max=x_max)
 
 
-        new_h = torch.linspace(-1, 1, out_h).view(1,out_h,1).expand(out_z,out_h,out_w)
-        new_w = torch.linspace(-1, 1, out_w).view(1,1,out_w).expand(out_z,out_h,out_w)
-        new_z = torch.linspace(-1, 1, out_z).view(out_z,1,1).expand(out_z,out_h,out_w)
+        new_h = torch.linspace(-1, 1, out_h).view(1, out_h, 1).expand(out_z, out_h, out_w)
+        new_w = torch.linspace(-1, 1, out_w).view(1, 1, out_w).expand(out_z, out_h, out_w)
+        new_z = torch.linspace(-1, 1, out_z).view(out_z, 1, 1).expand(out_z, out_h, out_w)
 
-        grid = torch.cat((new_w.unsqueeze(3),new_h.unsqueeze(3), new_z.unsqueeze(3)), dim=-1)
+        grid = torch.cat((new_w.unsqueeze(3), new_h.unsqueeze(3), new_z.unsqueeze(3)), dim=-1)
 
         grid = grid.unsqueeze(0).to(occupancy.device)
 
+        torch.npu.set_compile_mode(jit_compile=True)
         out_logit = F.grid_sample(occupancy, grid=grid)
+        torch.npu.set_compile_mode(jit_compile=False)
 
-        pts_pred = out_logit[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
+        pts_pred = out_logit[:, :, pts_coors[:, 0], pts_coors[:, 1], pts_coors[:, 2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
         return pts_pred
 
-    def decode_occupancy(self,points,occupancy):
-        out_h = 400
-        out_w = 400
-        out_z  = 64
-        self.voxel_lidar = [102.4/out_h,102.4/out_w,8/out_z]
-
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
-
-
-        # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
-
-
-        new_h = torch.linspace(-1, 1, out_h).view(1,out_h,1).expand(out_z,out_h,out_w)
-        new_w = torch.linspace(-1, 1, out_w).view(1,1,out_w).expand(out_z,out_h,out_w)
-        new_z = torch.linspace(-1, 1, out_z).view(out_z,1,1).expand(out_z,out_h,out_w)
-
-        grid = torch.cat((new_w.unsqueeze(3),new_h.unsqueeze(3), new_z.unsqueeze(3)), dim=-1)
-
-        grid = grid.unsqueeze(0).to(occupancy.device)
-
-        out_logit = F.grid_sample(occupancy, grid=grid)
-
-        # Occupancy Visualize
-        out_class = out_logit.sigmoid()>0.2
-        all_index = out_class.sum(dim=1).nonzero()
-
-        out_voxel = out_logit[:,:,all_index[:,1],all_index[:,2],all_index[:,3]]
-        out_voxel_scores = out_voxel.sigmoid()
-        out_voxel_confidence,out_voxel_labels = out_voxel_scores.max(dim=1)
-        output_occupancy = torch.cat((all_index.unsqueeze(0),out_voxel_labels.unsqueeze(-1)),dim=-1).cpu().numpy()[...,1:]
-
-        return output_occupancy
-
     def decode_lidar_seg_dense(self, dense, occupancy):
         dense  = dense.long()
-        pts_pred = occupancy[:,:,dense[0,:,0],dense[0,:,1],dense[0,:,2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
+        pts_pred = occupancy[:, :, dense[0, :, 0], dense[0, :, 1], dense[0, :, 2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
         return pts_pred
-
-    @torch.no_grad()
-    def label_voxelization(self, pts_semantic_mask, pts_coors, voxel_coors):
-        mask = pts_semantic_mask
-        assert mask.size(0) == pts_coors.size(0)
-
-        pts_coors_cls = torch.cat([pts_coors, mask], dim=1) #[N, 5]
-        unq_coors_cls, unq_inv, unq_cnt = torch.unique(pts_coors_cls, return_inverse=True, return_counts=True, dim=0) #[N1, 5], [N], [N1]
-
-        unq_coors, unq_inv_2, _ = torch.unique(unq_coors_cls[:, :4], return_inverse=True, return_counts=True, dim=0) #[N2, 4], [N1], [N2,]
-        max_num, max_inds = torch_scatter.scatter_max(unq_cnt.float()[:,None], unq_inv_2, dim=0) #[N2, 1], [N2, 1]
-
-        cls_of_max_num = unq_coors_cls[:, -1][max_inds.reshape(-1)] #[N2,]
-        cls_of_max_num_N1 = cls_of_max_num[unq_inv_2] #[N1]
-        cls_of_max_num_at_pts = cls_of_max_num_N1[unq_inv] #[N]
-
-        assert cls_of_max_num_at_pts.size(0) == mask.size(0)
-
-        cls_no_change = cls_of_max_num_at_pts == mask[:,0] # fix memory bug when scale up
-        # cls_no_change = cls_of_max_num_at_pts == mask
-        assert cls_no_change.any()
-
-        max_pts_coors = pts_coors.max(0)[0]
-        max_voxel_coors = voxel_coors.max(0)[0]
-        assert (max_voxel_coors <= max_pts_coors).all()
-        bsz, num_win_z, num_win_y, num_win_x = \
-        int(max_pts_coors[0].item() + 1), int(max_pts_coors[1].item() + 1), int(max_pts_coors[2].item() + 1), int(max_pts_coors[3].item() + 1)
-
-        canvas = -pts_coors.new_ones((bsz, num_win_z, num_win_y, num_win_x))
-
-        canvas[pts_coors[:, 0], pts_coors[:, 1], pts_coors[:, 2], pts_coors[:, 3]] = \
-            torch.arange(pts_coors.size(0), dtype=pts_coors.dtype, device=pts_coors.device)
-
-        fetch_inds_of_points = canvas[voxel_coors[:, 0], voxel_coors[:, 1], voxel_coors[:, 2], voxel_coors[:, 3]]
-
-        assert (fetch_inds_of_points >= 0).all(), '-1 should not be in it.'
-
-        voxel_label = cls_of_max_num_at_pts[fetch_inds_of_points]
-
-        voxel_label = torch.clamp(voxel_label,min=0).long()
-
-        return voxel_label
-
-    @torch.no_grad()
-    def get_point_pred(self,occupancy,pts_coors,voxel_coors,voxel_label,pts_semantic_mask):
-
-        voxel_pred = occupancy[:,:,voxel_coors[:,1],voxel_coors[:,2],voxel_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu()
-
-        voxel_gt = voxel_label.long().cpu()
-
-        accurate = voxel_pred==voxel_gt
-
-        acc = accurate.sum()/len(voxel_gt)
-
-        pts_pred = occupancy[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu()
-        pts_gt  = pts_semantic_mask.long().squeeze(1).cpu()
-
-        pts_accurate = pts_pred==pts_gt
-        pts_acc = pts_accurate.sum()/len(pts_gt)
-
-        return pts_acc
diff --git a/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_sparse_head.py b/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_sparse_head.py
deleted file mode 100644
index dd6602e..0000000
--- a/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_sparse_head.py
+++ /dev/null
@@ -1,763 +0,0 @@
-import copy
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from mmcv.cnn import Linear, bias_init_with_prob
-from mmcv.utils import TORCH_VERSION, digit_version
-
-from mmdet.core import (multi_apply, multi_apply, reduce_mean)
-from mmdet.models.utils.transformer import inverse_sigmoid
-from mmdet.models import HEADS
-from mmdet.models.dense_heads import DETRHead
-from mmdet3d.core.bbox.coders import build_bbox_coder
-from projects.mmdet3d_plugin.core.bbox.util import normalize_bbox
-from mmcv.cnn.bricks.transformer import build_positional_encoding
-from mmcv.runner import force_fp32, auto_fp16
-from projects.mmdet3d_plugin.models.utils.bricks import run_time
-import numpy as np
-import mmcv
-import cv2 as cv
-from projects.mmdet3d_plugin.models.utils.visual import save_tensor
-from mmdet3d.ops import scatter_v2
-import torch_scatter
-from mmdet.models.builder import build_loss
-from spconv.pytorch import SparseConvTensor 
-
-@HEADS.register_module()
-class SparseOccupancyHead(DETRHead):
-    """Head of Detr3D.
-    Args:
-        with_box_refine (bool): Whether to refine the reference points
-            in the decoder. Defaults to False.
-        as_two_stage (bool) : Whether to generate the proposal from
-            the outputs of encoder.
-        transformer (obj:`ConfigDict`): ConfigDict is used for building
-            the Encoder and Decoder.
-        bev_h, bev_w (int): spatial shape of BEV queries.
-    """
-
-    def __init__(self,
-                 *args,
-                 with_box_refine=False,
-                 as_two_stage=False,
-                 transformer=None,
-                 bbox_coder=None,
-                 num_cls_fcs=2,
-                 code_weights=None,
-                 bev_h=30,
-                 bev_w=30,
-                 bev_z=5,
-                 num_occ_classes=17,
-                 voxel_lidar = [0.05, 0.05, 0.05],
-                 voxel_det = [2.048,2.048,1],
-                 loss_occupancy=dict(
-                    type='FocalLoss',
-                    use_sigmoid=True,
-                    gamma=2.0,
-                    alpha=0.25,
-                    loss_weight=5.0),
-                loss_occupancy_layer0=None,
-                loss_occupancy_aux=None,
-                loss_occupancy_det=dict(
-                    type='FocalLoss',
-                    use_sigmoid=True,
-                    gamma=2.0,
-                    alpha=0.25,
-                    loss_weight=5.0),
-                bg_weight=0.02,
-                early_supervision_cfg=dict(),
-                 **kwargs):
-
-        self.bev_h = bev_h
-        self.bev_w = bev_w
-        self.bev_z = bev_z
-        self.voxel_lidar = voxel_lidar
-        self.voxel_det = voxel_det
-        self.fp16_enabled = False
-        self.bg_weight = bg_weight
-        self.num_occ_classes = num_occ_classes
-
-        self.with_box_refine = with_box_refine
-        self.as_two_stage = as_two_stage
-        if self.as_two_stage:
-            transformer['as_two_stage'] = self.as_two_stage
-        if 'code_size' in kwargs:
-            self.code_size = kwargs['code_size']
-        else:
-            self.code_size = 10
-        if code_weights is not None:
-            self.code_weights = code_weights
-        else:
-            self.code_weights = [1.0, 1.0, 1.0,
-                                 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
-
-        self.bbox_coder = build_bbox_coder(bbox_coder)
-        self.pc_range = self.bbox_coder.pc_range
-        self.real_w = self.pc_range[3] - self.pc_range[0]
-        self.real_h = self.pc_range[4] - self.pc_range[1]
-        self.span_x, self.span_y, self.span_z = self.real_w, self.real_h, self.pc_range[5] - self.pc_range[2]
-        self.num_cls_fcs = num_cls_fcs - 1
-        super(SparseOccupancyHead, self).__init__(
-            *args, transformer=transformer, **kwargs)
-        self.code_weights = nn.Parameter(torch.tensor(
-            self.code_weights, requires_grad=False), requires_grad=False)
-        self.lidar_seg_loss = build_loss(loss_occupancy)
-        self.early_supervision_cfg = early_supervision_cfg
-        self.build_early_loss()
-        # self.lidar_det_loss = build_loss(loss_occupancy_det)
-        if loss_occupancy_aux is not None:
-            self.lidar_seg_aux_loss = build_loss(loss_occupancy_aux)
-    
-    def build_early_loss(self,):
-        cfg = self.early_supervision_cfg
-        num = cfg.get('num_early_loss_layers', 1)
-        for i in range(num):
-            if cfg.get(f'layer{i}_loss', None) is not None:
-                setattr(self, f'occ_loss_layer{i}', build_loss(cfg[f'layer{i}_loss']))
-
-    def _init_layers(self):
-        """Initialize classification branch and regression branch of head."""
-
-        if not self.as_two_stage:
-            self.bev_embedding = nn.Embedding(
-                self.bev_h * self.bev_w * self.bev_z, self.embed_dims)
-
-    def init_weights(self):
-        """Initialize weights of the DeformDETR head."""
-        self.transformer.init_weights()
-
-    @auto_fp16(apply_to=('mlvl_feats'))
-    def forward(self, mlvl_feats, img_metas, prev_bev=None,  only_bev=False):
-        """Forward function.
-        Args:
-            mlvl_feats (tuple[Tensor]): Features from the upstream
-                network, each is a 5D-tensor with shape
-                (B, N, C, H, W).
-            prev_bev: previous bev featues
-            only_bev: only compute BEV features with encoder. 
-        Returns:
-            all_cls_scores (Tensor): Outputs from the classification head, \
-                shape [nb_dec, bs, num_query, cls_out_channels]. Note \
-                cls_out_channels should includes background.
-            all_bbox_preds (Tensor): Sigmoid outputs from the regression \
-                head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy). \
-                Shape [nb_dec, bs, num_query, 9].
-        """
-        bs, num_cam, _, _, _ = mlvl_feats[0].shape
-        dtype = mlvl_feats[0].dtype
-        bev_queries = self.bev_embedding.weight.to(dtype)
-
-        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w, self.bev_z),device=bev_queries.device).to(dtype)
-        bev_pos = self.positional_encoding(bev_mask).to(dtype)
-
-        if only_bev:
-
-            outputs = self.transformer(
-                mlvl_feats,
-                bev_queries,
-                self.bev_h,
-                self.bev_w,
-                self.bev_z,
-                grid_length=(self.real_h / self.bev_h,
-                                self.real_w / self.bev_w),
-                bev_pos=bev_pos,
-                reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
-                cls_branches=self.cls_branches if self.as_two_stage else None,
-                img_metas=img_metas,
-                prev_bev=prev_bev
-            )
-            bev_feat, occupancy = outputs
-            return bev_feat, bev_feat
-
-        else:
-            outputs = self.transformer(
-                mlvl_feats,
-                bev_queries,
-                self.bev_h,
-                self.bev_w,
-                self.bev_z,
-                grid_length=(self.real_h / self.bev_h,
-                                self.real_w / self.bev_w),
-                bev_pos=bev_pos,
-                reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
-                cls_branches=self.cls_branches if self.as_two_stage else None,
-                img_metas=img_metas,
-                prev_bev=prev_bev
-            )
-            bev_feat, occupancy = outputs
-            outs = {
-                'bev_embed': bev_feat,
-                'occupancy': occupancy[-1],
-                'early_occupancy': occupancy[:-1]
-            }
-
-        return outs
-
-    def _get_target_single(self,
-                           cls_score,
-                           bbox_pred,
-                           gt_labels,
-                           gt_bboxes,
-                           gt_bboxes_ignore=None):
-        """"Compute regression and classification targets for one image.
-        Outputs from a single decoder layer of a single feature level are used.
-        Args:
-            cls_score (Tensor): Box score logits from a single decoder layer
-                for one image. Shape [num_query, cls_out_channels].
-            bbox_pred (Tensor): Sigmoid outputs from a single decoder layer
-                for one image, with normalized coordinate (cx, cy, w, h) and
-                shape [num_query, 4].
-            gt_bboxes (Tensor): Ground truth bboxes for one image with
-                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
-            gt_labels (Tensor): Ground truth class indices for one image
-                with shape (num_gts, ).
-            gt_bboxes_ignore (Tensor, optional): Bounding boxes
-                which can be ignored. Default None.
-        Returns:
-            tuple[Tensor]: a tuple containing the following for one image.
-                - labels (Tensor): Labels of each image.
-                - label_weights (Tensor]): Label weights of each image.
-                - bbox_targets (Tensor): BBox targets of each image.
-                - bbox_weights (Tensor): BBox weights of each image.
-                - pos_inds (Tensor): Sampled positive indices for each image.
-                - neg_inds (Tensor): Sampled negative indices for each image.
-        """
-
-        num_bboxes = bbox_pred.size(0)
-        # assigner and sampler
-        gt_c = gt_bboxes.shape[-1]
-
-        assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes,
-                                             gt_labels, gt_bboxes_ignore)
-
-        sampling_result = self.sampler.sample(assign_result, bbox_pred,
-                                              gt_bboxes)
-        pos_inds = sampling_result.pos_inds
-        neg_inds = sampling_result.neg_inds
-
-        # label targets
-        labels = gt_bboxes.new_full((num_bboxes,),
-                                    self.num_classes,
-                                    dtype=torch.long)
-        labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]
-        label_weights = gt_bboxes.new_ones(num_bboxes)
-
-        # bbox targets
-        bbox_targets = torch.zeros_like(bbox_pred)[..., :gt_c]
-        bbox_weights = torch.zeros_like(bbox_pred)
-        bbox_weights[pos_inds] = 1.0
-
-        # DETR
-        bbox_targets[pos_inds] = sampling_result.pos_gt_bboxes
-        return (labels, label_weights, bbox_targets, bbox_weights,
-                pos_inds, neg_inds)
-
-    def get_targets(self,
-                    cls_scores_list,
-                    bbox_preds_list,
-                    gt_bboxes_list,
-                    gt_labels_list,
-                    gt_bboxes_ignore_list=None):
-        """"Compute regression and classification targets for a batch image.
-        Outputs from a single decoder layer of a single feature level are used.
-        Args:
-            cls_scores_list (list[Tensor]): Box score logits from a single
-                decoder layer for each image with shape [num_query,
-                cls_out_channels].
-            bbox_preds_list (list[Tensor]): Sigmoid outputs from a single
-                decoder layer for each image, with normalized coordinate
-                (cx, cy, w, h) and shape [num_query, 4].
-            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image
-                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
-            gt_labels_list (list[Tensor]): Ground truth class indices for each
-                image with shape (num_gts, ).
-            gt_bboxes_ignore_list (list[Tensor], optional): Bounding
-                boxes which can be ignored for each image. Default None.
-        Returns:
-            tuple: a tuple containing the following targets.
-                - labels_list (list[Tensor]): Labels for all images.
-                - label_weights_list (list[Tensor]): Label weights for all \
-                    images.
-                - bbox_targets_list (list[Tensor]): BBox targets for all \
-                    images.
-                - bbox_weights_list (list[Tensor]): BBox weights for all \
-                    images.
-                - num_total_pos (int): Number of positive samples in all \
-                    images.
-                - num_total_neg (int): Number of negative samples in all \
-                    images.
-        """
-        assert gt_bboxes_ignore_list is None, \
-            'Only supports for gt_bboxes_ignore setting to None.'
-        num_imgs = len(cls_scores_list)
-        gt_bboxes_ignore_list = [
-            gt_bboxes_ignore_list for _ in range(num_imgs)
-        ]
-
-        (labels_list, label_weights_list, bbox_targets_list,
-         bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(
-            self._get_target_single, cls_scores_list, bbox_preds_list,
-            gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)
-        num_total_pos = sum((inds.numel() for inds in pos_inds_list))
-        num_total_neg = sum((inds.numel() for inds in neg_inds_list))
-        return (labels_list, label_weights_list, bbox_targets_list,
-                bbox_weights_list, num_total_pos, num_total_neg)
-
-    def loss_single(self,
-                    cls_scores,
-                    bbox_preds,
-                    gt_bboxes_list,
-                    gt_labels_list,
-                    gt_bboxes_ignore_list=None):
-        """"Loss function for outputs from a single decoder layer of a single
-        feature level.
-        Args:
-            cls_scores (Tensor): Box score logits from a single decoder layer
-                for all images. Shape [bs, num_query, cls_out_channels].
-            bbox_preds (Tensor): Sigmoid outputs from a single decoder layer
-                for all images, with normalized coordinate (cx, cy, w, h) and
-                shape [bs, num_query, 4].
-            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image
-                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
-            gt_labels_list (list[Tensor]): Ground truth class indices for each
-                image with shape (num_gts, ).
-            gt_bboxes_ignore_list (list[Tensor], optional): Bounding
-                boxes which can be ignored for each image. Default None.
-        Returns:
-            dict[str, Tensor]: A dictionary of loss components for outputs from
-                a single decoder layer.
-        """
-        num_imgs = cls_scores.size(0)
-        cls_scores_list = [cls_scores[i] for i in range(num_imgs)]
-        bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]
-        cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list,
-                                           gt_bboxes_list, gt_labels_list,
-                                           gt_bboxes_ignore_list)
-        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,
-         num_total_pos, num_total_neg) = cls_reg_targets
-        labels = torch.cat(labels_list, 0)
-        label_weights = torch.cat(label_weights_list, 0)
-        bbox_targets = torch.cat(bbox_targets_list, 0)
-        bbox_weights = torch.cat(bbox_weights_list, 0)
-
-        # classification loss
-        cls_scores = cls_scores.reshape(-1, self.cls_out_channels)
-        # construct weighted avg_factor to match with the official DETR repo
-        cls_avg_factor = num_total_pos * 1.0 + \
-            num_total_neg * self.bg_cls_weight
-        if self.sync_cls_avg_factor:
-            cls_avg_factor = reduce_mean(
-                cls_scores.new_tensor([cls_avg_factor]))
-
-        cls_avg_factor = max(cls_avg_factor, 1)
-
-        loss_cls = self.loss_cls(
-            cls_scores, labels, label_weights, avg_factor=cls_avg_factor)
-
-        # Compute the average number of gt boxes accross all gpus, for
-        # normalization purposes
-        num_total_pos = loss_cls.new_tensor([num_total_pos])
-        num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()
-
-        # regression L1 loss
-        bbox_preds = bbox_preds.reshape(-1, bbox_preds.size(-1))
-        normalized_bbox_targets = normalize_bbox(bbox_targets, self.pc_range)
-        isnotnan = torch.isfinite(normalized_bbox_targets).all(dim=-1)
-        bbox_weights = bbox_weights * self.code_weights
-
-        loss_bbox = self.loss_bbox(
-            bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan,
-                                                               :10], bbox_weights[isnotnan, :10],
-            avg_factor=num_total_pos)
-        if digit_version(TORCH_VERSION) >= digit_version('1.8'):
-            loss_cls = torch.nan_to_num(loss_cls)
-            loss_bbox = torch.nan_to_num(loss_bbox)
-        return loss_cls, loss_bbox
-
-    @force_fp32(apply_to=('preds_dicts'))
-    def loss(self,
-             gt_bboxes_list,
-             gt_labels_list,
-             pts_sem,
-             preds_dicts,
-             dense_occupancy=None,
-             gt_bboxes_ignore=None,
-             img_metas=None):
-        """"Loss function.
-        Args:
-
-            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image
-                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
-            gt_labels_list (list[Tensor]): Ground truth class indices for each
-                image with shape (num_gts, ).
-            preds_dicts:
-                all_cls_scores (Tensor): Classification score of all
-                    decoder layers, has shape
-                    [nb_dec, bs, num_query, cls_out_channels].
-                all_bbox_preds (Tensor): Sigmoid regression
-                    outputs of all decode layers. Each is a 4D-tensor with
-                    normalized coordinate format (cx, cy, w, h) and shape
-                    [nb_dec, bs, num_query, 4].
-                enc_cls_scores (Tensor): Classification scores of
-                    points on encode feature map , has shape
-                    (N, h*w, num_classes). Only be passed when as_two_stage is
-                    True, otherwise is None.
-                enc_bbox_preds (Tensor): Regression results of each points
-                    on the encode feature map, has shape (N, h*w, 4). Only be
-                    passed when as_two_stage is True, otherwise is None.
-            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes
-                which can be ignored for each image. Default None.
-        Returns:
-            dict[str, Tensor]: A dictionary of loss components.
-        """
-        assert gt_bboxes_ignore is None, \
-            f'{self.__class__.__name__} only supports ' \
-            f'for gt_bboxes_ignore setting to None.'
-        
-        occupancy = preds_dicts['occupancy']
-
-        if isinstance(occupancy, SparseConvTensor):
-            return self.loss_sparse(preds_dicts, pts_sem)
-        else:
-            assert isinstance(occupancy, torch.Tensor)
-        
-        # GT voxel supervision
-        pts = pts_sem[:,:3]
-        pts_semantic_mask = pts_sem[:,3:4]
-        if dense_occupancy is None:
-            pts_coors,voxelized_data,voxel_coors = self.voxelize(pts, self.pc_range, self.voxel_lidar)
-            voxel_label = self.label_voxelization(pts_semantic_mask, pts_coors, voxel_coors)
-
-
-        occupancy_pred = occupancy.squeeze(0)
-        # occupancy_det_pred = occupancy_det.squeeze(0)
-
-        cls_num,occ_z,occ_h,occ_w = occupancy_pred.shape
-        occupancy_label = (torch.ones(1,occ_z,occ_h,occ_w)*cls_num).to(occupancy_pred.device).long()
-        
-        # Matrix operation acceleration
-        if dense_occupancy is None:
-            voxel_coors[:,1] = voxel_coors[:,1].clip(min=0,max=occ_z-1)
-            voxel_coors[:,2] = voxel_coors[:,2].clip(min=0,max=occ_h-1)
-            voxel_coors[:,3] = voxel_coors[:,3].clip(min=0,max=occ_w-1)
-            occupancy_label[0,voxel_coors[:,1],voxel_coors[:,2],voxel_coors[:,3]] = voxel_label
-        else:
-            dense_occupancy = dense_occupancy.long().squeeze(0)
-            occupancy_label[0,dense_occupancy[:,0],dense_occupancy[:,1],dense_occupancy[:,2]]=dense_occupancy[:,3]
-        
-        losses_seg_aux = self.lidar_seg_aux_loss(occupancy_pred.unsqueeze(0),occupancy_label)
-
-        # occupancy_det_label = occupancy_det_label.reshape(-1)
-        occupancy_label = occupancy_label.reshape(-1) 
-
-        assert occupancy_label.max()<=cls_num and occupancy_label.min()>=0
-        occupancy_pred = occupancy_pred.reshape(cls_num,-1).permute(1,0)
-
-        loss_dict = dict()
-        
-        # Lidar seg loss
-        if dense_occupancy is None:
-            num_total_pos = len(voxel_label)
-        else:
-            num_total_pos = len(dense_occupancy)
-        num_total_neg = len(occupancy_label)-num_total_pos
-        avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_weight
-        if self.sync_cls_avg_factor:
-            avg_factor = reduce_mean(
-                occupancy_pred.new_tensor([avg_factor]))
-        avg_factor = max(avg_factor, 1)
-
-        losses_seg = self.lidar_seg_loss(occupancy_pred, occupancy_label, avg_factor=avg_factor)
-
-        loss_dict['loss_seg'] = losses_seg
-        loss_dict['loss_seg_aux'] = losses_seg_aux
-
-        if self.early_supervision_cfg.get('layer0_loss', None) is not None:
-            occ_pred_0 = preds_dicts['early_occupancy'][0]
-            lidar_seg_loss_layer0 = self.get_layer0_loss(occ_pred_0, pts, pts_semantic_mask)
-            loss_dict['losss_occ_layer0'] = lidar_seg_loss_layer0
-        
-        return loss_dict
-    
-    def loss_sparse(self, preds_dicts, pts_sem):
-        pts = pts_sem[:,:3]
-        pts_semantic_mask = pts_sem[:,3:4]
-
-        occupancy = preds_dicts['occupancy']
-        early_occ = preds_dicts['early_occupancy']
-
-        dense_occ = early_occ[0]
-        sparse_occ = early_occ[1:] + [occupancy,]
-
-        loss_dict = {}
-        loss_layer0 = self.get_layer0_loss(dense_occ, pts, pts_semantic_mask)
-        loss_dict['loss_occ_layer0'] = loss_layer0
-
-        final = False
-        for i, occ in enumerate(sparse_occ):
-
-            if i == len(sparse_occ) - 1:
-                final = True
-
-            this_loss_dict = self.get_sparse_occ_loss(occ, pts, pts_semantic_mask, i+1, final)
-            loss_dict.update(this_loss_dict)
-        
-        return loss_dict
-    
-    def get_sparse_occ_loss(self, occ_sp, pts, pts_semantic_mask, index, final=False):
-        assert isinstance(occ_sp, SparseConvTensor)
-        occ = occ_sp.features
-        loss_dict = {}
-
-        occ_z, occ_h, occ_w = occ_sp.spatial_shape
-
-
-        vs_x = self.span_x / occ_w
-        vs_y = self.span_y / occ_h
-        vs_z = self.span_z / occ_z
-        voxel_size = (vs_x, vs_y, vs_z)
-
-        pts_coors, _, voxel_coors = self.voxelize(pts, self.pc_range, voxel_size)
-        voxel_label = self.label_voxelization(pts_semantic_mask, pts_coors, voxel_coors)
-        voxel_coors = self.clip_coors(voxel_coors, occ_z, occ_h, occ_w)
-
-        dense_label = torch.ones(1, occ_z, occ_h, occ_w).to(occ.device).long() * self.num_occ_classes
-        dense_label[0, voxel_coors[:,1], voxel_coors[:,2], voxel_coors[:,3]] = voxel_label
-
-        occ_coors = occ_sp.indices.long()
-
-        sparse_label = dense_label[occ_coors[:, 0], occ_coors[:, 1], occ_coors[:, 2], occ_coors[:, 3]]
-
-        if final:
-            loss_lovasz = self.lidar_seg_aux_loss(occ, sparse_label)
-            loss_dict[f'loss_sparse_lovasz_final'] = loss_lovasz
-
-        num_total_pos = (sparse_label < self.num_occ_classes).sum()
-        num_total_neg = len(sparse_label) - num_total_pos
-        avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_weight
-        if self.sync_cls_avg_factor:
-            avg_factor = reduce_mean(occ.new_tensor([avg_factor]))
-        avg_factor = max(avg_factor, 1)
-
-        if final:
-            loss_seg = self.lidar_seg_loss(occ, sparse_label, avg_factor=avg_factor)
-            loss_dict[f'loss_sparse_seg_final'] = loss_seg
-        else:
-            assert index > 0, 'first layer has dense loss, calculated outside'
-            loss_early = getattr(self, f'occ_loss_layer{index}')
-            if occ.shape[-1] == 1:
-                occ_label = (sparse_label == self.num_occ_classes).long()
-                loss_dict[f'loss_sparse_seg_{index}'] = loss_early(occ, occ_label, avg_factor=avg_factor)
-            else:
-                assert occ.shape[-1] == 17, 'For nus, it is fine to delete this assertion'
-                loss_dict[f'loss_sparse_seg_{index}'] = loss_early(occ, sparse_label, avg_factor=avg_factor)
-
-        return loss_dict
-    
-    def clip_coors(self, coors, z, h, w):
-        coors[:,1] = coors[:,1].clip(min=0, max=z-1)
-        coors[:,2] = coors[:,2].clip(min=0, max=h-1)
-        coors[:,3] = coors[:,3].clip(min=0, max=w-1)
-        return coors
-
-    
-    def get_layer0_loss(self, occupancy_pred, pts, pts_semantic_mask):
-
-        seg_loss = self.occ_loss_layer0
-
-        occupancy_pred = occupancy_pred.squeeze(0)
-        cls_num, occ_z, occ_h, occ_w = occupancy_pred.shape
-        assert cls_num == 1, 'occupied or not occupied'
-
-        vs_x = self.span_x / occ_w
-        vs_y = self.span_y / occ_h
-        vs_z = self.span_z / occ_z
-        voxel_size = (vs_x, vs_y, vs_z)
-
-        pts_coors, _, voxel_coors = self.voxelize(pts, self.pc_range, voxel_size)
-        # voxel_label = self.label_voxelization(pts_semantic_mask, pts_coors, voxel_coors)
-
-        # assert voxel_label.max().item() <= 16, 'A hard code num classes'
-
-        occupancy_label = torch.ones(1, occ_z, occ_h, occ_w).to(occupancy_pred.device).long()
-
-        voxel_coors[:,1] = voxel_coors[:,1].clip(min=0, max=occ_z-1)
-        voxel_coors[:,2] = voxel_coors[:,2].clip(min=0, max=occ_h-1)
-        voxel_coors[:,3] = voxel_coors[:,3].clip(min=0, max=occ_w-1)
-        # occupancy_label[0, voxel_coors[:,1], voxel_coors[:,2], voxel_coors[:,3]] = voxel_label
-        occupancy_label[0, voxel_coors[:,1], voxel_coors[:,2], voxel_coors[:,3]] = 0
-
-        occupancy_pred = occupancy_pred.reshape(cls_num, -1).permute(1,0)
-        occupancy_label = occupancy_label.reshape(-1)
-
-        num_total_pos = len(voxel_coors)
-        num_total_neg = len(occupancy_label)-num_total_pos
-        avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_weight
-        if self.sync_cls_avg_factor:
-            avg_factor = reduce_mean(
-                occupancy_pred.new_tensor([avg_factor]))
-        avg_factor = max(avg_factor, 1)
-
-        losses_seg = seg_loss(occupancy_pred, occupancy_label, avg_factor=avg_factor)
-        return losses_seg
-
-    @force_fp32(apply_to=('preds_dicts'))
-    def get_bboxes(self, preds_dicts, img_metas, rescale=False):
-        """Generate bboxes from bbox head predictions.
-        Args:
-            preds_dicts (tuple[list[dict]]): Prediction results.
-            img_metas (list[dict]): Point cloud and image's meta info.
-        Returns:
-            list[dict]: Decoded bbox, scores and labels after nms.
-        """
-
-        bboxes = torch.zeros(1, 7, dtype=torch.float32)
-        bboxes = img_metas[0]['box_type_3d'](bboxes, 7)
-        scores = torch.zeros(1, dtype=torch.float32)
-        labels = torch.zeros(1, dtype=torch.long)
-        ret_list = [[bboxes, scores, labels]]
-        return ret_list
-    
-    def decode_lidar_seg(self,points,occupancy):
-
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
-        
-        # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
-
-        if isinstance(occupancy, SparseConvTensor):
-            assert (z_max + 1, y_max + 1, x_max + 1) == tuple(occupancy.spatial_shape)
-            occupancy = occupancy.dense().squeeze(0)
-            padding_mask = (occupancy == 0).all(0)
-            occupancy[0, padding_mask] = 1 # regarding all empty positions as the first class
-            occupancy = occupancy[None, ...]
-
-        pts_pred = occupancy[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
-        return pts_pred
-    
-    def voxelize(self, points,point_cloud_range,voxelization_size):
-        """
-        Input:
-            points
-
-        Output:
-            coors [N,4]
-            voxelized_data [M,3]
-            voxel_coors [M,4]
-
-        """
-
-        voxel_size = torch.tensor(voxelization_size, device=points.device)
-        pc_range = torch.tensor(point_cloud_range, device=points.device)
-        coors = torch.div(points[:, :3] - pc_range[None, :3], voxel_size[None, :], rounding_mode='floor').long()
-        coors = coors[:, [2, 1, 0]] # to zyx order
-
-        new_coors, unq_inv  = torch.unique(coors, return_inverse=True, return_counts=False, dim=0)
-
-        voxelized_data, voxel_coors = scatter_v2(points, coors, mode='avg', return_inv=False, new_coors=new_coors, unq_inv=unq_inv)
-
-        batch_idx_pts = torch.zeros(coors.size(0),1).to(device=points.device)
-        batch_idx_vox = torch.zeros(voxel_coors.size(0),1).to(device=points.device)
-
-        coors_batch = torch.cat([batch_idx_pts,coors],dim=1)
-        voxel_coors_batch = torch.cat([batch_idx_vox,voxel_coors],dim=1)
-
-        return coors_batch.long(),voxelized_data,voxel_coors_batch.long()
-    
-    def decode_lidar_seg_hr(self,points,occupancy):
-
-        out_h = 512
-        out_w = 512
-        out_z = 160
-        
-        self.voxel_lidar = [102.4/out_h,102.4/out_w,8/out_z]
-
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
-        
-        # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
-
-
-        new_h = torch.linspace(-1, 1, out_h).view(1,out_h,1).expand(out_z,out_h,out_w)
-        new_w = torch.linspace(-1, 1, out_w).view(1,1,out_w).expand(out_z,out_h,out_w)
-        new_z = torch.linspace(-1, 1, out_z).view(out_z,1,1).expand(out_z,out_h,out_w)
-
-        grid = torch.cat((new_w.unsqueeze(3),new_h.unsqueeze(3), new_z.unsqueeze(3)), dim=-1)
-
-        grid = grid.unsqueeze(0).to(occupancy.device)
-
-        out_logit = F.grid_sample(occupancy, grid=grid)
-        
-        pts_pred = out_logit[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
-        return pts_pred
-    
-    @torch.no_grad()
-    def label_voxelization(self, pts_semantic_mask, pts_coors, voxel_coors):
-        mask = pts_semantic_mask
-        assert mask.size(0) == pts_coors.size(0)
-
-        pts_coors_cls = torch.cat([pts_coors, mask], dim=1) #[N, 5]
-        unq_coors_cls, unq_inv, unq_cnt = torch.unique(pts_coors_cls, return_inverse=True, return_counts=True, dim=0) #[N1, 5], [N], [N1]
-
-        unq_coors, unq_inv_2, _ = torch.unique(unq_coors_cls[:, :4], return_inverse=True, return_counts=True, dim=0) #[N2, 4], [N1], [N2,]
-        max_num, max_inds = torch_scatter.scatter_max(unq_cnt.float()[:,None], unq_inv_2, dim=0) #[N2, 1], [N2, 1]
-
-        cls_of_max_num = unq_coors_cls[:, -1][max_inds.reshape(-1)] #[N2,]
-        cls_of_max_num_N1 = cls_of_max_num[unq_inv_2] #[N1]
-        cls_of_max_num_at_pts = cls_of_max_num_N1[unq_inv] #[N]
-
-        assert cls_of_max_num_at_pts.size(0) == mask.size(0)
-
-        cls_no_change = cls_of_max_num_at_pts == mask[:,0] # fix memory bug when scale up
-        # cls_no_change = cls_of_max_num_at_pts == mask
-        assert cls_no_change.any()
-
-        max_pts_coors = pts_coors.max(0)[0]
-        max_voxel_coors = voxel_coors.max(0)[0]
-        assert (max_voxel_coors <= max_pts_coors).all()
-        bsz, num_win_z, num_win_y, num_win_x = \
-        int(max_pts_coors[0].item() + 1), int(max_pts_coors[1].item() + 1), int(max_pts_coors[2].item() + 1), int(max_pts_coors[3].item() + 1)
-
-        canvas = -pts_coors.new_ones((bsz, num_win_z, num_win_y, num_win_x))
-
-        canvas[pts_coors[:, 0], pts_coors[:, 1], pts_coors[:, 2], pts_coors[:, 3]] = \
-            torch.arange(pts_coors.size(0), dtype=pts_coors.dtype, device=pts_coors.device)
-
-        fetch_inds_of_points = canvas[voxel_coors[:, 0], voxel_coors[:, 1], voxel_coors[:, 2], voxel_coors[:, 3]]
-
-        assert (fetch_inds_of_points >= 0).all(), '-1 should not be in it.'
-
-        voxel_label = cls_of_max_num_at_pts[fetch_inds_of_points]
-
-        voxel_label = torch.clamp(voxel_label,min=0).long()
-
-        return voxel_label
-    
-    @torch.no_grad()
-    def get_point_pred(self,occupancy,pts_coors,voxel_coors,voxel_label,pts_semantic_mask):
-        
-        voxel_pred = occupancy[:,:,voxel_coors[:,1],voxel_coors[:,2],voxel_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu()
-
-        voxel_gt = voxel_label.long().cpu()
-
-        accurate = voxel_pred==voxel_gt
-
-        acc = accurate.sum()/len(voxel_gt)
-
-        pts_pred = occupancy[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu()
-        pts_gt  = pts_semantic_mask.long().squeeze(1).cpu()
-
-        pts_accurate = pts_pred==pts_gt
-        pts_acc = pts_accurate.sum()/len(pts_gt)
-
-        return pts_acc
diff --git a/projects/mmdet3d_plugin/bevformer/detectors/__init__.py b/projects/mmdet3d_plugin/bevformer/detectors/__init__.py
index 1012ef3..bf7f763 100644
--- a/projects/mmdet3d_plugin/bevformer/detectors/__init__.py
+++ b/projects/mmdet3d_plugin/bevformer/detectors/__init__.py
@@ -1,3 +1,2 @@
 from .pano_occ import PanoOcc
-from .panoseg_occ import PanoSegOcc
-from .panoseg_occ_sparse import PanoSegOccSparse
\ No newline at end of file
+from .panoseg_occ import PanoSegOcc
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py b/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py
index 46a8b99..b8d36ee 100644
--- a/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py
+++ b/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py
@@ -1,3 +1,9 @@
+# ---------------------------------------------
+# Copyright (c) OpenMMLab. All rights reserved.
+# ---------------------------------------------
+#  Modified by Zhiqi Li
+# ---------------------------------------------
+
 import torch
 from mmcv.runner import force_fp32, auto_fp16
 from mmdet.models import DETECTORS
diff --git a/projects/mmdet3d_plugin/bevformer/detectors/panoseg_occ.py b/projects/mmdet3d_plugin/bevformer/detectors/panoseg_occ.py
index 92e74bc..46fd950 100644
--- a/projects/mmdet3d_plugin/bevformer/detectors/panoseg_occ.py
+++ b/projects/mmdet3d_plugin/bevformer/detectors/panoseg_occ.py
@@ -1,3 +1,10 @@
+# ---------------------------------------------
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+# ---------------------------------------------
+#  Modified by Zhiqi Li
+# ---------------------------------------------
+
 import torch
 from mmcv.runner import force_fp32, auto_fp16
 from mmdet.models import DETECTORS
@@ -69,25 +76,27 @@ class PanoSegOcc(MVXTwoStageDetector):
             'prev_angle': 0,
         }
 
-
     def extract_img_feat(self, img, img_metas, len_queue=None):
         """Extract features of images."""
-        B = img.size(0)
-        if img is not None:
-
-            if img.dim() == 5 and img.size(0) == 1:
-                img.squeeze_()
-            elif img.dim() == 5 and img.size(0) > 1:
-                B, N, C, H, W = img.size()
-                img = img.reshape(B * N, C, H, W)
-            if self.use_grid_mask:
-                img = self.grid_mask(img)
-
-            img_feats = self.img_backbone(img)
-            if isinstance(img_feats, dict):
-                img_feats = list(img_feats.values())
-        else:
+
+        if img is None:
             return None
+        
+        B = img.size(0)
+        
+        if img.dim() == 5 and img.size(0) == 1:
+            img.squeeze_()
+        elif img.dim() == 5 and img.size(0) > 1:
+            B, N, C, H, W = img.size()
+            img = img.reshape(B * N, C, H, W)
+            
+        if self.use_grid_mask:
+            img = self.grid_mask(img)
+
+        img_feats = self.img_backbone(img)
+        if isinstance(img_feats, dict):
+            img_feats = list(img_feats.values())
+        
         if self.with_img_neck:
             img_feats = self.img_neck(img_feats)
 
@@ -95,9 +104,9 @@ class PanoSegOcc(MVXTwoStageDetector):
         for img_feat in img_feats:
             BN, C, H, W = img_feat.size()
             if len_queue is not None:
-                img_feats_reshaped.append(img_feat.view(int(B/len_queue), len_queue, int(BN / B), C, H, W))
+                img_feats_reshaped.append(img_feat.view(int(B/len_queue), len_queue, int(BN/B), C, H, W))
             else:
-                img_feats_reshaped.append(img_feat.view(B, int(BN / B), C, H, W))
+                img_feats_reshaped.append(img_feat.view(B, int(BN/B), C, H, W))
         return img_feats_reshaped
 
     @auto_fp16(apply_to=('img'))
@@ -105,10 +114,8 @@ class PanoSegOcc(MVXTwoStageDetector):
         """Extract features from images and points."""
 
         img_feats = self.extract_img_feat(img, img_metas, len_queue=len_queue)
-
         return img_feats
 
-
     def forward_pts_train(self,
                           pts_feats,
                           gt_bboxes_3d,
@@ -142,10 +149,6 @@ class PanoSegOcc(MVXTwoStageDetector):
         losses = self.pts_bbox_head.loss(*loss_inputs, img_metas=img_metas)
         return losses
 
-    def forward_dummy(self, img):
-        dummy_metas = None
-        return self.forward_test(img=img, img_metas=[[dummy_metas]])
-
     def forward(self, return_loss=True, **kwargs):
         """Calls either forward_train or forward_test depending on whether
         return_loss=True.
@@ -171,6 +174,7 @@ class PanoSegOcc(MVXTwoStageDetector):
             bs, len_queue, num_cams, C, H, W = imgs_queue.shape
             imgs_queue = imgs_queue.reshape(bs*len_queue, num_cams, C, H, W)
             img_feats_list = self.extract_feat(img=imgs_queue, len_queue=len_queue)
+            
             for i in range(len_queue):
                 img_metas = [each[i] for each in img_metas_list]
                 img_feats = [each_scale[:, i] for each_scale in img_feats_list]
@@ -189,6 +193,7 @@ class PanoSegOcc(MVXTwoStageDetector):
                         img_metas[0]["ego2global_transform_lst"] = img_metas_list[0][len_queue]["ego2global_transform_lst"][:i+1]
                     else:
                         prev_bev = None
+                
                 bev_feat, temporal_fused_bev_feat = self.pts_bbox_head(img_feats, img_metas, prev_bev=prev_bev, only_bev=True)
                 if self.temporal_fuse_type == "rnn":
                     prev_bev = temporal_fused_bev_feat
@@ -197,6 +202,7 @@ class PanoSegOcc(MVXTwoStageDetector):
                 prev_bev = prev_bev.permute(0, 2, 1)
                 prev_bev = prev_bev.reshape(prev_bev.shape[0], 1, -1, self.pts_bbox_head.bev_h, self.pts_bbox_head.bev_w, self.pts_bbox_head.bev_z)
                 prev_bev_lst.append(prev_bev)
+            
             self.train()
             # (bs, embed_dims, H, W)
             return prev_bev_lst
@@ -248,7 +254,7 @@ class PanoSegOcc(MVXTwoStageDetector):
         pts_sem = pts_semantic_mask[-1]
 
         # prev frame = 0, no temporal
-        if prev_img.size(1)==0:
+        if prev_img.size(1) == 0:
             prev_bev = None
         else:
             prev_img_metas = copy.deepcopy(img_metas)
@@ -271,6 +277,7 @@ class PanoSegOcc(MVXTwoStageDetector):
         if not img_metas[0]['prev_bev_exists']:
             prev_bev = None
         img_feats = self.extract_feat(img=img, img_metas=img_metas)
+
         losses = dict()
         if not self.DENSE_LABEL:
             losses_pts = self.forward_pts_train(img_feats, gt_bboxes_3d,
@@ -401,6 +408,3 @@ class PanoSegOcc(MVXTwoStageDetector):
         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):
             result_dict['pts_bbox'] = pts_bbox
         return prev_bev_feat, fused_prev_bev_feat, bbox_list, lidar_seg
-
-
-
diff --git a/projects/mmdet3d_plugin/bevformer/detectors/panoseg_occ_sparse.py b/projects/mmdet3d_plugin/bevformer/detectors/panoseg_occ_sparse.py
deleted file mode 100644
index 564a480..0000000
--- a/projects/mmdet3d_plugin/bevformer/detectors/panoseg_occ_sparse.py
+++ /dev/null
@@ -1,390 +0,0 @@
-# ---------------------------------------------
-# Copyright (c) OpenMMLab. All rights reserved.
-# ---------------------------------------------
-#  Modified by Zhiqi Li
-# ---------------------------------------------
-
-import torch
-from mmcv.runner import force_fp32, auto_fp16
-from mmdet.models import DETECTORS
-from mmdet3d.core import bbox3d2result
-from mmdet3d.models.detectors.mvx_two_stage import MVXTwoStageDetector
-from projects.mmdet3d_plugin.models.utils.grid_mask import GridMask
-import time
-import copy
-import numpy as np
-import mmdet3d
-from projects.mmdet3d_plugin.models.utils.bricks import run_time
-
-@DETECTORS.register_module()
-class PanoSegOccSparse(MVXTwoStageDetector):
-    def __init__(self,
-                 use_grid_mask=False,
-                 pts_voxel_layer=None,
-                 pts_voxel_encoder=None,
-                 pts_middle_encoder=None,
-                 pts_fusion_layer=None,
-                 img_backbone=None,
-                 pts_backbone=None,
-                 img_neck=None,
-                 pts_neck=None,
-                 pts_bbox_head=None,
-                 img_roi_head=None,
-                 img_rpn_head=None,
-                 train_cfg=None,
-                 test_cfg=None,
-                 pretrained=None,
-                 video_test_mode=False,
-                 time_interval=1,
-                 temporal_fuse_type="rnn",
-                 HR_TEST = False,
-                 DENSE_LABEL =False,
-                 ):
-
-        super(PanoSegOccSparse,
-              self).__init__(pts_voxel_layer, pts_voxel_encoder,
-                             pts_middle_encoder, pts_fusion_layer,
-                             img_backbone, pts_backbone, img_neck, pts_neck,
-                             pts_bbox_head, img_roi_head, img_rpn_head,
-                             train_cfg, test_cfg, pretrained)
-        self.grid_mask = GridMask(
-            True, True, rotate=1, offset=False, ratio=0.5, mode=1, prob=0.7)
-        self.use_grid_mask = use_grid_mask
-        self.fp16_enabled = False
-        self.time_interval = time_interval
-        self.temporal_fuse_type = temporal_fuse_type
-        self.HR_TEST = HR_TEST
-        self.DENSE_LABEL = DENSE_LABEL
-
-        # temporal
-        self.video_test_mode = video_test_mode
-        self.prev_frame_info = {
-            'prev_bev': [],
-            "ego2global_transform_lst": [],
-            'scene_token': None,
-            'prev_pos': 0,
-            'prev_angle': 0,
-        }
-
-
-    def extract_img_feat(self, img, img_metas, len_queue=None):
-        """Extract features of images."""
-        B = img.size(0)
-        if img is not None:
-
-            # input_shape = img.shape[-2:]
-            # # update real input shape of each single img
-            # for img_meta in img_metas:
-            #     img_meta.update(input_shape=input_shape)
-
-            if img.dim() == 5 and img.size(0) == 1:
-                img.squeeze_()
-            elif img.dim() == 5 and img.size(0) > 1:
-                B, N, C, H, W = img.size()
-                img = img.reshape(B * N, C, H, W)
-            if self.use_grid_mask:
-                img = self.grid_mask(img)
-
-            img_feats = self.img_backbone(img)
-            if isinstance(img_feats, dict):
-                img_feats = list(img_feats.values())
-        else:
-            return None
-        if self.with_img_neck:
-            img_feats = self.img_neck(img_feats)
-
-        img_feats_reshaped = []
-        for img_feat in img_feats:
-            BN, C, H, W = img_feat.size()
-            if len_queue is not None:
-                img_feats_reshaped.append(img_feat.view(int(B/len_queue), len_queue, int(BN / B), C, H, W))
-            else:
-                img_feats_reshaped.append(img_feat.view(B, int(BN / B), C, H, W))
-        return img_feats_reshaped
-
-    @auto_fp16(apply_to=('img'))
-    def extract_feat(self, img, img_metas=None, len_queue=None):
-        """Extract features from images and points."""
-
-        img_feats = self.extract_img_feat(img, img_metas, len_queue=len_queue)
-
-        return img_feats
-
-
-    def forward_pts_train(self,
-                          pts_feats,
-                          gt_bboxes_3d,
-                          gt_labels_3d,
-                          pts_sem,
-                          img_metas,
-                          gt_bboxes_ignore=None,
-                          prev_bev=None,
-                          dense_occupancy=None):
-        """Forward function'
-        Args:
-            pts_feats (list[torch.Tensor]): Features of point cloud branch
-            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`]): Ground truth
-                boxes for each sample.
-            gt_labels_3d (list[torch.Tensor]): Ground truth labels for
-                boxes of each sampole
-            img_metas (list[dict]): Meta information of samples.
-            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth
-                boxes to be ignored. Defaults to None.
-            prev_bev (torch.Tensor, optional): BEV features of previous frame.
-        Returns:
-            dict: Losses of each branch.
-        """
-        outs = self.pts_bbox_head(pts_feats, img_metas, prev_bev)
-        if self.DENSE_LABEL:
-            loss_inputs = [gt_bboxes_3d, gt_labels_3d, pts_sem, outs, dense_occupancy]
-        else:
-            loss_inputs = [gt_bboxes_3d, gt_labels_3d, pts_sem, outs]
-        losses = self.pts_bbox_head.loss(*loss_inputs, img_metas=img_metas)
-        return losses
-
-    def forward_dummy(self, img):
-        dummy_metas = None
-        return self.forward_test(img=img, img_metas=[[dummy_metas]])
-
-    def forward(self, return_loss=True, **kwargs):
-        """Calls either forward_train or forward_test depending on whether
-        return_loss=True.
-        Note this setting will change the expected inputs. When
-        `return_loss=True`, img and img_metas are single-nested (i.e.
-        torch.Tensor and list[dict]), and when `resturn_loss=False`, img and
-        img_metas should be double nested (i.e.  list[torch.Tensor],
-        list[list[dict]]), with the outer list indicating test time
-        augmentations.
-        """
-        if return_loss:
-            return self.forward_train(**kwargs)
-        else:
-            return self.forward_test(**kwargs)
-
-    def obtain_history_bev(self, imgs_queue, img_metas_list):
-        """Obtain history BEV features iteratively. To save GPU memory, gradients are not calculated.
-        """
-        self.eval()
-
-        prev_bev_lst = []
-        with torch.no_grad():
-            bs, len_queue, num_cams, C, H, W = imgs_queue.shape
-            imgs_queue = imgs_queue.reshape(bs*len_queue, num_cams, C, H, W)
-            img_feats_list = self.extract_feat(img=imgs_queue, len_queue=len_queue)
-            for i in range(len_queue):
-                img_metas = [each[i] for each in img_metas_list]
-                img_feats = [each_scale[:, i] for each_scale in img_feats_list]
-
-                if self.temporal_fuse_type == "rnn":
-                    if len(prev_bev_lst) > 0:
-                        prev_bev = prev_bev_lst[-1]
-                        # prev frame and current frame ego2global transformation
-                        img_metas[0]["ego2global_transform_lst"] = img_metas_list[0][len_queue]["ego2global_transform_lst"][i-1:i+1]
-                    else:
-                        prev_bev = None
-                elif self.temporal_fuse_type == "concat":
-                    if len(prev_bev_lst) > 0:
-                        prev_bev = torch.cat(prev_bev_lst, dim=1)
-                        # all prev frame ego2global transformation
-                        img_metas[0]["ego2global_transform_lst"] = img_metas_list[0][len_queue]["ego2global_transform_lst"][:i+1]
-                    else:
-                        prev_bev = None
-                bev_feat, temporal_fused_bev_feat = self.pts_bbox_head(img_feats, img_metas, prev_bev=prev_bev, only_bev=True)
-                if self.temporal_fuse_type == "rnn":
-                    prev_bev = temporal_fused_bev_feat
-                elif self.temporal_fuse_type == "concat":
-                    prev_bev = bev_feat
-                prev_bev = prev_bev.permute(0, 2, 1)
-                prev_bev = prev_bev.reshape(prev_bev.shape[0], 1, -1, self.pts_bbox_head.bev_h, self.pts_bbox_head.bev_w, self.pts_bbox_head.bev_z)
-                prev_bev_lst.append(prev_bev)
-            self.train()
-        # (bs, embed_dims, H, W)
-        return prev_bev_lst
-
-    @auto_fp16(apply_to=('img', 'points'))
-    def forward_train(self,
-                      points=None,
-                      img_metas=None,
-                      gt_bboxes_3d=None,
-                      gt_labels_3d=None,
-                      gt_labels=None,
-                      gt_bboxes=None,
-                      img=None,
-                      proposals=None,
-                      gt_bboxes_ignore=None,
-                      img_depth=None,
-                      img_mask=None,
-                      pts_semantic_mask= None,
-                      dense_occupancy = None, 
-                      ):
-        """Forward training function.
-        Args:
-            points (list[torch.Tensor], optional): Points of each sample.
-                Defaults to None.
-            img_metas (list[dict], optional): Meta information of each sample.
-                Defaults to None.
-            gt_bboxes_3d (list[:obj:`BaseInstance3DBoxes`], optional):
-                Ground truth 3D boxes. Defaults to None.
-            gt_labels_3d (list[torch.Tensor], optional): Ground truth labels
-                of 3D boxes. Defaults to None.
-            gt_labels (list[torch.Tensor], optional): Ground truth labels
-                of 2D boxes in images. Defaults to None.
-            gt_bboxes (list[torch.Tensor], optional): Ground truth 2D boxes in
-                images. Defaults to None.
-            img (torch.Tensor optional): Images of each sample with shape
-                (N, C, H, W). Defaults to None.
-            proposals ([list[torch.Tensor], optional): Predicted proposals
-                used for training Fast RCNN. Defaults to None.
-            gt_bboxes_ignore (list[torch.Tensor], optional): Ground truth
-                2D boxes in images to be ignored. Defaults to None.
-        Returns:
-            dict: Losses of different branches.
-        """
-        len_queue = img.size(1)
-        prev_img = img[:, :-1, ...]
-        img = img[:, -1, ...]
-
-        # Load Lidar semantic seg
-        pts_sem = pts_semantic_mask[-1]
-
-        # prev frame = 0, no temporal
-        if prev_img.size(1)==0:
-            prev_bev = None
-        else:
-            prev_img_metas = copy.deepcopy(img_metas)
-            prev_bev = self.obtain_history_bev(prev_img, prev_img_metas)
-        
-        if self.temporal_fuse_type == "rnn":
-            if prev_bev is not None and len(prev_bev) > 0:
-                prev_bev = prev_bev[-1]
-                # prev frame and current frame ego2global transformation
-                img_metas[0][len_queue-1]["ego2global_transform_lst"] = img_metas[0][len_queue-1]["ego2global_transform_lst"][-2:]
-            else:
-                prev_bev = None
-
-        img_metas = [each[len_queue-1] for each in img_metas]
-        if not img_metas[0]['prev_bev_exists']:
-            prev_bev = None
-        img_feats = self.extract_feat(img=img, img_metas=img_metas)
-        losses = dict()
-        if not self.DENSE_LABEL:
-            losses_pts = self.forward_pts_train(img_feats, gt_bboxes_3d,
-                                            gt_labels_3d, pts_sem, img_metas,
-                                            gt_bboxes_ignore, prev_bev)
-        else:
-            assert dense_occupancy is not None
-            losses_pts = self.forward_pts_train(img_feats, gt_bboxes_3d,
-                                            gt_labels_3d, pts_sem, img_metas,
-                                            gt_bboxes_ignore, prev_bev, dense_occupancy)
-
-        losses.update(losses_pts)
-        return losses
-
-    def forward_test(self, img_metas, img=None, points=None,pts_semantic_mask=None, **kwargs):
-        for var, name in [(img_metas, 'img_metas')]:
-            if not isinstance(var, list):
-                raise TypeError('{} must be a list, but got {}'.format(
-                    name, type(var)))
-        img = [img] if img is None else img
-
-        if img_metas[0][0]['scene_token'] != self.prev_frame_info['scene_token']:
-            # the first sample of each scene is truncated
-            self.prev_frame_info['prev_bev'] = []
-            self.prev_frame_info["ego2global_transformation_lst"] = []
-        # update idx
-        self.prev_frame_info['scene_token'] = img_metas[0][0]['scene_token']
-
-        # do not use temporal information
-        if not self.video_test_mode:
-            self.prev_frame_info['prev_bev'] = []
-            self.prev_frame_info["ego2global_transformation_lst"] = []
-
-        # Get the delta of ego position and angle between two timestamps.
-        tmp_pos = copy.deepcopy(img_metas[0][0]['can_bus'][:3])
-        tmp_angle = copy.deepcopy(img_metas[0][0]['can_bus'][-1])
-        if self.prev_frame_info['prev_bev'] is not None:
-            img_metas[0][0]['can_bus'][:3] -= self.prev_frame_info['prev_pos']
-            img_metas[0][0]['can_bus'][-1] -= self.prev_frame_info['prev_angle']
-        else:
-            img_metas[0][0]['can_bus'][-1] = 0
-            img_metas[0][0]['can_bus'][:3] = 0
-        if points is not None:
-            points = points[0]
-        if pts_semantic_mask is not None:
-            pts_semantic_mask = pts_semantic_mask[0][0].long().cpu().numpy()
-            pts_semantic_mask = pts_semantic_mask[:,-1]
-
-        self.prev_frame_info["ego2global_transformation_lst"].append(img_metas[0][0]["ego2global_transformation"])
-
-        img_metas[0][0]["ego2global_transform_lst"] = self.prev_frame_info["ego2global_transformation_lst"][-1::-self.time_interval][::-1]
-
-        if self.temporal_fuse_type == "concat":
-            prev_bev = torch.cat(self.prev_frame_info["prev_bev"][-self.time_interval::-self.time_interval][::-1], dim=1) if len(self.prev_frame_info["prev_bev"]) > 0 else None
-        elif self.temporal_fuse_type == "rnn":
-            prev_bev = self.prev_frame_info["prev_bev"][-1] if len(self.prev_frame_info["prev_bev"]) > 0 else None
-
-        fused_prev_bev_feat, bbox_results, lidar_seg = self.simple_test(
-                img_metas[0], img[0], points = points[0],
-                prev_bev=prev_bev,
-                **kwargs)
-        
-
-        if self.temporal_fuse_type == "concat":
-            raise NotImplementedError
-        elif self.temporal_fuse_type == "rnn":
-            prev_bev = fused_prev_bev_feat
-
-        # prev_bev = self.prev_frame_info['prev_bev'][-self.time_interval:: -self.time_interval][:: -1]
-        # prev_bev = torch.stack(prev_bev, dim=1) if len(prev_bev) > 0 else None
-        # new_prev_bev, bbox_results, lidar_seg = self.simple_test(
-        #     img_metas[0], img[0], points = points[0],
-        #     prev_bev=prev_bev,
-        #     **kwargs)
-
-        # During inference, we save the BEV features and ego motion of each timestamp.
-        self.prev_frame_info['prev_pos'] = tmp_pos
-        self.prev_frame_info['prev_angle'] = tmp_angle
-        # (bs, H*W*Z, embed_dims) ->
-        # (bs, num_queue, embed_dims, H, W, Z)
-        prev_bev = prev_bev.permute(0, 2, 1).reshape(1, 1,-1, self.pts_bbox_head.bev_h, self.pts_bbox_head.bev_w, self.pts_bbox_head.bev_z)
-        self.prev_frame_info['prev_bev'].append(prev_bev)
-
-        while len(self.prev_frame_info["prev_bev"]) >= self.pts_bbox_head.transformer.temporal_encoder.num_bev_queue * self.time_interval:
-            self.prev_frame_info["prev_bev"].pop(0)
-            self.prev_frame_info["ego2global_transformation_lst"].pop(0)
-
-        lidar_results = dict(token= img_metas[0][0]['sample_idx'],lidar_pred = lidar_seg, lidar_label = pts_semantic_mask)
-
-        return bbox_results, lidar_results
-
-    def simple_test_pts(self, x, img_metas, points=None, prev_bev=None, rescale=False):
-        """Test function"""
-        outs = self.pts_bbox_head(x, img_metas, prev_bev=prev_bev)
-
-        bbox_list = self.pts_bbox_head.get_bboxes(outs, img_metas, rescale=rescale)
-        
-        if self.HR_TEST:
-            lidar_seg = self.pts_bbox_head.decode_lidar_seg_hr(points,outs['occupancy'])
-        else:
-            lidar_seg = self.pts_bbox_head.decode_lidar_seg(points,outs['occupancy'])
-
-        bbox_results = [
-            bbox3d2result(bboxes, scores, labels)
-            for bboxes, scores, labels in bbox_list
-        ]
-        return outs['bev_embed'], bbox_results, lidar_seg
-
-    def simple_test(self, img_metas, img=None, points=None, prev_bev=None, rescale=False):
-        """Test function without augmentaiton."""
-        
-        img_feats = self.extract_feat(img=img, img_metas=img_metas)
-
-        bbox_list = [dict() for i in range(len(img_metas))]
-        
-        new_prev_bev, bbox_pts, lidar_seg = self.simple_test_pts(img_feats, img_metas, points, prev_bev, rescale=rescale)
-        for result_dict, pts_bbox in zip(bbox_list, bbox_pts):
-            result_dict['pts_bbox'] = pts_bbox
-        return new_prev_bev, bbox_list, lidar_seg
-
-
-
diff --git a/projects/mmdet3d_plugin/bevformer/modules/__init__.py b/projects/mmdet3d_plugin/bevformer/modules/__init__.py
index 17ded68..cab017e 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/__init__.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/__init__.py
@@ -7,12 +7,12 @@ from .decoder import DetectionTransformerDecoder
 from .occ_temporal_attention import OccTemporalAttention
 from .occ_spatial_attention import OccSpatialAttention
 from .occ_decoder import OccupancyDecoder
-from .occ_mlp_decoder import MLP_Decoder, SparseMLPDecoder
+from .occ_mlp_decoder import MLP_Decoder#, SparseMLPDecoder
 from .occ_temporal_encoder import OccTemporalEncoder
 from .transformer_occ import TransformerOcc
 from .occ_voxel_decoder import VoxelDecoder
 from .pano_transformer_occ import PanoOccTransformer
 from .panoseg_transformer_occ import PanoSegOccTransformer
 from .occ_voxel_seg_decoder import VoxelNaiveDecoder
-from .sparse_occ_decoder import SparseOccupancyDecoder
-from .sparse_occ_transformer import SparseOccupancyTransformer
\ No newline at end of file
+# from .sparse_occ_decoder import SparseOccupancyDecoder
+# from .sparse_occ_transformer import SparseOccupancyTransformer
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/modules/decoder.py b/projects/mmdet3d_plugin/bevformer/modules/decoder.py
index 33024f8..dc050ca 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/decoder.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/decoder.py
@@ -23,12 +23,7 @@ from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
 from mmcv.utils import (ConfigDict, build_from_cfg, deprecated_api_warning,
                         to_2tuple)
 
-from mmcv.utils import ext_loader
-from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32, \
-    MultiScaleDeformableAttnFunction_fp16
-
-ext_module = ext_loader.load_ext(
-    '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+from mx_driving.fused import npu_multi_scale_deformable_attn_function
 
 
 def inverse_sigmoid(x, eps=1e-5):
@@ -290,7 +285,7 @@ class CustomMSDeformableAttention(BaseModule):
 
         bs, num_query, _ = query.shape
         bs, num_value, _ = value.shape
-        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        # assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
 
         value = self.value_proj(value)
         if key_padding_mask is not None:
@@ -323,15 +318,8 @@ class CustomMSDeformableAttention(BaseModule):
                 f'Last dim of reference_points must be'
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = npu_multi_scale_deformable_attn_function(value, spatial_shapes, level_start_index, 
+                sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
diff --git a/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py b/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py
index 15058e4..e4caa64 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py
@@ -4,6 +4,23 @@ from mmcv.cnn.bricks.registry import TRANSFORMER_LAYER_SEQUENCE
 import torch.nn.functional as F
 
 
+def interpolate_trilinear(x, scale_factor, mode, align_corners):
+    # assert mode == 'trilinear'
+    # assert align_corners == False
+    # bilinear + bilinear
+    scale_t, scale_h, scale_w = scale_factor
+    N, C, T, H, W = x.size(0), x.size(1), x.size(2), x.size(3), x.size(4)
+
+    x_fused_nc = x.reshape(N*C, T, H, W)
+    y_resize_hw = F.interpolate(x_fused_nc, scale_factor=(scale_h, scale_w), mode='bilinear')
+    new_shape_h, new_shape_w = y_resize_hw.shape[-2], y_resize_hw.shape[-1]
+    y_fused_hw = y_resize_hw.reshape(N, C, T, new_shape_h*new_shape_w)
+    y_resize_t = F.interpolate(y_fused_hw, scale_factor=(scale_t, 1), mode='bilinear')
+    new_shape_t = y_resize_t.shape[-2]
+    y = y_resize_t.reshape(N, C, new_shape_t, new_shape_h, new_shape_w)
+    return y
+
+
 @TRANSFORMER_LAYER_SEQUENCE.register_module()
 class OccupancyDecoder(BaseModule):
 
@@ -66,6 +83,8 @@ class OccupancyDecoder(BaseModule):
 
         voxel_cls = self.semantic_cls(voxel_up1)
 
-        voxel_pred = F.interpolate(voxel_cls,scale_factor=(self.inter_up_rate[0],self.inter_up_rate[1],self.inter_up_rate[2]),mode=self.upsampling_method,align_corners=self.align_corners)
+        voxel_pred = interpolate_trilinear(voxel_cls, 
+                                           scale_factor=(self.inter_up_rate[0], self.inter_up_rate[1], self.inter_up_rate[2]), 
+                                           mode=self.upsampling_method, align_corners=self.align_corners)
 
         return voxel_pred, voxel_det
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py b/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py
index 615e26b..3886252 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py
@@ -4,6 +4,24 @@ from mmcv.cnn.bricks.registry import TRANSFORMER_LAYER_SEQUENCE
 import torch.nn.functional as F
 import torch
 
+
+def interpolate_trilinear(x, scale_factor, mode, align_corners):
+    # assert mode == 'trilinear'
+    # assert align_corners == False
+    # bilinear + bilinear
+    scale_t, scale_h, scale_w = scale_factor
+    N, C, T, H, W = x.size(0), x.size(1), x.size(2), x.size(3), x.size(4)
+
+    x_fused_nc = x.reshape(N*C, T, H, W)
+    y_resize_hw = F.interpolate(x_fused_nc, scale_factor=(scale_h, scale_w), mode='bilinear')
+    new_shape_h, new_shape_w = y_resize_hw.shape[-2], y_resize_hw.shape[-1]
+    y_fused_hw = y_resize_hw.reshape(N, C, T, new_shape_h*new_shape_w)
+    y_resize_t = F.interpolate(y_fused_hw, scale_factor=(scale_t, 1), mode='bilinear')
+    new_shape_t = y_resize_t.shape[-2]
+    y = y_resize_t.reshape(N, C, new_shape_t, new_shape_h, new_shape_w)
+    return y
+
+
 @TRANSFORMER_LAYER_SEQUENCE.register_module()
 class MLP_Decoder(BaseModule):
 
@@ -32,7 +50,9 @@ class MLP_Decoder(BaseModule):
 
         voxel_point_cls = point_cls.view(1,inputs.shape[2],inputs.shape[3],inputs.shape[4],-1).permute(0,4,1,2,3)
 
-        voxel_logits = F.interpolate(voxel_point_cls,scale_factor=(self.inter_up_rate[0],self.inter_up_rate[1],self.inter_up_rate[2]),mode=self.upsampling_method,align_corners=self.align_corners)
+        voxel_logits = interpolate_trilinear(voxel_point_cls, 
+                                             scale_factor=(self.inter_up_rate[0], self.inter_up_rate[1], self.inter_up_rate[2]), 
+                                             mode=self.upsampling_method, align_corners=self.align_corners)
         
         return voxel_logits
 
@@ -71,51 +91,3 @@ class MLP(torch.nn.Module):
             x = layer(x)
                 
         return x
-    
-@TRANSFORMER_LAYER_SEQUENCE.register_module()
-class SimpleMLPDecoder(BaseModule):
-
-    def __init__(self,
-                 num_classes,
-                 out_dim=64,
-                 ):
-        super().__init__()
-        self.num_classes = num_classes
-        self.out_dim = out_dim
-    
-        self.mlp_decoder = MLP(dim_x=self.out_dim,act_fn='softplus',layer_size=2)
-        self.classifier = nn.Linear(self.out_dim, self.num_classes)
-                
-    def forward(self, inputs):
-        # z h w
-        voxel_point = inputs.permute(0, 2, 3, 4, 1).reshape(1,-1,self.out_dim)
-        voxel_point_feat = self.mlp_decoder(voxel_point)
-        point_cls = self.classifier(voxel_point_feat)
-
-        voxel_point_cls = point_cls.reshape(1,inputs.shape[2],inputs.shape[3],inputs.shape[4],-1).permute(0,4,1,2,3)
-        return voxel_point_cls
-
-
-@TRANSFORMER_LAYER_SEQUENCE.register_module()
-class SparseMLPDecoder(BaseModule):
-
-    def __init__(self,
-                 num_classes,
-                 out_dim=64,
-                 ):
-        super().__init__()
-        self.num_classes = num_classes
-        self.out_dim = out_dim
-    
-        self.mlp_decoder = MLP(dim_x=self.out_dim,act_fn='softplus',layer_size=2)
-        self.classifier = nn.Linear(self.out_dim, self.num_classes)
-                
-    def forward(self, inputs):
-
-        feats = inputs.features
-        feats = self.mlp_decoder(feats)
-        logit = self.classifier(feats)
-
-        inputs = inputs.replace_feature(logit)
-
-        return inputs
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/modules/occ_spatial_attention.py b/projects/mmdet3d_plugin/bevformer/modules/occ_spatial_attention.py
index 05eb77f..1713992 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/occ_spatial_attention.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/occ_spatial_attention.py
@@ -1,3 +1,8 @@
+# Copyright (c) 2022-2023, NVIDIA Corporation & Affiliates. All rights reserved.
+#
+# This work is made available under the Nvidia Source Code License-NC.
+# To view a copy of this license, visit
+# https://github.com/NVlabs/VoxFormer/blob/main/LICENSE
 
 # ---------------------------------------------
 # Copyright (c) OpenMMLab. All rights reserved.
diff --git a/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py b/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py
index 8f62f3a..ccad6bc 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py
@@ -17,9 +17,7 @@ from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
 from mmcv.utils import (ConfigDict, build_from_cfg, deprecated_api_warning,
                         to_2tuple)
 
-from mmcv.utils import ext_loader
-ext_module = ext_loader.load_ext(
-    '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+from mx_driving.fused import npu_multi_scale_deformable_attn_function
 
 
 @ATTENTION.register_module()
@@ -243,15 +241,8 @@ class OccTemporalAttention(BaseModule):
 
         sampling_locations = sampling_locations.contiguous()
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = npu_multi_scale_deformable_attn_function(value, spatial_shapes, level_start_index, 
+                sampling_locations, attention_weights)
         else:
 
             output = multi_scale_deformable_attn_pytorch(
diff --git a/projects/mmdet3d_plugin/bevformer/modules/pano_transformer_occ.py b/projects/mmdet3d_plugin/bevformer/modules/pano_transformer_occ.py
index 9ef2321..f32d6b4 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/pano_transformer_occ.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/pano_transformer_occ.py
@@ -1,3 +1,9 @@
+# ---------------------------------------------
+# Copyright (c) OpenMMLab. All rights reserved.
+# ---------------------------------------------
+#  Modified by Zhiqi Li
+# ---------------------------------------------
+
 import numpy as np
 import torch
 import torch.nn as nn
diff --git a/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py b/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py
index be6c6ed..b1421a0 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py
@@ -1,3 +1,10 @@
+# ---------------------------------------------
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+# ---------------------------------------------
+#  Modified by Zhiqi Li
+# ---------------------------------------------
+
 import numpy as np
 import torch
 import torch.nn as nn
@@ -206,10 +213,12 @@ class PanoSegOccTransformer(BaseModule):
 
                 curr_grid_in_prev_frame = torch.stack(curr_grid_in_prev_frame_lst, dim=0)
 
+                torch.npu.set_compile_mode(jit_compile=True)
                 prev_bev_warp_to_curr_frame = nn.functional.grid_sample(
                     prev_bev[i].permute(0, 1, 4, 2, 3),  # [bs, dim, z, h, w]
                     curr_grid_in_prev_frame.permute(0, 3, 1, 2, 4),  # [bs, z, h, w, 3]
                     align_corners=False)
+                torch.npu.set_compile_mode(jit_compile=False)
                 prev_bev = prev_bev_warp_to_curr_frame.permute(0, 1, 3, 4, 2).unsqueeze(0) # add bs dim, [bs, dim, h, w, z]
 
             return prev_bev
diff --git a/projects/mmdet3d_plugin/bevformer/modules/sparse_occ_decoder.py b/projects/mmdet3d_plugin/bevformer/modules/sparse_occ_decoder.py
deleted file mode 100644
index 3e6854f..0000000
--- a/projects/mmdet3d_plugin/bevformer/modules/sparse_occ_decoder.py
+++ /dev/null
@@ -1,305 +0,0 @@
-from mmcv.runner import BaseModule
-from torch import nn as nn
-from mmcv.cnn.bricks.registry import TRANSFORMER_LAYER_SEQUENCE
-from mmcv.cnn.bricks.transformer import build_transformer_layer_sequence
-import torch.nn.functional as F
-import torch
-
-
-from spconv.pytorch import SparseConvTensor, SparseSequential
-from mmdet3d.ops import make_sparse_convmodule
-
-from ipdb import set_trace
-
-
-@TRANSFORMER_LAYER_SEQUENCE.register_module()
-class OccupancyOnlyDecoder(BaseModule):
-
-    def __init__(self,
-                 bev_h=50,
-                 bev_w=50,
-                 bev_z=8,
-                 conv_up_layer=2,
-                 embed_dim=256,
-                 out_dim=64,
-                 early_supervision_cfg=dict(),
-                 ):
-        super().__init__()
-        self.bev_h = bev_h
-        self.bev_w = bev_w
-        self.bev_z = bev_z
-        self.out_dim = out_dim
-        self.conv_up_layer = conv_up_layer
-        self.upsample = nn.Sequential(
-            nn.ConvTranspose3d(embed_dim,embed_dim,(1,5,5),padding=(0,2,2)),
-            nn.BatchNorm3d(embed_dim),
-            nn.ReLU(inplace=True),
-
-            nn.ConvTranspose3d(embed_dim, embed_dim, (1, 4, 4), stride=(1, 2, 2), padding=(0,1,1)),
-            nn.BatchNorm3d(embed_dim),
-            nn.ReLU(inplace=True),
-
-            nn.ConvTranspose3d(embed_dim, self.out_dim, (2, 4, 4), stride=(2, 2, 2),padding=(0,1,1)),
-            nn.BatchNorm3d(self.out_dim),
-            nn.ReLU(inplace=True),
-        )
-
-        for m in self.modules():
-            if isinstance(m, nn.Conv3d):
-                nn.init.kaiming_normal_(m.weight.data)
-                nn.init.zeros_(m.bias.data)
-
-        for m in self.modules():
-            if isinstance(m, nn.ConvTranspose3d):
-                nn.init.kaiming_normal_(m.weight.data)
-                nn.init.zeros_(m.bias.data)
-
-        cfg = early_supervision_cfg
-        if cfg.get('layer0_loss', None) is not None:
-            self.mlp_decoder0 = build_transformer_layer_sequence(cfg['layer0_decoder'])
-        
-
-                
-    def forward(self, inputs):
-        out_list = []
-        
-        voxel_input = inputs.view(1,self.bev_w,self.bev_h,self.bev_z, -1).permute(0,4,3,1,2) #[bsz, c, z, w, h]
-
-        if hasattr(self, 'mlp_decoder0'):
-            occ0 = self.mlp_decoder0(voxel_input)
-            out_list.append(occ0)
-
-        voxel_feat = self.upsample(voxel_input)
-        out_list.append(voxel_feat)
-        
-        return out_list
-
-@TRANSFORMER_LAYER_SEQUENCE.register_module()
-class SparseOccupancyDecoder(BaseModule):
-
-    def __init__(self,
-                 bev_h=50,
-                 bev_w=50,
-                 bev_z=8,
-                 conv_up_layer=2,
-                 embed_dim=256,
-                 out_dim=64,
-                 norm_cfg=dict(),
-                 early_supervision_cfg=dict(),
-                 sparse_cfg=dict(),
-                 ):
-        super().__init__()
-        self.bev_h = bev_h
-        self.bev_w = bev_w
-        self.bev_z = bev_z
-        self.out_dim = out_dim
-        self.conv_up_layer = conv_up_layer
-        self.sparse_cfg = sparse_cfg
-        
-        self.num_layers = len(sparse_cfg['strides'])
-
-        cfg = early_supervision_cfg
-        # if cfg.get('layer0_loss', None) is not None:
-        #     self.mlp_decoder0 = build_transformer_layer_sequence(cfg['layer0_decoder'])
-
-
-        for loss_i in range(cfg.get('num_early_loss_layers', 1)):
-            if cfg.get(f'layer{loss_i}_loss', None) is not None:
-                setattr(self, f'mlp_decoder{loss_i}', build_transformer_layer_sequence(cfg[f'layer{loss_i}_decoder']))
-
-        for i in range(self.num_layers):
-            stride = sparse_cfg['strides'][i]
-
-            if max(stride) == 1:
-                conv_type = 'SubMConv3d'
-            else:
-                conv_type = 'SparseConvTranspose3d'
-
-            this_deconv = make_sparse_convmodule(
-                sparse_cfg['in_channels'][i],
-                sparse_cfg['out_channels'][i],
-                kernel_size=sparse_cfg['kernel_sizes'][i],
-                indice_key=f'transpose{i}',
-                stride=stride,
-                norm_cfg=sparse_cfg['norm_cfg'],
-                padding=sparse_cfg['paddings'][i],
-                conv_type=conv_type,
-            )
-            if sparse_cfg.get('num_attached_subm', None) is None:
-                setattr(self, f'upsample_{i}', this_deconv)
-            else:
-                this_convs = [this_deconv,]
-                for subm_i in range(sparse_cfg['num_attached_subm'][i]):
-                    this_subm = make_sparse_convmodule(
-                        sparse_cfg['out_channels'][i],
-                        sparse_cfg['out_channels'][i],
-                        kernel_size=sparse_cfg['subm_kernel_sizes'][i],
-                        indice_key=f'subm_{i}_{subm_i}',
-                        stride=1,
-                        norm_cfg=sparse_cfg['norm_cfg'],
-                        conv_type='SubMConv3d',
-                    )
-                    this_convs.append(this_subm)
-                setattr(self, f'upsample_{i}', SparseSequential(*this_convs))
-
-        if 'extra_layer0_conv' in cfg:
-            num_extra_conv = cfg['extra_layer0_conv']['num_extra_conv']
-            conv_list = []
-            for i in range(num_extra_conv):
-
-                conv_list += [
-                    nn.Conv3d(embed_dim, embed_dim, (3,3,3), padding=(1,1,1)),
-                    nn.BatchNorm3d(embed_dim),
-                    nn.ReLU(inplace=True)
-                ]
-        
-            self.extra_layer0_conv = nn.Sequential(*conv_list)
-        
-
-                
-    def forward(self, inputs):
-        out_list = []
-        
-        voxel_input = inputs.view(1, self.bev_h, self.bev_w, self.bev_z, -1).permute(0,4,3,1,2) #[bsz, c, z, h, w]
-
-        if hasattr(self, 'extra_layer0_conv'):
-            voxel_input = self.extra_layer0_conv(voxel_input)
-
-        if hasattr(self, 'mlp_decoder0'):
-            occ0 = self.mlp_decoder0(voxel_input)
-            out_list.append(occ0)
-
-        sparse_data = self.sparsify(voxel_input, occ0)
-
-        for i in range(self.num_layers):
-            this_conv = getattr(self, f'upsample_{i}')
-            sparse_data = this_conv(sparse_data)
-            if hasattr(self, f'mlp_decoder{i+1}'):
-                this_mlp_decoder = getattr(self, f'mlp_decoder{i+1}')
-                this_occ = this_mlp_decoder(sparse_data)
-                out_list.append(this_occ)
-                
-                sparse_data = self.prune(sparse_data, this_occ, i+1)
-
-        out_list.append(sparse_data)
-        
-        return out_list
-    
-    def prune(self, sparse_feats, this_occ, index):
-        feats = sparse_feats.features
-        coors = sparse_feats.indices
-        old_len = len(feats)
-
-        sp_shape = sparse_feats.spatial_shape
-        bsz = sparse_feats.batch_size
-
-        assert (this_occ.indices == coors).all()
-
-        occ = this_occ.features
-
-        thresh = self.sparse_cfg['pruning_thresh'][index]
-        max_ratio = self.sparse_cfg['max_keep_ratio'][index]
-        
-        if max_ratio < 0: # conduct random sampling for debugging
-            max_ratio = abs(max_ratio)
-            keep_num = int(max_ratio * old_len)
-            top_inds = torch.randperm(old_len, device=feats.device)[:keep_num]
-            feats = feats[top_inds]
-            coors = coors[top_inds]
-            sparse_input = SparseConvTensor(feats, coors, sp_shape, bsz)
-            return sparse_input
-
-        occ_prob = occ.sigmoid().reshape(-1)
-        keep_mask = occ_prob > thresh
-
-
-        min_keep_num = self.sparse_cfg.get('min_keep_num', 500)
-        if keep_mask.sum() < min_keep_num:
-            print(f'Got too small number of occupied voxels at layer {index}!!!!')
-            top_inds = torch.sort(occ_prob, descending=True)[1]
-            top_inds = top_inds[:min_keep_num]
-            feats = feats[top_inds]
-            coors = coors[top_inds]
-            pruned_data = SparseConvTensor(feats, coors, sp_shape, bsz)
-            return pruned_data
-
-
-        feats = feats[keep_mask]
-        coors = coors[keep_mask]
-        occ_prob = occ_prob[keep_mask]
-
-        if len(feats) > old_len * max_ratio:
-
-            top_inds = torch.sort(occ_prob, descending=True)[1]
-            top_inds = top_inds[:int(old_len * max_ratio)]
-
-            feats = feats[top_inds]
-            coors = coors[top_inds]
-            occ_prob = occ_prob[top_inds]
-        
-        pruned_data = SparseConvTensor(feats, coors, sp_shape, bsz)
-
-        return pruned_data
-
-    
-    def sparsify(self, voxel_input, occ_prob):
-
-        device = voxel_input.device
-
-        occ_prob = occ_prob.sigmoid()
-        occ_prob = occ_prob.permute(0, 2, 3, 4, 1)
-        assert occ_prob.shape[-1] == 1
-        occ_prob = occ_prob.reshape(-1)
-
-        bsz, C, z, y, x = voxel_input.shape 
-        voxel_input = voxel_input.permute(0, 2, 3, 4, 1).reshape(-1, C) # to [bsz, z, y, x, C]
-
-        coors = -1 * torch.ones(bsz, z, y, x, 4, dtype=torch.int32, device=device)
-        coors[..., 0] = torch.arange(bsz, dtype=torch.int32, device=device)[:, None, None, None]
-        coors[..., 1] = torch.arange(z, dtype=torch.int32, device=device)[None, :, None, None]
-        coors[..., 2] = torch.arange(y, dtype=torch.int32, device=device)[None, None, :, None]
-        coors[..., 3] = torch.arange(x, dtype=torch.int32, device=device)[None, None, None, :]
-
-        coors = coors.reshape(-1, 4)
-
-        thresh = self.sparse_cfg['pruning_thresh']
-        if isinstance(thresh, (list, tuple)):
-            thresh = thresh[0]
-
-        max_ratio = self.sparse_cfg['max_keep_ratio']
-        if isinstance(max_ratio, (list, tuple)):
-            max_ratio = max_ratio[0]
-
-        keep_mask = occ_prob > thresh
-
-        min_keep_num = self.sparse_cfg.get('min_keep_num', 500)
-        if keep_mask.sum() < min_keep_num:
-            print('Got too small number of occupied voxels !!!!')
-            top_inds = torch.sort(occ_prob, descending=True)[1]
-            top_inds = top_inds[:min_keep_num]
-            voxel_input = voxel_input[top_inds]
-            coors = coors[top_inds]
-            sparse_input = SparseConvTensor(voxel_input, coors, (z, y, x), bsz)
-            return sparse_input
-
-
-        voxel_input = voxel_input[keep_mask]
-        coors = coors[keep_mask]
-        occ_prob = occ_prob[keep_mask]
-
-        dense_num = bsz * z * y * x
-
-        if len(voxel_input) > dense_num * max_ratio:
-
-            top_inds = torch.sort(occ_prob, descending=True)[1]
-            top_inds = top_inds[:int(dense_num * max_ratio)]
-
-            voxel_input = voxel_input[top_inds]
-            coors = coors[top_inds]
-            occ_prob = occ_prob[top_inds]
-        
-        sparse_input = SparseConvTensor(voxel_input, coors, (z, y, x), bsz)
-
-        return sparse_input
-        
-
diff --git a/projects/mmdet3d_plugin/bevformer/modules/sparse_occ_transformer.py b/projects/mmdet3d_plugin/bevformer/modules/sparse_occ_transformer.py
deleted file mode 100644
index a3487fd..0000000
--- a/projects/mmdet3d_plugin/bevformer/modules/sparse_occ_transformer.py
+++ /dev/null
@@ -1,338 +0,0 @@
-import numpy as np
-import torch
-import torch.nn as nn
-from mmcv.cnn import xavier_init
-from mmcv.cnn.bricks.transformer import build_transformer_layer_sequence
-from mmcv.runner.base_module import BaseModule
-from mmcv.cnn.bricks.registry import ATTENTION
-from mmcv.utils import build_from_cfg
-from typing import Optional
-
-from mmdet.models.utils.builder import TRANSFORMER
-from torch.nn.init import normal_
-from projects.mmdet3d_plugin.models.utils.visual import save_tensor
-from mmcv.runner.base_module import BaseModule
-from torchvision.transforms.functional import rotate
-from .temporal_self_attention import TemporalSelfAttention
-from .spatial_cross_attention import MSDeformableAttention3D
-from .decoder import CustomMSDeformableAttention
-from projects.mmdet3d_plugin.models.utils.bricks import run_time
-from mmcv.runner import force_fp32, auto_fp16
-
-from ipdb import set_trace
-
-
-@TRANSFORMER.register_module()
-class SparseOccupancyTransformer(BaseModule):
-    """Implements the Detr3D transformer.
-    Args:
-        as_two_stage (bool): Generate query from encoder features.
-            Default: False.
-        num_feature_levels (int): Number of feature maps from FPN:
-            Default: 4.
-        two_stage_num_proposals (int): Number of proposals when set
-            `as_two_stage` as True. Default: 300.
-    """
-
-    def __init__(self,
-                 num_feature_levels=4,
-                 num_cams=6,
-                 two_stage_num_proposals=300,
-                 cam_encoder=None,
-                 temporal_encoder=None,
-                 voxel_encoder = None,
-                 seg_decoder = None,
-                 embed_dims=256,
-                 rotate_prev_bev=True,
-                 use_shift=True,
-                 use_can_bus=True,
-                 can_bus_norm=True,
-                 use_cams_embeds=True,
-                 rotate_center=[100, 100],
-                 **kwargs):
-        super(SparseOccupancyTransformer, self).__init__(**kwargs)
-        self.cam_encoder = build_transformer_layer_sequence(cam_encoder)
-        self.temporal_encoder = build_from_cfg(temporal_encoder, ATTENTION)
-        # self.decoder = build_transformer_layer_sequence(decoder)
-        self.voxel_encoder = build_transformer_layer_sequence(voxel_encoder)
-        self.seg_decoder = build_transformer_layer_sequence(seg_decoder)
-        self.embed_dims = embed_dims
-        self.num_feature_levels = num_feature_levels
-        self.num_cams = num_cams
-        self.fp16_enabled = False
-
-        self.rotate_prev_bev = rotate_prev_bev
-        self.use_shift = use_shift
-        self.use_can_bus = use_can_bus
-        self.can_bus_norm = can_bus_norm
-        self.use_cams_embeds = use_cams_embeds
-
-        self.two_stage_num_proposals = two_stage_num_proposals
-        self.init_layers()
-        self.rotate_center = rotate_center
-
-    def init_layers(self):
-        """Initialize layers of the Detr3DTransformer."""
-        self.level_embeds = nn.Parameter(torch.Tensor(
-            self.num_feature_levels, self.embed_dims))
-        self.cams_embeds = nn.Parameter(
-            torch.Tensor(self.num_cams, self.embed_dims))
-        # self.reference_points = nn.Linear(self.embed_dims, 3)
-        self.can_bus_mlp = nn.Sequential(
-            nn.Linear(18, self.embed_dims // 2),
-            nn.ReLU(inplace=True),
-            nn.Linear(self.embed_dims // 2, self.embed_dims),
-            nn.ReLU(inplace=True),
-        )
-        if self.can_bus_norm:
-            self.can_bus_mlp.add_module('norm', nn.LayerNorm(self.embed_dims))
-
-    def init_weights(self):
-        """Initialize the transformer weights."""
-        for p in self.parameters():
-            if p.dim() > 1:
-                nn.init.xavier_uniform_(p)
-        for m in self.modules():
-            if isinstance(m, MSDeformableAttention3D) or isinstance(m, TemporalSelfAttention) \
-                    or isinstance(m, CustomMSDeformableAttention):
-                try:
-                    m.init_weight()
-                except AttributeError:
-                    m.init_weights()
-        normal_(self.level_embeds)
-        normal_(self.cams_embeds)
-        # xavier_init(self.reference_points, distribution='uniform', bias=0.)
-        xavier_init(self.can_bus_mlp, distribution='uniform', bias=0.)
-
-    @auto_fp16(apply_to=('mlvl_feats', 'bev_queries', 'prev_bev', 'bev_pos'))
-    def get_bev_features(
-            self,
-            mlvl_feats,
-            bev_queries,
-            bev_h,
-            bev_w,
-            bev_z,
-            grid_length=[0.512, 0.512],
-            bev_pos=None,
-            **kwargs):
-        """
-        obtain bev features.
-        """
-
-        bs = mlvl_feats[0].size(0)
-        bev_queries = bev_queries.unsqueeze(1).repeat(1, bs, 1)
-        bev_pos = bev_pos.flatten(2).permute(2, 0, 1)
-
-        # obtain rotation angle and shift with ego motion
-        delta_x = np.array([each['can_bus'][0]
-                           for each in kwargs['img_metas']])
-        delta_y = np.array([each['can_bus'][1]
-                           for each in kwargs['img_metas']])
-        ego_angle = np.array(
-            [each['can_bus'][-2] / np.pi * 180 for each in kwargs['img_metas']])
-        grid_length_y = grid_length[0]
-        grid_length_x = grid_length[1]
-        translation_length = np.sqrt(delta_x ** 2 + delta_y ** 2)
-        translation_angle = np.arctan2(delta_y, delta_x) / np.pi * 180
-        bev_angle = ego_angle - translation_angle
-        shift_y = translation_length * \
-            np.cos(bev_angle / 180 * np.pi) / grid_length_y / bev_h
-        shift_x = translation_length * \
-            np.sin(bev_angle / 180 * np.pi) / grid_length_x / bev_w
-        shift_y = shift_y * self.use_shift
-        shift_x = shift_x * self.use_shift
-        shift = bev_queries.new_tensor([shift_x, shift_y]).permute(1, 0)  # xy, bs -> bs, xy
-
-        # add can bus signals
-        can_bus = bev_queries.new_tensor(
-            [each['can_bus'] for each in kwargs['img_metas']])  # [:, :]
-        can_bus = self.can_bus_mlp(can_bus)[None, :, :]
-        bev_queries = bev_queries + can_bus * self.use_can_bus
-
-        feat_flatten = []
-        spatial_shapes = []
-        for lvl, feat in enumerate(mlvl_feats):
-            bs, num_cam, c, h, w = feat.shape
-            spatial_shape = (h, w)
-            feat = feat.flatten(3).permute(1, 0, 3, 2)
-            if self.use_cams_embeds:
-                feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)
-            feat = feat + self.level_embeds[None,
-                                            None, lvl:lvl + 1, :].to(feat.dtype)
-            spatial_shapes.append(spatial_shape)
-            feat_flatten.append(feat)
-
-        feat_flatten = torch.cat(feat_flatten, 2)
-        spatial_shapes = torch.as_tensor(
-            spatial_shapes, dtype=torch.long, device=bev_pos.device)
-        level_start_index = torch.cat((spatial_shapes.new_zeros(
-            (1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))
-
-        feat_flatten = feat_flatten.permute(
-            0, 2, 1, 3)  # (num_cam, H*W, bs, embed_dims)
-
-        bev_embed = self.cam_encoder(
-            bev_queries,
-            feat_flatten,
-            feat_flatten,
-            bev_h=bev_h,
-            bev_w=bev_w,
-            bev_z=bev_z,
-            bev_pos=bev_pos,
-            spatial_shapes=spatial_shapes,
-            level_start_index=level_start_index,
-            shift=shift,
-            **kwargs
-        )
-
-        return bev_embed
-
-    def align_prev_bev(self, prev_bev, bev_h, bev_w, bev_z, **kwargs):
-        if prev_bev is not None:
-            pc_range = self.cam_encoder.pc_range
-            ref_y, ref_x, ref_z = torch.meshgrid(
-                    torch.linspace(0.5, bev_h - 0.5, bev_h, dtype=prev_bev.dtype, device=prev_bev.device),
-                    torch.linspace(0.5, bev_w - 0.5, bev_w, dtype=prev_bev.dtype, device=prev_bev.device),
-                    torch.linspace(0.5, bev_z - 0.5, bev_z, dtype=prev_bev.dtype, device=prev_bev.device),
-                )
-            ref_y = ref_y / bev_h
-            ref_x = ref_x / bev_w
-            ref_z = ref_z / bev_z
-
-            GROUND_HEIGHT = -2
-            grid = torch.stack(
-                    (ref_x,
-                    ref_y,
-                    # ref_x.new_full(ref_x.shape, GROUND_HEIGHT),
-                    ref_z,
-                    ref_x.new_ones(ref_x.shape)), dim=-1)
-
-            min_x, min_y, min_z, max_x, max_y, max_z = pc_range
-            grid[..., 0] = grid[..., 0] * (max_x - min_x) + min_x
-            grid[..., 1] = grid[..., 1] * (max_y - min_y) + min_y
-            grid[..., 2] = grid[..., 2] * (max_z - min_z) + min_z
-            grid = grid.reshape(-1, 4)
-
-            bs = prev_bev.shape[0]
-            len_queue = prev_bev.shape[1]
-            assert bs == 1
-            for i in range(bs):
-                lidar_to_ego = kwargs['img_metas'][i]['lidar2ego_transformation']
-                curr_ego_to_global = kwargs['img_metas'][i]['ego2global_transform_lst'][-1]
-
-                curr_grid_in_prev_frame_lst = []
-                for j in range(len_queue):
-                    prev_ego_to_global = kwargs['img_metas'][i]['ego2global_transform_lst'][j]
-                    prev_lidar_to_curr_lidar = np.linalg.inv(lidar_to_ego) @ np.linalg.inv(curr_ego_to_global) @ prev_ego_to_global @ lidar_to_ego
-                    curr_lidar_to_prev_lidar = np.linalg.inv(prev_lidar_to_curr_lidar)
-                    curr_lidar_to_prev_lidar = grid.new_tensor(curr_lidar_to_prev_lidar)
-
-                    curr_grid_in_prev_frame = torch.matmul(curr_lidar_to_prev_lidar, grid.T).T.reshape(bev_h, bev_w, bev_z, -1)[..., :3]
-                    curr_grid_in_prev_frame[..., 0] = (curr_grid_in_prev_frame[..., 0] - min_x) / (max_x - min_x)
-                    curr_grid_in_prev_frame[..., 1] = (curr_grid_in_prev_frame[..., 1] - min_y) / (max_y - min_y)
-                    curr_grid_in_prev_frame[..., 2] = (curr_grid_in_prev_frame[..., 2] - min_z) / (max_z - min_z)
-                    curr_grid_in_prev_frame = curr_grid_in_prev_frame * 2.0 - 1.0
-                    curr_grid_in_prev_frame_lst.append(curr_grid_in_prev_frame)
-
-                curr_grid_in_prev_frame = torch.stack(curr_grid_in_prev_frame_lst, dim=0)
-
-                prev_bev_warp_to_curr_frame = torch.nn.functional.grid_sample(
-                    prev_bev[i].permute(0, 1, 4, 2, 3),  # [bs, dim, z, h, w]
-                    curr_grid_in_prev_frame.permute(0, 3, 1, 2, 4),  # [bs, z, h, w, 3]
-                    align_corners=False)
-                prev_bev = prev_bev_warp_to_curr_frame.permute(0, 1, 3, 4, 2).unsqueeze(0) # add bs dim, [bs, dim, h, w, z]
-            return prev_bev
-
-    def bev_temporal_fuse(
-        self,
-        bev_embeds: torch.Tensor,
-        prev_bev: Optional[torch.Tensor],
-        bev_h,
-        bev_w,
-        bev_z,
-        **kwargs
-    ) -> torch.Tensor:
-        # [bs, num_queue, embed_dims, bev_h, bev_w]
-        prev_bev = self.align_prev_bev(prev_bev, bev_h, bev_w, bev_z, **kwargs)
-
-        ref_2d = self.cam_encoder.get_reference_points(
-            bev_h, bev_w, dim='2d', bs=bev_embeds.size(0), device=bev_embeds.device, dtype=bev_embeds.dtype)
-        bev_pos = kwargs["bev_pos"].flatten(2).permute(0, 2, 1)
-        bev_embeds = self.temporal_encoder(bev_embeds, prev_bev, ref_2d=ref_2d, bev_pos=bev_pos)
-
-        return bev_embeds
-
-
-    @auto_fp16(apply_to=('mlvl_feats', 'bev_queries', 'prev_bev', 'bev_pos'))
-    def forward(self,
-                mlvl_feats,
-                bev_queries,
-                bev_h,
-                bev_w,
-                bev_z,
-                grid_length=[0.512, 0.512],
-                bev_pos=None,
-                reg_branches=None,
-                cls_branches=None,
-                prev_bev=None,
-                **kwargs):
-        """Forward function for `Detr3DTransformer`.
-        Args:
-            mlvl_feats (list(Tensor)): Input queries from
-                different level. Each element has shape
-                [bs, num_cams, embed_dims, h, w].
-            bev_queries (Tensor): (bev_h*bev_w, c)
-            bev_pos (Tensor): (bs, embed_dims, bev_h, bev_w)
-            object_query_embed (Tensor): The query embedding for decoder,
-                with shape [num_query, c].
-            reg_branches (obj:`nn.ModuleList`): Regression heads for
-                feature maps from each decoder layer. Only would
-                be passed when `with_box_refine` is True. Default to None.
-        Returns:
-            tuple[Tensor]: results of decoder containing the following tensor.
-                - bev_embed: BEV features
-                - inter_states: Outputs from decoder. If
-                    return_intermediate_dec is True output has shape \
-                      (num_dec_layers, bs, num_query, embed_dims), else has \
-                      shape (1, bs, num_query, embed_dims).
-                - init_reference_out: The initial value of reference \
-                    points, has shape (bs, num_queries, 4).
-                - inter_references_out: The internal value of reference \
-                    points in decoder, has shape \
-                    (num_dec_layers, bs,num_query, embed_dims)
-                - enc_outputs_class: The classification score of \
-                    proposals generated from \
-                    encoder's feature maps, has shape \
-                    (batch, h*w, num_classes). \
-                    Only would be returned when `as_two_stage` is True, \
-                    otherwise None.
-                - enc_outputs_coord_unact: The regression results \
-                    generated from encoder's feature maps., has shape \
-                    (batch, h*w, 4). Only would \
-                    be returned when `as_two_stage` is True, \
-                    otherwise None.
-        """
-
-        bev_feat = self.get_bev_features(
-            mlvl_feats,
-            bev_queries,
-            bev_h,
-            bev_w,
-            bev_z,
-            grid_length=grid_length,
-            bev_pos=bev_pos,
-            **kwargs)  # bev_embed shape: bs, bev_h*bev_w, embed_dims
-
-        bev_embed = self.bev_temporal_fuse(bev_feat, prev_bev, bev_h, bev_w, bev_z, bev_pos=bev_pos, **kwargs)
-
-        bev_embed_vox = bev_embed.view(1, bev_h*bev_w,bev_z, -1)
-
-        outs = self.voxel_encoder(bev_embed_vox)
-
-        voxel_feat = outs[-1]
-
-        occupancy = self.seg_decoder(voxel_feat)
-
-        occ_list = outs[:-1] + [occupancy,]
-
-        return bev_feat, occ_list
diff --git a/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py b/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py
index b53b66c..12314b5 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py
@@ -1,31 +1,31 @@
 
 # ---------------------------------------------
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 # ---------------------------------------------
-#  Modified by Zhiqi Li
+#  Modified by Zhexu Liu
 # ---------------------------------------------
 
-from mmcv.ops.multi_scale_deform_attn import multi_scale_deformable_attn_pytorch
 import warnings
+import math
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+from mmcv.ops.multi_scale_deform_attn import multi_scale_deformable_attn_pytorch
 from mmcv.cnn import xavier_init, constant_init
 from mmcv.cnn.bricks.registry import (ATTENTION,
                                       TRANSFORMER_LAYER,
                                       TRANSFORMER_LAYER_SEQUENCE)
 from mmcv.cnn.bricks.transformer import build_attention
-import math
-from mmcv.runner import force_fp32, auto_fp16
-
+from mmcv.runner import force_fp32
 from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
-
-from mmcv.utils import ext_loader
-from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32, \
-    MultiScaleDeformableAttnFunction_fp16
 from projects.mmdet3d_plugin.models.utils.bricks import run_time
-ext_module = ext_loader.load_ext(
-    '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+from mx_driving.fused import npu_multi_scale_deformable_attn_function
+
+indexes_global = None
+max_len_global = None
+bev_mask_id_global = -1
+count_global = None
 
 
 @ATTENTION.register_module()
@@ -135,10 +135,27 @@ class SpatialCrossAttention(BaseModule):
         # bevformer reference_points_cam shape: (num_cam,bs,h*w,num_points_in_pillar,2)
         D = reference_points_cam.size(3)
         indexes = []
-        for i, mask_per_img in enumerate(bev_mask):
-            index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
-            indexes.append(index_query_per_img)
-        max_len = max([len(each) for each in indexes])
+        global indexes_global, max_len_global, bev_mask_id_global, count_global
+        bev_mask_id = id(bev_mask)
+        if bev_mask_id == bev_mask_id_global:
+            indexes = indexes_global
+            max_len = max_len_global
+            count = count_global
+        else:
+            count = torch.any(bev_mask, 3)
+            bev_mask_ = count.squeeze()
+            for i, mask_per_img in enumerate(bev_mask_):
+                index_query_per_img = mask_per_img.nonzero().squeeze(-1)
+                indexes.append(index_query_per_img)
+
+            max_len = max([len(each) for each in indexes])
+            count = count.permute(1, 2, 0).sum(-1)
+            count = torch.clamp(count, min=1.0)
+            count = count[..., None]
+            count_global = count
+            indexes_global = indexes
+            max_len_global = max_len
+            bev_mask_id_global = bev_mask_id
 
         # each camera only interacts with its corresponding BEV queries. This step can  greatly save GPU memory.
         queries_rebatch = query.new_zeros(
@@ -146,9 +163,9 @@ class SpatialCrossAttention(BaseModule):
         reference_points_rebatch = reference_points_cam.new_zeros(
             [bs, self.num_cams, max_len, D, 2])
         
-        for j in range(bs):
-            for i, reference_points_per_img in enumerate(reference_points_cam):   
-                index_query_per_img = indexes[i]
+        for i, reference_points_per_img in enumerate(reference_points_cam):   
+            index_query_per_img = indexes[i]
+            for j in range(bs):
                 queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
                 reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
 
@@ -159,17 +176,15 @@ class SpatialCrossAttention(BaseModule):
         value = value.permute(2, 0, 1, 3).reshape(
             bs * self.num_cams, l, self.embed_dims)
 
-        queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
-                                            reference_points=reference_points_rebatch.view(bs*self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
+        queries = self.deformable_attention(query=queries_rebatch.view(bs * self.num_cams, max_len, self.embed_dims), key=key, value=value,
+                                            reference_points=reference_points_rebatch.view(bs * self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
                                             level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims)
         for j in range(bs):
             for i, index_query_per_img in enumerate(indexes):
                 slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
 
-        count = bev_mask.sum(-1) > 0
-        count = count.permute(1, 2, 0).sum(-1)
-        count = torch.clamp(count, min=1.0)
-        slots = slots / count[..., None]
+
+        slots = slots / count
         slots = self.output_proj(slots)
 
         return self.dropout(slots) + inp_residual
@@ -329,7 +344,7 @@ class MSDeformableAttention3D(BaseModule):
 
         bs, num_query, _ = query.shape
         bs, num_value, _ = value.shape
-        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        # assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
 
         value = self.value_proj(value)
         if key_padding_mask is not None:
@@ -366,7 +381,7 @@ class MSDeformableAttention3D(BaseModule):
                 bs, num_query, num_heads, num_levels, num_all_points // num_Z_anchors, num_Z_anchors, xy)
             sampling_locations = reference_points + sampling_offsets
             bs, num_query, num_heads, num_levels, num_points, num_Z_anchors, xy = sampling_locations.shape
-            assert num_all_points == num_points * num_Z_anchors
+            # assert num_all_points == num_points * num_Z_anchors
 
             sampling_locations = sampling_locations.view(
                 bs, num_query, num_heads, num_levels, num_all_points, xy)
@@ -379,13 +394,8 @@ class MSDeformableAttention3D(BaseModule):
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
 
         if torch.cuda.is_available() and value.is_cuda:
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = npu_multi_scale_deformable_attn_function(value, spatial_shapes, level_start_index,
+                                                                         sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
diff --git a/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py b/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py
index 78fb9f5..0655f5d 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py
@@ -17,9 +17,7 @@ from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
 from mmcv.utils import (ConfigDict, build_from_cfg, deprecated_api_warning,
                         to_2tuple)
 
-from mmcv.utils import ext_loader
-ext_module = ext_loader.load_ext(
-    '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+from mx_driving.fused import npu_multi_scale_deformable_attn_function
 
 
 @ATTENTION.register_module()
@@ -238,15 +236,8 @@ class TemporalSelfAttention(BaseModule):
                 f'Last dim of reference_points must be'
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = npu_multi_scale_deformable_attn_function(value, spatial_shapes, level_start_index, 
+                sampling_locations, attention_weights)
         else:
 
             output = multi_scale_deformable_attn_pytorch(
diff --git a/projects/mmdet3d_plugin/datasets/builder.py b/projects/mmdet3d_plugin/datasets/builder.py
index 0ad7a92..c16c58f 100644
--- a/projects/mmdet3d_plugin/datasets/builder.py
+++ b/projects/mmdet3d_plugin/datasets/builder.py
@@ -1,5 +1,6 @@
-
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+
 import copy
 import platform
 import random
@@ -25,6 +26,7 @@ def build_dataloader(dataset,
                      seed=None,
                      shuffler_sampler=None,
                      nonshuffler_sampler=None,
+                     pin_memory=False,
                      **kwargs):
     """Build PyTorch DataLoader.
     In distributed training, each GPU/process has a dataloader.
@@ -86,7 +88,7 @@ def build_dataloader(dataset,
         sampler=sampler,
         num_workers=num_workers,
         collate_fn=partial(collate, samples_per_gpu=samples_per_gpu),
-        pin_memory=False,
+        pin_memory=pin_memory,
         worker_init_fn=init_fn,
         **kwargs)
 
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/compose.py b/projects/mmdet3d_plugin/datasets/pipelines/compose.py
index 08e46a8..585505b 100644
--- a/projects/mmdet3d_plugin/datasets/pipelines/compose.py
+++ b/projects/mmdet3d_plugin/datasets/pipelines/compose.py
@@ -1,8 +1,12 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
+
 import collections
 
 from mmcv.utils import build_from_cfg
 
 from mmdet.datasets.builder import PIPELINES
+from mmdet3d.datasets.builder import PIPELINES as PIPELINES_3d
 
 @PIPELINES.register_module()
 class CustomCompose:
@@ -16,7 +20,10 @@ class CustomCompose:
         self.transforms = []
         for transform in transforms:
             if isinstance(transform, dict):
-                transform = build_from_cfg(transform, PIPELINES)
+                if transform["type"] not in PIPELINES:
+                    transform = build_from_cfg(transform, PIPELINES_3d)
+                else:
+                    transform = build_from_cfg(transform, PIPELINES)
                 self.transforms.append(transform)
             elif callable(transform):
                 self.transforms.append(transform)
diff --git a/projects/mmdet3d_plugin/models/backbones/__init__.py b/projects/mmdet3d_plugin/models/backbones/__init__.py
index f86b114..cea72f5 100755
--- a/projects/mmdet3d_plugin/models/backbones/__init__.py
+++ b/projects/mmdet3d_plugin/models/backbones/__init__.py
@@ -1,5 +1,3 @@
 from .vovnet import VoVNet
-from .internv2_impl16 import InternV2Impl16
-from .sam_modeling import ImageEncoderViT
 
-__all__ = ['VoVNet', "InternV2Impl16", "ImageEncoderViT"]
\ No newline at end of file
+__all__ = ['VoVNet']
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/models/backbones/internv2_impl16.py b/projects/mmdet3d_plugin/models/backbones/internv2_impl16.py
deleted file mode 100644
index 359941c..0000000
--- a/projects/mmdet3d_plugin/models/backbones/internv2_impl16.py
+++ /dev/null
@@ -1,416 +0,0 @@
-import torch.utils.checkpoint as cp
-from timm.models.layers import DropPath, to_2tuple, trunc_normal_
-from mmdet.models.builder import BACKBONES
-from mmcv.runner import BaseModule, ModuleList, _load_checkpoint
-from mmcv.cnn import build_norm_layer, constant_init, trunc_normal_init
-from collections import OrderedDict
-import warnings
-import math
-import torch
-import torch.nn as nn
-from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
-from mmdet.utils import get_root_logger
-from ops.modules import MSDeformAttnGrid_final_softmax as MSDeformAttn
-
-
-class ConvTokenizer(nn.Module):
-    def __init__(self, in_chans=3, embed_dim=96, norm_layer=None):
-        super().__init__()
-        self.proj = nn.Sequential(
-            nn.Conv2d(in_chans, embed_dim // 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),
-            nn.BatchNorm2d(embed_dim // 2),
-            nn.GELU(),
-            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),
-            nn.BatchNorm2d(embed_dim),
-        )
-
-    def forward(self, x):
-        x = self.proj(x).permute(0, 2, 3, 1)
-        return x
-
-
-class ConvDownsampler(nn.Module):
-    def __init__(self, dim, norm_layer=nn.LayerNorm):
-        super().__init__()
-        self.reduction = nn.Conv2d(dim, 2 * dim, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
-        self.norm = norm_layer(2 * dim)
-
-    def forward(self, x):
-        x = self.reduction(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)
-        x = self.norm(x)
-        return x
-
-
-class Mlp(nn.Module):
-    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
-        super().__init__()
-        out_features = out_features or in_features
-        hidden_features = hidden_features or in_features
-        self.fc1 = nn.Linear(in_features, hidden_features)
-        self.act = act_layer()
-        self.fc2 = nn.Linear(hidden_features, out_features)
-        self.drop = nn.Dropout(drop)
-
-    def forward(self, x):
-        x = self.fc1(x)
-        x = self.act(x)
-        x = self.drop(x)
-        x = self.fc2(x)
-        x = self.drop(x)
-        return x
-
-
-# @FEEDFORWARD_NETWORK.register_module()
-# class Mlp(BaseModule):
-
-
-#     def __init__(self,
-#                  in_features=256,
-#                  hidden_features=1024,
-#                  num_fcs=2,
-#                  act_layer=nn.GELU,
-#                  drop=0.,
-#                  act_cfg=dict(type='ReLU', inplace=True),
-#                  ffn_drop=0.,
-#                  dropout_layer=None,
-#                  add_identity=True,
-#                  init_cfg=None,
-#                  split=4,
-#                  use_checkpoint=False,
-#                  **kwargs):
-#         super(Mlp, self).__init__(init_cfg)
-#         assert num_fcs >= 2, 'num_fcs should be no less ' \
-#             f'than 2. got {num_fcs}.'
-#         embed_dims = in_features
-#         feedforward_channels = hidden_features
-#         ffn_drop = drop
-
-#         self.embed_dims = embed_dims
-#         self.feedforward_channels = feedforward_channels
-#         self.num_fcs = num_fcs
-#         self.act_cfg = act_cfg
-#         # self.activate = build_activation_layer(act_cfg)
-#         self.activate = act_layer()
-#         self.drop = nn.Dropout(ffn_drop)
-#         in_channels = embed_dims
-#         self.use_checkpoint = use_checkpoint
-#         self.split = split
-#         for i in range(split):
-#             fc1 = nn.Linear(in_channels, feedforward_channels //
-#                             self.split, bias=True)
-#             setattr(self, f"fc1_{i}", fc1)
-
-#         for i in range(split):
-#             fc2 = nn.Linear(feedforward_channels // self.split,
-#                             embed_dims, bias=False)
-#             setattr(self, f"fc2_{i}", fc2)
-#         self.fc2_bias = nn.Parameter(torch.zeros(
-#             (embed_dims)), requires_grad=True)
-#         self.dropout_layer = build_dropout(
-#             dropout_layer) if dropout_layer else torch.nn.Identity()
-#         self.add_identity = add_identity
-
-#     def forward(self, x, identity=None):
-
-#         def _inner_forward(x, i):
-#             fc1 = getattr(self, f"fc1_{i}")
-#             x = fc1(x)
-#             x = self.activate(x)
-#             x = self.drop(x)
-#             fc2 = getattr(self, f"fc2_{i}")
-#             x = fc2(x)
-#             x = self.drop(x)
-#             return x
-
-#         out = 0
-#         for i in range(self.split):
-#             if self.use_checkpoint and x.requires_grad:
-#                 out = out + checkpoint.checkpoint(_inner_forward, x, i)
-#             else:
-#                 out = out + _inner_forward(x, i)
-
-#         out += self.fc2_bias
-
-#         if not self.add_identity:
-#             return self.dropout_layer(out)
-#         if identity is None:
-#             identity = x
-#         return identity + self.dropout_layer(out)
-
-
-
-class NATLayer(nn.Module):
-    def __init__(self, dim, num_heads, kernel_size=7, deform_points=25, deform_ratio=1.0,
-                 dilation_rates=[1, 2, 3], deform_padding=True, use_hw_scaler=None,
-                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
-                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, layer_scale=None):
-        super().__init__()
-        self.dim = dim
-        self.num_heads = num_heads
-        self.mlp_ratio = mlp_ratio
-
-        self.norm1 = norm_layer(dim)
-        # self.attn = NeighborhoodAttention(
-        #     dim, kernel_size=kernel_size, num_heads=num_heads,
-        #     qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
-        self.attn = MSDeformAttn(d_model=dim, n_levels=1, n_heads=num_heads,
-                                 n_points=deform_points, ratio=deform_ratio, dilation_rates=dilation_rates,
-                                 padding=deform_padding, dw_ks=kernel_size,
-                                 use_hw_scaler=use_hw_scaler)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.norm2 = norm_layer(dim)
-        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)
-        self.layer_scale = False
-        if layer_scale is not None and type(layer_scale) in [int, float]:
-            self.layer_scale = True
-            self.gamma1 = nn.Parameter(layer_scale * torch.ones(dim), requires_grad=True)
-            self.gamma2 = nn.Parameter(layer_scale * torch.ones(dim), requires_grad=True)
-
-    def forward(self, x, deform_inputs):
-        def deform_forward(x):
-            n, h, w, c = x.shape
-            x = self.attn(
-                query=x.view(n, h * w, c),
-                reference_points=deform_inputs[0],
-                input_flatten=None,
-                input_spatial_shapes=deform_inputs[1],
-                input_level_start_index=deform_inputs[2],
-                input_padding_mask=None).view(n, h, w, c)
-            return x
-
-        if not self.layer_scale:
-            shortcut = x
-            x = self.norm1(x)
-            x = deform_forward(x)
-            x = shortcut + self.drop_path(x)
-            x = x + self.drop_path(self.mlp(self.norm2(x)))
-            return x
-        shortcut = x
-
-        x = self.norm1(x)
-        x = deform_forward(x)
-        x = shortcut + self.drop_path(self.gamma1 * x)
-        x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
-        return x
-
-
-class NATBlock(nn.Module):
-    def __init__(self, dim, depth, num_heads, kernel_size, downsample=True,
-                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
-                 deform_points=4, deform_padding=True, dilation_rates=[1, 2, 3],
-                 drop_path=0., norm_layer=nn.LayerNorm, use_hw_scaler=None,
-                 layer_scale=None, with_cp=False):
-        super().__init__()
-        self.dim = dim
-        self.depth = depth
-        self.with_cp = with_cp
-        self.blocks = nn.ModuleList([
-            NATLayer(dim=dim,
-                     num_heads=num_heads, kernel_size=kernel_size,
-                     mlp_ratio=mlp_ratio,
-                     deform_points=deform_points,
-                     deform_padding=deform_padding,
-                     dilation_rates=dilation_rates,
-                     qkv_bias=qkv_bias, qk_scale=qk_scale,
-                     drop=drop, attn_drop=attn_drop,
-                     drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
-                     norm_layer=norm_layer, layer_scale=layer_scale,
-                     use_hw_scaler=use_hw_scaler)
-            for i in range(depth)])
-
-        self.downsample = None if not downsample else ConvDownsampler(dim=dim, norm_layer=norm_layer)
-
-    def forward(self, x, deform_inputs):
-        for blk in self.blocks:
-            if self.with_cp and x.requires_grad:
-                x = cp.checkpoint(blk, x, deform_inputs)
-            else:
-                x = blk(x, deform_inputs)
-        if self.downsample is None:
-            return x, x
-        return self.downsample(x), x
-
-
-@BACKBONES.register_module()
-class InternV2Impl16(BaseModule):
-    def __init__(self, embed_dim=64, mlp_ratio=3., depths=[3, 4, 18, 5], num_heads=[3, 6, 12, 24],
-                 drop_path_rate=0.2, in_chans=3, kernel_size=5, qkv_bias=True, qk_scale=None,
-                 drop_rate=0., attn_drop_rate=0., norm_layer=nn.LayerNorm, layer_scale=None,
-                 deform_points=25, deform_padding=True, dilation_rates=[1], init_cfg=None,
-                 pretrained=None, norm_cfg=dict(type='LN'), out_indices=(0, 1, 2, 3),
-                 use_hw_scaler=None, with_cp=False, cp_level=0, **kwargs):
-
-        super().__init__(init_cfg=init_cfg)
-
-        assert not (init_cfg and pretrained), \
-            'init_cfg and pretrained cannot be specified at the same time'
-        if isinstance(pretrained, str):
-            warnings.warn('DeprecationWarning: pretrained is deprecated, '
-                          'please use "init_cfg" instead')
-            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
-        elif pretrained is None:
-            self.init_cfg = init_cfg
-        else:
-            raise TypeError('pretrained must be a str or None')
-
-        self.out_indices = out_indices
-        self.num_levels = len(depths)
-        self.embed_dim = embed_dim
-        self.num_features = [int(embed_dim * 2 ** i) for i in range(self.num_levels)]
-        print(self.num_features)
-        self.mlp_ratio = mlp_ratio
-        self.deform_padding = deform_padding
-        self.deform_points = deform_points
-        print("deform padding:", deform_padding)
-        self.patch_embed = ConvTokenizer(in_chans=in_chans, embed_dim=embed_dim, norm_layer=norm_layer)
-
-        self.pos_drop = nn.Dropout(p=drop_rate)
-
-        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
-        self.levels = nn.ModuleList()
-        for i in range(self.num_levels):
-            level = NATBlock(dim=int(embed_dim * 2 ** i),
-                             depth=depths[i],
-                             num_heads=num_heads[i],
-                             kernel_size=kernel_size,
-                             mlp_ratio=self.mlp_ratio,
-                             qkv_bias=qkv_bias, qk_scale=qk_scale,
-                             drop=drop_rate, attn_drop=attn_drop_rate,
-                             drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],
-                             norm_layer=norm_layer,
-                             downsample=(i < self.num_levels - 1),
-                             layer_scale=layer_scale,
-                             dilation_rates=dilation_rates,
-                             deform_points=deform_points,
-                             deform_padding=deform_padding,
-                             use_hw_scaler=use_hw_scaler,
-                             with_cp=with_cp if i < cp_level else False)
-            self.levels.append(level)
-            if i < self.num_levels - 1:
-                norm = norm_layer(int(embed_dim * 2 ** (i + 1)))
-                setattr(self, f"norm{i + 1}", norm)
-
-        self.num_layers = len(depths)
-        for i in self.out_indices:
-            layer = build_norm_layer(norm_cfg, self.num_features[i])[1]
-            layer_name = f'final_norm{i}'
-            self.add_module(layer_name, layer)
-
-        # self.apply(self._init_weights)
-        # self.apply(self._init_deform_weights)
-
-    def _init_weights(self, m):
-        if isinstance(m, nn.Linear):
-            trunc_normal_(m.weight, std=.02)
-            if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-        elif isinstance(m, nn.LayerNorm):
-            nn.init.constant_(m.bias, 0)
-            nn.init.constant_(m.weight, 1.0)
-            self.apply(self._init_deform_weights)
-
-    def _init_deform_weights(self, m):
-        if isinstance(m, MSDeformAttn):
-            m._reset_parameters()
-
-    def init_weights(self):
-        logger = get_root_logger()
-        if self.init_cfg is None:
-            logger.warn(f'No pre-trained weights for '
-                        f'{self.__class__.__name__}, '
-                        f'training start from scratch')
-            for m in self.modules():
-                if isinstance(m, nn.Linear):
-                    trunc_normal_init(m, std=.02, bias=0.)
-                elif isinstance(m, nn.LayerNorm):
-                    constant_init(m, 1.0)
-        else:
-            assert 'checkpoint' in self.init_cfg, f'Only support ' \
-                                                  f'specify `Pretrained` in ' \
-                                                  f'`init_cfg` in ' \
-                                                  f'{self.__class__.__name__} '
-            ckpt = _load_checkpoint(
-                self.init_cfg.checkpoint, logger=logger, map_location='cpu')
-            if 'state_dict' in ckpt:
-                _state_dict = ckpt['state_dict']
-            elif 'model' in ckpt:
-                _state_dict = ckpt['model']
-            else:
-                _state_dict = ckpt
-
-            state_dict = OrderedDict()
-            for k, v in _state_dict.items():
-                if k.startswith('backbone.'):
-                    state_dict[k[9:]] = v
-                else:
-                    state_dict[k] = v
-
-            # strip prefix of state_dict
-            if list(state_dict.keys())[0].startswith('module.'):
-                state_dict = {k[7:]: v for k, v in state_dict.items()}
-
-            # load state_dict
-            meg = self.load_state_dict(state_dict, False)
-            logger.info(meg)
-
-    def _get_reference_points(self, spatial_shapes, device, padding=0):
-        reference_points_list = []
-        for lvl, (H_, W_) in enumerate(spatial_shapes):
-            ref_y, ref_x = torch.meshgrid(
-                torch.linspace(padding + 0.5, H_ - padding - 0.5,
-                               int(H_ - 2 * padding),
-                               dtype=torch.float32, device=device),
-                torch.linspace(padding + 0.5, W_ - padding - 0.5,
-                               int(W_ - 2 * padding),
-                               dtype=torch.float32, device=device))
-            ref_y = ref_y.reshape(-1)[None] / H_
-            ref_x = ref_x.reshape(-1)[None] / W_
-            ref = torch.stack((ref_x, ref_y), -1)
-            reference_points_list.append(ref)
-        reference_points = torch.cat(reference_points_list, 1)
-        reference_points = reference_points[:, :, None]
-        return reference_points
-
-    def _deform_inputs(self, x):
-        bs, c, h, w = x.shape
-        deform_inputs = list()
-        if self.deform_padding:
-            padding = int(math.sqrt(self.deform_points) // 2)
-        else:
-            padding = int(0)
-
-        for i in range(self.num_layers):
-            spatial_shapes = torch.as_tensor(
-                [(h // pow(2, i + 2) + 2 * padding,
-                  w // pow(2, i + 2) + 2 * padding)],
-                dtype=torch.long, device=x.device)
-            level_start_index = torch.cat(
-                (spatial_shapes.new_zeros((1,)),
-                 spatial_shapes.prod(1).cumsum(0)[:-1]))
-            reference_points = self._get_reference_points(
-                [(h // pow(2, i + 2) + 2 * padding,
-                  w // pow(2, i + 2) + 2 * padding)],
-                device=x.device, padding=padding)
-            deform_inputs.append(
-                [reference_points, spatial_shapes, level_start_index,
-                 (h // pow(2, i + 2), w // pow(2, i + 2))])
-        return deform_inputs
-
-    def forward(self, x):
-        deform_inputs = self._deform_inputs(x)
-        x = self.patch_embed(x)
-        x = self.pos_drop(x)
-
-        outs = []
-        for i, level in enumerate(self.levels):
-            x, xo = level(x, deform_inputs[i])
-            if i != self.num_levels - 1:
-                norm = getattr(self, f'norm{i + 1}')
-                x = norm(x)
-            if i in self.out_indices:
-                final_norm = getattr(self, f'final_norm{i}')
-                xo = final_norm(xo)
-                outs.append(xo.permute(0, 3, 1, 2).contiguous())
-        return outs
-
-
diff --git a/projects/mmdet3d_plugin/models/backbones/sam_modeling/__init__.py b/projects/mmdet3d_plugin/models/backbones/sam_modeling/__init__.py
deleted file mode 100644
index 50f3bd6..0000000
--- a/projects/mmdet3d_plugin/models/backbones/sam_modeling/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from .image_encoder import ImageEncoderViT
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/models/backbones/sam_modeling/common.py b/projects/mmdet3d_plugin/models/backbones/sam_modeling/common.py
deleted file mode 100644
index d67662c..0000000
--- a/projects/mmdet3d_plugin/models/backbones/sam_modeling/common.py
+++ /dev/null
@@ -1,43 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import torch
-import torch.nn as nn
-
-from typing import Type
-
-
-class MLPBlock(nn.Module):
-    def __init__(
-        self,
-        embedding_dim: int,
-        mlp_dim: int,
-        act: Type[nn.Module] = nn.GELU,
-    ) -> None:
-        super().__init__()
-        self.lin1 = nn.Linear(embedding_dim, mlp_dim)
-        self.lin2 = nn.Linear(mlp_dim, embedding_dim)
-        self.act = act()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return self.lin2(self.act(self.lin1(x)))
-
-
-# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa
-# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa
-class LayerNorm2d(nn.Module):
-    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:
-        super().__init__()
-        self.weight = nn.Parameter(torch.ones(num_channels))
-        self.bias = nn.Parameter(torch.zeros(num_channels))
-        self.eps = eps
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        u = x.mean(1, keepdim=True)
-        s = (x - u).pow(2).mean(1, keepdim=True)
-        x = (x - u) / torch.sqrt(s + self.eps)
-        x = self.weight[:, None, None] * x + self.bias[:, None, None]
-        return x
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/models/backbones/sam_modeling/image_encoder.py b/projects/mmdet3d_plugin/models/backbones/sam_modeling/image_encoder.py
deleted file mode 100644
index f78fdc2..0000000
--- a/projects/mmdet3d_plugin/models/backbones/sam_modeling/image_encoder.py
+++ /dev/null
@@ -1,436 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-
-# This source code is licensed under the license found in the
-# LICENSE file in the root directory of this source tree.
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from functools import partial
-from mmdet.models.builder import BACKBONES
-from mmcv.runner import BaseModule
-
-from typing import Optional, Tuple, Type, Union
-
-from .common import LayerNorm2d, MLPBlock
-
-
-# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa
-@BACKBONES.register_module()
-class ImageEncoderViT(BaseModule):
-    def __init__(
-        self,
-        img_size: Union[int, Tuple[int]] = 1024,
-        patch_size: int = 16,
-        in_chans: int = 3,
-        embed_dim: int = 768,
-        depth: int = 12,
-        num_heads: int = 12,
-        mlp_ratio: float = 4.0,
-        out_chans: int = 256,
-        qkv_bias: bool = True,
-        norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-        act_layer: Type[nn.Module] = nn.GELU,
-        use_abs_pos: bool = True,
-        use_rel_pos: bool = False,
-        rel_pos_zero_init: bool = True,
-        window_size: int = 0,
-        global_attn_indexes: Tuple[int, ...] = (),
-        init_cfg=None,
-    ) -> None:
-        """
-        Args:
-            img_size (int): Input image size, H, W.
-            patch_size (int): Patch size.
-            in_chans (int): Number of input image channels.
-            embed_dim (int): Patch embedding dimension.
-            depth (int): Depth of ViT.
-            num_heads (int): Number of attention heads in each ViT block.
-            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-            qkv_bias (bool): If True, add a learnable bias to query, key, value.
-            norm_layer (nn.Module): Normalization layer.
-            act_layer (nn.Module): Activation layer.
-            use_abs_pos (bool): If True, use absolute positional embeddings.
-            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.
-            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.
-            window_size (int): Window size for window attention blocks.
-            global_attn_indexes (list): Indexes for blocks using global attention.
-        """
-        super().__init__(init_cfg=init_cfg)
-        if isinstance(img_size, int):
-            self.img_size = (img_size, img_size)
-        else:
-            self.img_size = img_size
-
-        self.patch_embed = PatchEmbed(
-            kernel_size=(patch_size, patch_size),
-            stride=(patch_size, patch_size),
-            in_chans=in_chans,
-            embed_dim=embed_dim,
-        )
-
-        self.pos_embed: Optional[nn.Parameter] = None
-        if use_abs_pos:
-            # Initialize absolute positional embedding with pretrain image size.
-            self.pos_embed = nn.Parameter(
-                torch.zeros(1, self.img_size[0] // patch_size, self.img_size[1] // patch_size, embed_dim)
-            )
-
-        self.blocks = nn.ModuleList()
-        for i in range(depth):
-            block = Block(
-                dim=embed_dim,
-                num_heads=num_heads,
-                mlp_ratio=mlp_ratio,
-                qkv_bias=qkv_bias,
-                norm_layer=norm_layer,
-                act_layer=act_layer,
-                use_rel_pos=use_rel_pos,
-                rel_pos_zero_init=rel_pos_zero_init,
-                window_size=window_size if i not in global_attn_indexes else 0,
-                input_size=(self.img_size[0] // patch_size, self.img_size[1] // patch_size),
-            )
-            self.blocks.append(block)
-
-        self.neck = nn.Sequential(
-            nn.Conv2d(
-                embed_dim,
-                out_chans,
-                kernel_size=1,
-                bias=False,
-            ),
-            LayerNorm2d(out_chans),
-            nn.Conv2d(
-                out_chans,
-                out_chans,
-                kernel_size=3,
-                padding=1,
-                bias=False,
-            ),
-            LayerNorm2d(out_chans),
-        )
-
-        self.fpn1 = nn.Sequential(
-            nn.ConvTranspose2d(out_chans, out_chans, kernel_size=2, stride=2),
-        )
-
-        self.fpn2 = nn.Identity()
-
-        self.fpn3 = nn.MaxPool2d(kernel_size=2, stride=2)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        x = self.patch_embed(x)
-        if self.pos_embed is not None:
-            x = x + self.pos_embed
-
-        for blk in self.blocks:
-            x = blk(x)
-
-        x = self.neck(x.permute(0, 3, 1, 2))
-
-        features = [self.fpn1(x), self.fpn2(x), self.fpn3(x)]
-
-        return features
-
-
-class Block(nn.Module):
-    """Transformer blocks with support of window attention and residual propagation blocks"""
-
-    def __init__(
-        self,
-        dim: int,
-        num_heads: int,
-        mlp_ratio: float = 4.0,
-        qkv_bias: bool = True,
-        norm_layer: Type[nn.Module] = nn.LayerNorm,
-        act_layer: Type[nn.Module] = nn.GELU,
-        use_rel_pos: bool = False,
-        rel_pos_zero_init: bool = True,
-        window_size: int = 0,
-        input_size: Optional[Tuple[int, int]] = None,
-    ) -> None:
-        """
-        Args:
-            dim (int): Number of input channels.
-            num_heads (int): Number of attention heads in each ViT block.
-            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
-            qkv_bias (bool): If True, add a learnable bias to query, key, value.
-            norm_layer (nn.Module): Normalization layer.
-            act_layer (nn.Module): Activation layer.
-            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.
-            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.
-            window_size (int): Window size for window attention blocks. If it equals 0, then
-                use global attention.
-            input_size (tuple(int, int) or None): Input resolution for calculating the relative
-                positional parameter size.
-        """
-        super().__init__()
-        self.norm1 = norm_layer(dim)
-        self.attn = Attention(
-            dim,
-            num_heads=num_heads,
-            qkv_bias=qkv_bias,
-            use_rel_pos=use_rel_pos,
-            rel_pos_zero_init=rel_pos_zero_init,
-            input_size=input_size if window_size == 0 else (window_size, window_size),
-        )
-
-        self.norm2 = norm_layer(dim)
-        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)
-
-        self.window_size = window_size
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        shortcut = x
-        x = self.norm1(x)
-        # Window partition
-        if self.window_size > 0:
-            H, W = x.shape[1], x.shape[2]
-            x, pad_hw = window_partition(x, self.window_size)
-
-        x = self.attn(x)
-        # Reverse window partition
-        if self.window_size > 0:
-            x = window_unpartition(x, self.window_size, pad_hw, (H, W))
-
-        x = shortcut + x
-        x = x + self.mlp(self.norm2(x))
-
-        return x
-
-
-class Attention(nn.Module):
-    """Multi-head Attention block with relative position embeddings."""
-
-    def __init__(
-        self,
-        dim: int,
-        num_heads: int = 8,
-        qkv_bias: bool = True,
-        use_rel_pos: bool = False,
-        rel_pos_zero_init: bool = True,
-        input_size: Optional[Tuple[int, int]] = None,
-    ) -> None:
-        """
-        Args:
-            dim (int): Number of input channels.
-            num_heads (int): Number of attention heads.
-            qkv_bias (bool):  If True, add a learnable bias to query, key, value.
-            rel_pos (bool): If True, add relative positional embeddings to the attention map.
-            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.
-            input_size (tuple(int, int) or None): Input resolution for calculating the relative
-                positional parameter size.
-        """
-        super().__init__()
-        self.num_heads = num_heads
-        head_dim = dim // num_heads
-        self.scale = head_dim**-0.5
-
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
-        self.proj = nn.Linear(dim, dim)
-
-        self.use_rel_pos = use_rel_pos
-        if self.use_rel_pos:
-            assert (
-                input_size is not None
-            ), "Input size must be provided if using relative positional encoding."
-            # initialize relative positional embeddings
-            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))
-            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        B, H, W, _ = x.shape
-        # qkv with shape (3, B, nHead, H * W, C)
-        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
-        # q, k, v with shape (B * nHead, H * W, C)
-        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)
-
-        attn = (q * self.scale) @ k.transpose(-2, -1)
-
-        if self.use_rel_pos:
-            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))
-
-        attn = attn.softmax(dim=-1)
-        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)
-        x = self.proj(x)
-
-        return x
-
-
-def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:
-    """
-    Partition into non-overlapping windows with padding if needed.
-    Args:
-        x (tensor): input tokens with [B, H, W, C].
-        window_size (int): window size.
-
-    Returns:
-        windows: windows after partition with [B * num_windows, window_size, window_size, C].
-        (Hp, Wp): padded height and width before partition
-    """
-    B, H, W, C = x.shape
-
-    pad_h = (window_size - H % window_size) % window_size
-    pad_w = (window_size - W % window_size) % window_size
-    if pad_h > 0 or pad_w > 0:
-        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))
-    Hp, Wp = H + pad_h, W + pad_w
-
-    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)
-    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
-    return windows, (Hp, Wp)
-
-
-def window_unpartition(
-    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]
-) -> torch.Tensor:
-    """
-    Window unpartition into original sequences and removing padding.
-    Args:
-        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].
-        window_size (int): window size.
-        pad_hw (Tuple): padded height and width (Hp, Wp).
-        hw (Tuple): original height and width (H, W) before padding.
-
-    Returns:
-        x: unpartitioned sequences with [B, H, W, C].
-    """
-    Hp, Wp = pad_hw
-    H, W = hw
-    B = windows.shape[0] // (Hp * Wp // window_size // window_size)
-    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)
-    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)
-
-    if Hp > H or Wp > W:
-        x = x[:, :H, :W, :].contiguous()
-    return x
-
-
-def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:
-    """
-    Get relative positional embeddings according to the relative positions of
-        query and key sizes.
-    Args:
-        q_size (int): size of query q.
-        k_size (int): size of key k.
-        rel_pos (Tensor): relative position embeddings (L, C).
-
-    Returns:
-        Extracted positional embeddings according to relative positions.
-    """
-    max_rel_dist = int(2 * max(q_size, k_size) - 1)
-    # Interpolate rel pos if needed.
-    if rel_pos.shape[0] != max_rel_dist:
-        # Interpolate rel pos.
-        rel_pos_resized = F.interpolate(
-            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),
-            size=max_rel_dist,
-            mode="linear",
-        )
-        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)
-    else:
-        rel_pos_resized = rel_pos
-
-    # Scale the coords with short length if shapes for q and k are different.
-    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)
-    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)
-    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)
-
-    return rel_pos_resized[relative_coords.long()]
-
-
-def add_decomposed_rel_pos(
-    attn: torch.Tensor,
-    q: torch.Tensor,
-    rel_pos_h: torch.Tensor,
-    rel_pos_w: torch.Tensor,
-    q_size: Tuple[int, int],
-    k_size: Tuple[int, int],
-) -> torch.Tensor:
-    """
-    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.
-    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950
-    Args:
-        attn (Tensor): attention map.
-        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).
-        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.
-        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.
-        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).
-        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).
-
-    Returns:
-        attn (Tensor): attention map with added relative positional embeddings.
-    """
-    q_h, q_w = q_size
-    k_h, k_w = k_size
-    Rh = get_rel_pos(q_h, k_h, rel_pos_h)
-    Rw = get_rel_pos(q_w, k_w, rel_pos_w)
-
-    B, _, dim = q.shape
-    r_q = q.reshape(B, q_h, q_w, dim)
-    rel_h = torch.einsum("bhwc,hkc->bhwk", r_q, Rh)
-    rel_w = torch.einsum("bhwc,wkc->bhwk", r_q, Rw)
-
-    attn = (
-        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]
-    ).view(B, q_h * q_w, k_h * k_w)
-
-    return attn
-
-
-class PatchEmbed(nn.Module):
-    """
-    Image to Patch Embedding.
-    """
-
-    def __init__(
-        self,
-        kernel_size: Tuple[int, int] = (16, 16),
-        stride: Tuple[int, int] = (16, 16),
-        padding: Tuple[int, int] = (0, 0),
-        in_chans: int = 3,
-        embed_dim: int = 768,
-    ) -> None:
-        """
-        Args:
-            kernel_size (Tuple): kernel size of the projection layer.
-            stride (Tuple): stride of the projection layer.
-            padding (Tuple): padding size of the projection layer.
-            in_chans (int): Number of input image channels.
-            embed_dim (int): Patch embedding dimension.
-        """
-        super().__init__()
-
-        self.proj = nn.Conv2d(
-            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding
-        )
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        x = self.proj(x)
-        # B C H W -> B H W C
-        x = x.permute(0, 2, 3, 1)
-        return x
-
-if __name__ == "__main__":
-    from functools import partial
-    model = ImageEncoderViT(
-        depth=32,
-        embed_dim=1280,
-        img_size=(480, 800),
-        mlp_ratio=4,
-        norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),
-        num_heads=16,
-        patch_size=16,
-        qkv_bias=True,
-        use_rel_pos=True,
-        global_attn_indexes=[7, 15, 23, 31],
-        window_size=14,
-        out_chans=256,
-        init_cfg=dict(type='Pretrained', checkpoint="./ckpts/sam_vit_H.pth")
-        )
-    model.init_weights()
-
-    images = torch.randn(1, 3, 480, 800)
-    features = model(images)
-    print(features)
diff --git a/tools/analysis_tools/__init__.py b/tools/analysis_tools/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/tools/analysis_tools/analyze_logs.py b/tools/analysis_tools/analyze_logs.py
deleted file mode 100755
index 806175f..0000000
--- a/tools/analysis_tools/analyze_logs.py
+++ /dev/null
@@ -1,201 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import json
-import numpy as np
-import seaborn as sns
-from collections import defaultdict
-from matplotlib import pyplot as plt
-
-
-def cal_train_time(log_dicts, args):
-    for i, log_dict in enumerate(log_dicts):
-        print(f'{"-" * 5}Analyze train time of {args.json_logs[i]}{"-" * 5}')
-        all_times = []
-        for epoch in log_dict.keys():
-            if args.include_outliers:
-                all_times.append(log_dict[epoch]['time'])
-            else:
-                all_times.append(log_dict[epoch]['time'][1:])
-        all_times = np.array(all_times)
-        epoch_ave_time = all_times.mean(-1)
-        slowest_epoch = epoch_ave_time.argmax()
-        fastest_epoch = epoch_ave_time.argmin()
-        std_over_epoch = epoch_ave_time.std()
-        print(f'slowest epoch {slowest_epoch + 1}, '
-              f'average time is {epoch_ave_time[slowest_epoch]:.4f}')
-        print(f'fastest epoch {fastest_epoch + 1}, '
-              f'average time is {epoch_ave_time[fastest_epoch]:.4f}')
-        print(f'time std over epochs is {std_over_epoch:.4f}')
-        print(f'average iter time: {np.mean(all_times):.4f} s/iter')
-        print()
-
-
-def plot_curve(log_dicts, args):
-    if args.backend is not None:
-        plt.switch_backend(args.backend)
-    sns.set_style(args.style)
-    # if legend is None, use {filename}_{key} as legend
-    legend = args.legend
-    if legend is None:
-        legend = []
-        for json_log in args.json_logs:
-            for metric in args.keys:
-                legend.append(f'{json_log}_{metric}')
-    assert len(legend) == (len(args.json_logs) * len(args.keys))
-    metrics = args.keys
-
-    num_metrics = len(metrics)
-    for i, log_dict in enumerate(log_dicts):
-        epochs = list(log_dict.keys())
-        for j, metric in enumerate(metrics):
-            print(f'plot curve of {args.json_logs[i]}, metric is {metric}')
-            if metric not in log_dict[epochs[args.interval - 1]]:
-                raise KeyError(
-                    f'{args.json_logs[i]} does not contain metric {metric}')
-
-            if args.mode == 'eval':
-                if min(epochs) == args.interval:
-                    x0 = args.interval
-                else:
-                    # if current training is resumed from previous checkpoint
-                    # we lost information in early epochs
-                    # `xs` should start according to `min(epochs)`
-                    if min(epochs) % args.interval == 0:
-                        x0 = min(epochs)
-                    else:
-                        # find the first epoch that do eval
-                        x0 = min(epochs) + args.interval - \
-                            min(epochs) % args.interval
-                xs = np.arange(x0, max(epochs) + 1, args.interval)
-                ys = []
-                for epoch in epochs[args.interval - 1::args.interval]:
-                    ys += log_dict[epoch][metric]
-
-                # if training is aborted before eval of the last epoch
-                # `xs` and `ys` will have different length and cause an error
-                # check if `ys[-1]` is empty here
-                if not log_dict[epoch][metric]:
-                    xs = xs[:-1]
-
-                ax = plt.gca()
-                ax.set_xticks(xs)
-                plt.xlabel('epoch')
-                plt.plot(xs, ys, label=legend[i * num_metrics + j], marker='o')
-            else:
-                xs = []
-                ys = []
-                num_iters_per_epoch = \
-                    log_dict[epochs[args.interval-1]]['iter'][-1]
-                for epoch in epochs[args.interval - 1::args.interval]:
-                    iters = log_dict[epoch]['iter']
-                    if log_dict[epoch]['mode'][-1] == 'val':
-                        iters = iters[:-1]
-                    xs.append(
-                        np.array(iters) + (epoch - 1) * num_iters_per_epoch)
-                    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))
-                xs = np.concatenate(xs)
-                ys = np.concatenate(ys)
-                plt.xlabel('iter')
-                plt.plot(
-                    xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)
-            plt.legend()
-        if args.title is not None:
-            plt.title(args.title)
-    if args.out is None:
-        plt.show()
-    else:
-        print(f'save curve to: {args.out}')
-        plt.savefig(args.out)
-        plt.cla()
-
-
-def add_plot_parser(subparsers):
-    parser_plt = subparsers.add_parser(
-        'plot_curve', help='parser for plotting curves')
-    parser_plt.add_argument(
-        'json_logs',
-        type=str,
-        nargs='+',
-        help='path of train log in json format')
-    parser_plt.add_argument(
-        '--keys',
-        type=str,
-        nargs='+',
-        default=['mAP_0.25'],
-        help='the metric that you want to plot')
-    parser_plt.add_argument('--title', type=str, help='title of figure')
-    parser_plt.add_argument(
-        '--legend',
-        type=str,
-        nargs='+',
-        default=None,
-        help='legend of each plot')
-    parser_plt.add_argument(
-        '--backend', type=str, default=None, help='backend of plt')
-    parser_plt.add_argument(
-        '--style', type=str, default='dark', help='style of plt')
-    parser_plt.add_argument('--out', type=str, default=None)
-    parser_plt.add_argument('--mode', type=str, default='train')
-    parser_plt.add_argument('--interval', type=int, default=1)
-
-
-def add_time_parser(subparsers):
-    parser_time = subparsers.add_parser(
-        'cal_train_time',
-        help='parser for computing the average time per training iteration')
-    parser_time.add_argument(
-        'json_logs',
-        type=str,
-        nargs='+',
-        help='path of train log in json format')
-    parser_time.add_argument(
-        '--include-outliers',
-        action='store_true',
-        help='include the first value of every epoch when computing '
-        'the average time')
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(description='Analyze Json Log')
-    # currently only support plot curve and calculate average train time
-    subparsers = parser.add_subparsers(dest='task', help='task parser')
-    add_plot_parser(subparsers)
-    add_time_parser(subparsers)
-    args = parser.parse_args()
-    return args
-
-
-def load_json_logs(json_logs):
-    # load and convert json_logs to log_dict, key is epoch, value is a sub dict
-    # keys of sub dict is different metrics, e.g. memory, bbox_mAP
-    # value of sub dict is a list of corresponding values of all iterations
-    log_dicts = [dict() for _ in json_logs]
-    for json_log, log_dict in zip(json_logs, log_dicts):
-        with open(json_log, 'r') as log_file:
-            for line in log_file:
-                log = json.loads(line.strip())
-                # skip lines without `epoch` field
-                if 'epoch' not in log:
-                    continue
-                epoch = log.pop('epoch')
-                if epoch not in log_dict:
-                    log_dict[epoch] = defaultdict(list)
-                for k, v in log.items():
-                    log_dict[epoch][k].append(v)
-    return log_dicts
-
-
-def main():
-    args = parse_args()
-
-    json_logs = args.json_logs
-    for json_log in json_logs:
-        assert json_log.endswith('.json')
-
-    log_dicts = load_json_logs(json_logs)
-
-    eval(args.task)(log_dicts, args)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/analysis_tools/benchmark.py b/tools/analysis_tools/benchmark.py
deleted file mode 100755
index 487a348..0000000
--- a/tools/analysis_tools/benchmark.py
+++ /dev/null
@@ -1,98 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import time
-import torch
-from mmcv import Config
-from mmcv.parallel import MMDataParallel
-from mmcv.runner import load_checkpoint, wrap_fp16_model
-import sys
-sys.path.append('.')
-from projects.mmdet3d_plugin.datasets.builder import build_dataloader
-from projects.mmdet3d_plugin.datasets import custom_build_dataset
-# from mmdet3d.datasets import build_dataloader, build_dataset
-from mmdet3d.models import build_detector
-#from tools.misc.fuse_conv_bn import fuse_module
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(description='MMDet benchmark a model')
-    parser.add_argument('config', help='test config file path')
-    parser.add_argument('--checkpoint', default=None, help='checkpoint file')
-    parser.add_argument('--samples', default=2000, help='samples to benchmark')
-    parser.add_argument(
-        '--log-interval', default=50, help='interval of logging')
-    parser.add_argument(
-        '--fuse-conv-bn',
-        action='store_true',
-        help='Whether to fuse conv and bn, this will slightly increase'
-        'the inference speed')
-    args = parser.parse_args()
-    return args
-
-
-def main():
-    args = parse_args()
-
-    cfg = Config.fromfile(args.config)
-    # set cudnn_benchmark
-    if cfg.get('cudnn_benchmark', False):
-        torch.backends.cudnn.benchmark = True
-    cfg.model.pretrained = None
-    cfg.data.test.test_mode = True
-
-    # build the dataloader
-    # TODO: support multiple images per gpu (only minor changes are needed)
-    print(cfg.data.test)
-    dataset = custom_build_dataset(cfg.data.test)
-    data_loader = build_dataloader(
-        dataset,
-        samples_per_gpu=1,
-        workers_per_gpu=cfg.data.workers_per_gpu,
-        dist=False,
-        shuffle=False)
-
-    # build the model and load checkpoint
-    cfg.model.train_cfg = None
-    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))
-    fp16_cfg = cfg.get('fp16', None)
-    if fp16_cfg is not None:
-        wrap_fp16_model(model)
-    if args.checkpoint is not None:
-        load_checkpoint(model, args.checkpoint, map_location='cpu')
-    #if args.fuse_conv_bn:
-    #    model = fuse_module(model)
-
-    model = MMDataParallel(model, device_ids=[0])
-
-    model.eval()
-
-    # the first several iterations may be very slow so skip them
-    num_warmup = 5
-    pure_inf_time = 0
-
-    # benchmark with several samples and take the average
-    for i, data in enumerate(data_loader):
-        torch.cuda.synchronize()
-        start_time = time.perf_counter()
-        with torch.no_grad():
-            model(return_loss=False, rescale=True, **data)
-
-        torch.cuda.synchronize()
-        elapsed = time.perf_counter() - start_time
-
-        if i >= num_warmup:
-            pure_inf_time += elapsed
-            if (i + 1) % args.log_interval == 0:
-                fps = (i + 1 - num_warmup) / pure_inf_time
-                print(f'Done image [{i + 1:<3}/ {args.samples}], '
-                      f'fps: {fps:.1f} img / s')
-
-        if (i + 1) == args.samples:
-            pure_inf_time += elapsed
-            fps = (i + 1 - num_warmup) / pure_inf_time
-            print(f'Overall fps: {fps:.1f} img / s')
-            break
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/analysis_tools/get_params.py b/tools/analysis_tools/get_params.py
deleted file mode 100644
index fb697ad..0000000
--- a/tools/analysis_tools/get_params.py
+++ /dev/null
@@ -1,10 +0,0 @@
-import torch
-file_path = './ckpts/bevformer_v4.pth'
-model = torch.load(file_path, map_location='cpu')
-all = 0
-for key in list(model['state_dict'].keys()):
-    all += model['state_dict'][key].nelement()
-print(all)
-
-# smaller 63374123
-# v4 69140395
diff --git a/tools/analysis_tools/visual.py b/tools/analysis_tools/visual.py
deleted file mode 100644
index f711b75..0000000
--- a/tools/analysis_tools/visual.py
+++ /dev/null
@@ -1,477 +0,0 @@
-# Based on https://github.com/nutonomy/nuscenes-devkit
-# ---------------------------------------------
-#  Modified by Zhiqi Li
-# ---------------------------------------------
-
-import mmcv
-from nuscenes.nuscenes import NuScenes
-from PIL import Image
-from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix
-from typing import Tuple, List, Iterable
-import matplotlib.pyplot as plt
-import numpy as np
-from PIL import Image
-from matplotlib import rcParams
-from matplotlib.axes import Axes
-from pyquaternion import Quaternion
-from PIL import Image
-from matplotlib import rcParams
-from matplotlib.axes import Axes
-from pyquaternion import Quaternion
-from tqdm import tqdm
-from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box
-from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix
-from nuscenes.eval.common.data_classes import EvalBoxes, EvalBox
-from nuscenes.eval.detection.data_classes import DetectionBox
-from nuscenes.eval.detection.utils import category_to_detection_name
-from nuscenes.eval.detection.render import visualize_sample
-
-
-
-
-cams = ['CAM_FRONT',
- 'CAM_FRONT_RIGHT',
- 'CAM_BACK_RIGHT',
- 'CAM_BACK',
- 'CAM_BACK_LEFT',
- 'CAM_FRONT_LEFT']
-
-import numpy as np
-import matplotlib.pyplot as plt
-from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box
-from PIL import Image
-from matplotlib import rcParams
-
-
-def render_annotation(
-        anntoken: str,
-        margin: float = 10,
-        view: np.ndarray = np.eye(4),
-        box_vis_level: BoxVisibility = BoxVisibility.ANY,
-        out_path: str = 'render.png',
-        extra_info: bool = False) -> None:
-    """
-    Render selected annotation.
-    :param anntoken: Sample_annotation token.
-    :param margin: How many meters in each direction to include in LIDAR view.
-    :param view: LIDAR view point.
-    :param box_vis_level: If sample_data is an image, this sets required visibility for boxes.
-    :param out_path: Optional path to save the rendered figure to disk.
-    :param extra_info: Whether to render extra information below camera view.
-    """
-    ann_record = nusc.get('sample_annotation', anntoken)
-    sample_record = nusc.get('sample', ann_record['sample_token'])
-    assert 'LIDAR_TOP' in sample_record['data'].keys(), 'Error: No LIDAR_TOP in data, unable to render.'
-
-    # Figure out which camera the object is fully visible in (this may return nothing).
-    boxes, cam = [], []
-    cams = [key for key in sample_record['data'].keys() if 'CAM' in key]
-    all_bboxes = []
-    select_cams = []
-    for cam in cams:
-        _, boxes, _ = nusc.get_sample_data(sample_record['data'][cam], box_vis_level=box_vis_level,
-                                           selected_anntokens=[anntoken])
-        if len(boxes) > 0:
-            all_bboxes.append(boxes)
-            select_cams.append(cam)
-            # We found an image that matches. Let's abort.
-    # assert len(boxes) > 0, 'Error: Could not find image where annotation is visible. ' \
-    #                      'Try using e.g. BoxVisibility.ANY.'
-    # assert len(boxes) < 2, 'Error: Found multiple annotations. Something is wrong!'
-
-    num_cam = len(all_bboxes)
-
-    fig, axes = plt.subplots(1, num_cam + 1, figsize=(18, 9))
-    select_cams = [sample_record['data'][cam] for cam in select_cams]
-    print('bbox in cams:', select_cams)
-    # Plot LIDAR view.
-    lidar = sample_record['data']['LIDAR_TOP']
-    data_path, boxes, camera_intrinsic = nusc.get_sample_data(lidar, selected_anntokens=[anntoken])
-    LidarPointCloud.from_file(data_path).render_height(axes[0], view=view)
-    for box in boxes:
-        c = np.array(get_color(box.name)) / 255.0
-        box.render(axes[0], view=view, colors=(c, c, c))
-        corners = view_points(boxes[0].corners(), view, False)[:2, :]
-        axes[0].set_xlim([np.min(corners[0, :]) - margin, np.max(corners[0, :]) + margin])
-        axes[0].set_ylim([np.min(corners[1, :]) - margin, np.max(corners[1, :]) + margin])
-        axes[0].axis('off')
-        axes[0].set_aspect('equal')
-
-    # Plot CAMERA view.
-    for i in range(1, num_cam + 1):
-        cam = select_cams[i - 1]
-        data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam, selected_anntokens=[anntoken])
-        im = Image.open(data_path)
-        axes[i].imshow(im)
-        axes[i].set_title(nusc.get('sample_data', cam)['channel'])
-        axes[i].axis('off')
-        axes[i].set_aspect('equal')
-        for box in boxes:
-            c = np.array(get_color(box.name)) / 255.0
-            box.render(axes[i], view=camera_intrinsic, normalize=True, colors=(c, c, c))
-
-        # Print extra information about the annotation below the camera view.
-        axes[i].set_xlim(0, im.size[0])
-        axes[i].set_ylim(im.size[1], 0)
-
-    if extra_info:
-        rcParams['font.family'] = 'monospace'
-
-        w, l, h = ann_record['size']
-        category = ann_record['category_name']
-        lidar_points = ann_record['num_lidar_pts']
-        radar_points = ann_record['num_radar_pts']
-
-        sample_data_record = nusc.get('sample_data', sample_record['data']['LIDAR_TOP'])
-        pose_record = nusc.get('ego_pose', sample_data_record['ego_pose_token'])
-        dist = np.linalg.norm(np.array(pose_record['translation']) - np.array(ann_record['translation']))
-
-        information = ' \n'.join(['category: {}'.format(category),
-                                  '',
-                                  '# lidar points: {0:>4}'.format(lidar_points),
-                                  '# radar points: {0:>4}'.format(radar_points),
-                                  '',
-                                  'distance: {:>7.3f}m'.format(dist),
-                                  '',
-                                  'width:  {:>7.3f}m'.format(w),
-                                  'length: {:>7.3f}m'.format(l),
-                                  'height: {:>7.3f}m'.format(h)])
-
-        plt.annotate(information, (0, 0), (0, -20), xycoords='axes fraction', textcoords='offset points', va='top')
-
-    if out_path is not None:
-        plt.savefig(out_path)
-
-
-
-def get_sample_data(sample_data_token: str,
-                    box_vis_level: BoxVisibility = BoxVisibility.ANY,
-                    selected_anntokens=None,
-                    use_flat_vehicle_coordinates: bool = False):
-    """
-    Returns the data path as well as all annotations related to that sample_data.
-    Note that the boxes are transformed into the current sensor's coordinate frame.
-    :param sample_data_token: Sample_data token.
-    :param box_vis_level: If sample_data is an image, this sets required visibility for boxes.
-    :param selected_anntokens: If provided only return the selected annotation.
-    :param use_flat_vehicle_coordinates: Instead of the current sensor's coordinate frame, use ego frame which is
-                                         aligned to z-plane in the world.
-    :return: (data_path, boxes, camera_intrinsic <np.array: 3, 3>)
-    """
-
-    # Retrieve sensor & pose records
-    sd_record = nusc.get('sample_data', sample_data_token)
-    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])
-    sensor_record = nusc.get('sensor', cs_record['sensor_token'])
-    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])
-
-    data_path = nusc.get_sample_data_path(sample_data_token)
-
-    if sensor_record['modality'] == 'camera':
-        cam_intrinsic = np.array(cs_record['camera_intrinsic'])
-        imsize = (sd_record['width'], sd_record['height'])
-    else:
-        cam_intrinsic = None
-        imsize = None
-
-    # Retrieve all sample annotations and map to sensor coordinate system.
-    if selected_anntokens is not None:
-        boxes = list(map(nusc.get_box, selected_anntokens))
-    else:
-        boxes = nusc.get_boxes(sample_data_token)
-
-    # Make list of Box objects including coord system transforms.
-    box_list = []
-    for box in boxes:
-        if use_flat_vehicle_coordinates:
-            # Move box to ego vehicle coord system parallel to world z plane.
-            yaw = Quaternion(pose_record['rotation']).yaw_pitch_roll[0]
-            box.translate(-np.array(pose_record['translation']))
-            box.rotate(Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)]).inverse)
-        else:
-            # Move box to ego vehicle coord system.
-            box.translate(-np.array(pose_record['translation']))
-            box.rotate(Quaternion(pose_record['rotation']).inverse)
-
-            #  Move box to sensor coord system.
-            box.translate(-np.array(cs_record['translation']))
-            box.rotate(Quaternion(cs_record['rotation']).inverse)
-
-        if sensor_record['modality'] == 'camera' and not \
-                box_in_image(box, cam_intrinsic, imsize, vis_level=box_vis_level):
-            continue
-
-        box_list.append(box)
-
-    return data_path, box_list, cam_intrinsic
-
-
-
-def get_predicted_data(sample_data_token: str,
-                       box_vis_level: BoxVisibility = BoxVisibility.ANY,
-                       selected_anntokens=None,
-                       use_flat_vehicle_coordinates: bool = False,
-                       pred_anns=None
-                       ):
-    """
-    Returns the data path as well as all annotations related to that sample_data.
-    Note that the boxes are transformed into the current sensor's coordinate frame.
-    :param sample_data_token: Sample_data token.
-    :param box_vis_level: If sample_data is an image, this sets required visibility for boxes.
-    :param selected_anntokens: If provided only return the selected annotation.
-    :param use_flat_vehicle_coordinates: Instead of the current sensor's coordinate frame, use ego frame which is
-                                         aligned to z-plane in the world.
-    :return: (data_path, boxes, camera_intrinsic <np.array: 3, 3>)
-    """
-
-    # Retrieve sensor & pose records
-    sd_record = nusc.get('sample_data', sample_data_token)
-    cs_record = nusc.get('calibrated_sensor', sd_record['calibrated_sensor_token'])
-    sensor_record = nusc.get('sensor', cs_record['sensor_token'])
-    pose_record = nusc.get('ego_pose', sd_record['ego_pose_token'])
-
-    data_path = nusc.get_sample_data_path(sample_data_token)
-
-    if sensor_record['modality'] == 'camera':
-        cam_intrinsic = np.array(cs_record['camera_intrinsic'])
-        imsize = (sd_record['width'], sd_record['height'])
-    else:
-        cam_intrinsic = None
-        imsize = None
-
-    # Retrieve all sample annotations and map to sensor coordinate system.
-    # if selected_anntokens is not None:
-    #    boxes = list(map(nusc.get_box, selected_anntokens))
-    # else:
-    #    boxes = nusc.get_boxes(sample_data_token)
-    boxes = pred_anns
-    # Make list of Box objects including coord system transforms.
-    box_list = []
-    for box in boxes:
-        if use_flat_vehicle_coordinates:
-            # Move box to ego vehicle coord system parallel to world z plane.
-            yaw = Quaternion(pose_record['rotation']).yaw_pitch_roll[0]
-            box.translate(-np.array(pose_record['translation']))
-            box.rotate(Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)]).inverse)
-        else:
-            # Move box to ego vehicle coord system.
-            box.translate(-np.array(pose_record['translation']))
-            box.rotate(Quaternion(pose_record['rotation']).inverse)
-
-            #  Move box to sensor coord system.
-            box.translate(-np.array(cs_record['translation']))
-            box.rotate(Quaternion(cs_record['rotation']).inverse)
-
-        if sensor_record['modality'] == 'camera' and not \
-                box_in_image(box, cam_intrinsic, imsize, vis_level=box_vis_level):
-            continue
-        box_list.append(box)
-
-    return data_path, box_list, cam_intrinsic
-
-
-
-
-def lidiar_render(sample_token, data,out_path=None):
-    bbox_gt_list = []
-    bbox_pred_list = []
-    anns = nusc.get('sample', sample_token)['anns']
-    for ann in anns:
-        content = nusc.get('sample_annotation', ann)
-        try:
-            bbox_gt_list.append(DetectionBox(
-                sample_token=content['sample_token'],
-                translation=tuple(content['translation']),
-                size=tuple(content['size']),
-                rotation=tuple(content['rotation']),
-                velocity=nusc.box_velocity(content['token'])[:2],
-                ego_translation=(0.0, 0.0, 0.0) if 'ego_translation' not in content
-                else tuple(content['ego_translation']),
-                num_pts=-1 if 'num_pts' not in content else int(content['num_pts']),
-                detection_name=category_to_detection_name(content['category_name']),
-                detection_score=-1.0 if 'detection_score' not in content else float(content['detection_score']),
-                attribute_name=''))
-        except:
-            pass
-
-    bbox_anns = data['results'][sample_token]
-    for content in bbox_anns:
-        bbox_pred_list.append(DetectionBox(
-            sample_token=content['sample_token'],
-            translation=tuple(content['translation']),
-            size=tuple(content['size']),
-            rotation=tuple(content['rotation']),
-            velocity=tuple(content['velocity']),
-            ego_translation=(0.0, 0.0, 0.0) if 'ego_translation' not in content
-            else tuple(content['ego_translation']),
-            num_pts=-1 if 'num_pts' not in content else int(content['num_pts']),
-            detection_name=content['detection_name'],
-            detection_score=-1.0 if 'detection_score' not in content else float(content['detection_score']),
-            attribute_name=content['attribute_name']))
-    gt_annotations = EvalBoxes()
-    pred_annotations = EvalBoxes()
-    gt_annotations.add_boxes(sample_token, bbox_gt_list)
-    pred_annotations.add_boxes(sample_token, bbox_pred_list)
-    print('green is ground truth')
-    print('blue is the predited result')
-    visualize_sample(nusc, sample_token, gt_annotations, pred_annotations, savepath=out_path+'_bev')
-
-
-def get_color(category_name: str):
-    """
-    Provides the default colors based on the category names.
-    This method works for the general nuScenes categories, as well as the nuScenes detection categories.
-    """
-    a = ['noise', 'animal', 'human.pedestrian.adult', 'human.pedestrian.child', 'human.pedestrian.construction_worker',
-     'human.pedestrian.personal_mobility', 'human.pedestrian.police_officer', 'human.pedestrian.stroller',
-     'human.pedestrian.wheelchair', 'movable_object.barrier', 'movable_object.debris',
-     'movable_object.pushable_pullable', 'movable_object.trafficcone', 'static_object.bicycle_rack', 'vehicle.bicycle',
-     'vehicle.bus.bendy', 'vehicle.bus.rigid', 'vehicle.car', 'vehicle.construction', 'vehicle.emergency.ambulance',
-     'vehicle.emergency.police', 'vehicle.motorcycle', 'vehicle.trailer', 'vehicle.truck', 'flat.driveable_surface',
-     'flat.other', 'flat.sidewalk', 'flat.terrain', 'static.manmade', 'static.other', 'static.vegetation',
-     'vehicle.ego']
-    class_names = [
-        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
-        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
-    ]
-    #print(category_name)
-    if category_name == 'bicycle':
-        return nusc.colormap['vehicle.bicycle']
-    elif category_name == 'construction_vehicle':
-        return nusc.colormap['vehicle.construction']
-    elif category_name == 'traffic_cone':
-        return nusc.colormap['movable_object.trafficcone']
-
-    for key in nusc.colormap.keys():
-        if category_name in key:
-            return nusc.colormap[key]
-    return [0, 0, 0]
-
-
-def render_sample_data(
-        sample_toekn: str,
-        with_anns: bool = True,
-        box_vis_level: BoxVisibility = BoxVisibility.ANY,
-        axes_limit: float = 40,
-        ax=None,
-        nsweeps: int = 1,
-        out_path: str = None,
-        underlay_map: bool = True,
-        use_flat_vehicle_coordinates: bool = True,
-        show_lidarseg: bool = False,
-        show_lidarseg_legend: bool = False,
-        filter_lidarseg_labels=None,
-        lidarseg_preds_bin_path: str = None,
-        verbose: bool = True,
-        show_panoptic: bool = False,
-        pred_data=None,
-      ) -> None:
-    """
-    Render sample data onto axis.
-    :param sample_data_token: Sample_data token.
-    :param with_anns: Whether to draw box annotations.
-    :param box_vis_level: If sample_data is an image, this sets required visibility for boxes.
-    :param axes_limit: Axes limit for lidar and radar (measured in meters).
-    :param ax: Axes onto which to render.
-    :param nsweeps: Number of sweeps for lidar and radar.
-    :param out_path: Optional path to save the rendered figure to disk.
-    :param underlay_map: When set to true, lidar data is plotted onto the map. This can be slow.
-    :param use_flat_vehicle_coordinates: Instead of the current sensor's coordinate frame, use ego frame which is
-        aligned to z-plane in the world. Note: Previously this method did not use flat vehicle coordinates, which
-        can lead to small errors when the vertical axis of the global frame and lidar are not aligned. The new
-        setting is more correct and rotates the plot by ~90 degrees.
-    :param show_lidarseg: When set to True, the lidar data is colored with the segmentation labels. When set
-        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.
-    :param show_lidarseg_legend: Whether to display the legend for the lidarseg labels in the frame.
-    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes. If None
-        or the list is empty, all classes will be displayed.
-    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation
-                                    predictions for the sample.
-    :param verbose: Whether to display the image after it is rendered.
-    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set
-        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.
-        If show_lidarseg is True, show_panoptic will be set to False.
-    """
-    lidiar_render(sample_toekn, pred_data, out_path=out_path)
-    sample = nusc.get('sample', sample_toekn)
-    # sample = data['results'][sample_token_list[0]][0]
-    cams = [
-        'CAM_FRONT_LEFT',
-        'CAM_FRONT',
-        'CAM_FRONT_RIGHT',
-        'CAM_BACK_LEFT',
-        'CAM_BACK',
-        'CAM_BACK_RIGHT',
-    ]
-    if ax is None:
-        _, ax = plt.subplots(4, 3, figsize=(24, 18))
-    j = 0
-    for ind, cam in enumerate(cams):
-        sample_data_token = sample['data'][cam]
-
-        sd_record = nusc.get('sample_data', sample_data_token)
-        sensor_modality = sd_record['sensor_modality']
-
-        if sensor_modality in ['lidar', 'radar']:
-            assert False
-        elif sensor_modality == 'camera':
-            # Load boxes and image.
-            boxes = [Box(record['translation'], record['size'], Quaternion(record['rotation']),
-                         name=record['detection_name'], token='predicted') for record in
-                     pred_data['results'][sample_toekn] if record['detection_score'] > 0.2]
-
-            data_path, boxes_pred, camera_intrinsic = get_predicted_data(sample_data_token,
-                                                                         box_vis_level=box_vis_level, pred_anns=boxes)
-            _, boxes_gt, _ = nusc.get_sample_data(sample_data_token, box_vis_level=box_vis_level)
-            if ind == 3:
-                j += 1
-            ind = ind % 3
-            data = Image.open(data_path)
-            # mmcv.imwrite(np.array(data)[:,:,::-1], f'{cam}.png')
-            # Init axes.
-
-            # Show image.
-            ax[j, ind].imshow(data)
-            ax[j + 2, ind].imshow(data)
-
-            # Show boxes.
-            if with_anns:
-                for box in boxes_pred:
-                    c = np.array(get_color(box.name)) / 255.0
-                    box.render(ax[j, ind], view=camera_intrinsic, normalize=True, colors=(c, c, c))
-                for box in boxes_gt:
-                    c = np.array(get_color(box.name)) / 255.0
-                    box.render(ax[j + 2, ind], view=camera_intrinsic, normalize=True, colors=(c, c, c))
-
-            # Limit visible range.
-            ax[j, ind].set_xlim(0, data.size[0])
-            ax[j, ind].set_ylim(data.size[1], 0)
-            ax[j + 2, ind].set_xlim(0, data.size[0])
-            ax[j + 2, ind].set_ylim(data.size[1], 0)
-
-        else:
-            raise ValueError("Error: Unknown sensor modality!")
-
-        ax[j, ind].axis('off')
-        ax[j, ind].set_title('PRED: {} {labels_type}'.format(
-            sd_record['channel'], labels_type='(predictions)' if lidarseg_preds_bin_path else ''))
-        ax[j, ind].set_aspect('equal')
-
-        ax[j + 2, ind].axis('off')
-        ax[j + 2, ind].set_title('GT:{} {labels_type}'.format(
-            sd_record['channel'], labels_type='(predictions)' if lidarseg_preds_bin_path else ''))
-        ax[j + 2, ind].set_aspect('equal')
-
-    if out_path is not None:
-        plt.savefig(out_path+'_camera', bbox_inches='tight', pad_inches=0, dpi=200)
-    if verbose:
-        plt.show()
-    plt.close()
-
-if __name__ == '__main__':
-    nusc = NuScenes(version='v1.0-trainval', dataroot='./data/nuscenes', verbose=True)
-    # render_annotation('7603b030b42a4b1caa8c443ccc1a7d52')
-    bevformer_results = mmcv.load('test/bevformer_base/Thu_Jun__9_16_22_37_2022/pts_bbox/results_nusc.json')
-    sample_token_list = list(bevformer_results['results'].keys())
-    for id in range(0, 10):
-        render_sample_data(sample_token_list[id], pred_data=bevformer_results, out_path=sample_token_list[id])
diff --git a/tools/data_converter/nuimage_converter.py b/tools/data_converter/nuimage_converter.py
deleted file mode 100755
index 92be1de..0000000
--- a/tools/data_converter/nuimage_converter.py
+++ /dev/null
@@ -1,225 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import base64
-import mmcv
-import numpy as np
-from nuimages import NuImages
-from nuimages.utils.utils import mask_decode, name_to_index_mapping
-from os import path as osp
-
-nus_categories = ('car', 'truck', 'trailer', 'bus', 'construction_vehicle',
-                  'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
-                  'barrier')
-
-NAME_MAPPING = {
-    'movable_object.barrier': 'barrier',
-    'vehicle.bicycle': 'bicycle',
-    'vehicle.bus.bendy': 'bus',
-    'vehicle.bus.rigid': 'bus',
-    'vehicle.car': 'car',
-    'vehicle.construction': 'construction_vehicle',
-    'vehicle.motorcycle': 'motorcycle',
-    'human.pedestrian.adult': 'pedestrian',
-    'human.pedestrian.child': 'pedestrian',
-    'human.pedestrian.construction_worker': 'pedestrian',
-    'human.pedestrian.police_officer': 'pedestrian',
-    'movable_object.trafficcone': 'traffic_cone',
-    'vehicle.trailer': 'trailer',
-    'vehicle.truck': 'truck',
-}
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(description='Data converter arg parser')
-    parser.add_argument(
-        '--data-root',
-        type=str,
-        default='./data/nuimages',
-        help='specify the root path of dataset')
-    parser.add_argument(
-        '--version',
-        type=str,
-        nargs='+',
-        default=['v1.0-mini'],
-        required=False,
-        help='specify the dataset version')
-    parser.add_argument(
-        '--out-dir',
-        type=str,
-        default='./data/nuimages/annotations/',
-        required=False,
-        help='path to save the exported json')
-    parser.add_argument(
-        '--nproc',
-        type=int,
-        default=4,
-        required=False,
-        help='workers to process semantic masks')
-    parser.add_argument('--extra-tag', type=str, default='nuimages')
-    args = parser.parse_args()
-    return args
-
-
-def get_img_annos(nuim, img_info, cat2id, out_dir, data_root, seg_root):
-    """Get semantic segmentation map for an image.
-
-    Args:
-        nuim (obj:`NuImages`): NuImages dataset object
-        img_info (dict): Meta information of img
-
-    Returns:
-        np.ndarray: Semantic segmentation map of the image
-    """
-    sd_token = img_info['token']
-    image_id = img_info['id']
-    name_to_index = name_to_index_mapping(nuim.category)
-
-    # Get image data.
-    width, height = img_info['width'], img_info['height']
-    semseg_mask = np.zeros((height, width)).astype('uint8')
-
-    # Load stuff / surface regions.
-    surface_anns = [
-        o for o in nuim.surface_ann if o['sample_data_token'] == sd_token
-    ]
-
-    # Draw stuff / surface regions.
-    for ann in surface_anns:
-        # Get color and mask.
-        category_token = ann['category_token']
-        category_name = nuim.get('category', category_token)['name']
-        if ann['mask'] is None:
-            continue
-        mask = mask_decode(ann['mask'])
-
-        # Draw mask for semantic segmentation.
-        semseg_mask[mask == 1] = name_to_index[category_name]
-
-    # Load object instances.
-    object_anns = [
-        o for o in nuim.object_ann if o['sample_data_token'] == sd_token
-    ]
-
-    # Sort by token to ensure that objects always appear in the
-    # instance mask in the same order.
-    object_anns = sorted(object_anns, key=lambda k: k['token'])
-
-    # Draw object instances.
-    # The 0 index is reserved for background; thus, the instances
-    # should start from index 1.
-    annotations = []
-    for i, ann in enumerate(object_anns, start=1):
-        # Get color, box, mask and name.
-        category_token = ann['category_token']
-        category_name = nuim.get('category', category_token)['name']
-        if ann['mask'] is None:
-            continue
-        mask = mask_decode(ann['mask'])
-
-        # Draw masks for semantic segmentation and instance segmentation.
-        semseg_mask[mask == 1] = name_to_index[category_name]
-
-        if category_name in NAME_MAPPING:
-            cat_name = NAME_MAPPING[category_name]
-            cat_id = cat2id[cat_name]
-
-            x_min, y_min, x_max, y_max = ann['bbox']
-            # encode calibrated instance mask
-            mask_anno = dict()
-            mask_anno['counts'] = base64.b64decode(
-                ann['mask']['counts']).decode()
-            mask_anno['size'] = ann['mask']['size']
-
-            data_anno = dict(
-                image_id=image_id,
-                category_id=cat_id,
-                bbox=[x_min, y_min, x_max - x_min, y_max - y_min],
-                area=(x_max - x_min) * (y_max - y_min),
-                segmentation=mask_anno,
-                iscrowd=0)
-            annotations.append(data_anno)
-
-    # after process, save semantic masks
-    img_filename = img_info['file_name']
-    seg_filename = img_filename.replace('jpg', 'png')
-    seg_filename = osp.join(seg_root, seg_filename)
-    mmcv.imwrite(semseg_mask, seg_filename)
-    return annotations, np.max(semseg_mask)
-
-
-def export_nuim_to_coco(nuim, data_root, out_dir, extra_tag, version, nproc):
-    print('Process category information')
-    categories = []
-    categories = [
-        dict(id=nus_categories.index(cat_name), name=cat_name)
-        for cat_name in nus_categories
-    ]
-    cat2id = {k_v['name']: k_v['id'] for k_v in categories}
-
-    images = []
-    print('Process image meta information...')
-    for sample_info in mmcv.track_iter_progress(nuim.sample_data):
-        if sample_info['is_key_frame']:
-            img_idx = len(images)
-            images.append(
-                dict(
-                    id=img_idx,
-                    token=sample_info['token'],
-                    file_name=sample_info['filename'],
-                    width=sample_info['width'],
-                    height=sample_info['height']))
-
-    seg_root = f'{out_dir}semantic_masks'
-    mmcv.mkdir_or_exist(seg_root)
-    mmcv.mkdir_or_exist(osp.join(data_root, 'calibrated'))
-
-    global process_img_anno
-
-    def process_img_anno(img_info):
-        single_img_annos, max_cls_id = get_img_annos(nuim, img_info, cat2id,
-                                                     out_dir, data_root,
-                                                     seg_root)
-        return single_img_annos, max_cls_id
-
-    print('Process img annotations...')
-    if nproc > 1:
-        outputs = mmcv.track_parallel_progress(
-            process_img_anno, images, nproc=nproc)
-    else:
-        outputs = []
-        for img_info in mmcv.track_iter_progress(images):
-            outputs.append(process_img_anno(img_info))
-
-    # Determine the index of object annotation
-    print('Process annotation information...')
-    annotations = []
-    max_cls_ids = []
-    for single_img_annos, max_cls_id in outputs:
-        max_cls_ids.append(max_cls_id)
-        for img_anno in single_img_annos:
-            img_anno.update(id=len(annotations))
-            annotations.append(img_anno)
-
-    max_cls_id = max(max_cls_ids)
-    print(f'Max ID of class in the semantic map: {max_cls_id}')
-
-    coco_format_json = dict(
-        images=images, annotations=annotations, categories=categories)
-
-    mmcv.mkdir_or_exist(out_dir)
-    out_file = osp.join(out_dir, f'{extra_tag}_{version}.json')
-    print(f'Annotation dumped to {out_file}')
-    mmcv.dump(coco_format_json, out_file)
-
-
-def main():
-    args = parse_args()
-    for version in args.version:
-        nuim = NuImages(
-            dataroot=args.data_root, version=version, verbose=True, lazy=True)
-        export_nuim_to_coco(nuim, args.data_root, args.out_dir, args.extra_tag,
-                            version, args.nproc)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/dist_test.sh b/tools/dist_test.sh
index 3e2ec30..931aa0f 100755
--- a/tools/dist_test.sh
+++ b/tools/dist_test.sh
@@ -6,5 +6,5 @@ GPUS=$3
 PORT=${PORT:-29503}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch --nproc_per_node=$GPUS --master_port=$PORT \
+torchrun --nproc_per_node=$GPUS --master_port=$PORT \
     $(dirname "$0")/test.py $CONFIG $CHECKPOINT --launcher pytorch ${@:4} --eval bbox
diff --git a/tools/dist_test_seg.sh b/tools/dist_test_seg.sh
index 7719313..0b4f6d2 100755
--- a/tools/dist_test_seg.sh
+++ b/tools/dist_test_seg.sh
@@ -6,5 +6,5 @@ GPUS=$3
 PORT=${PORT:-29503}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch --nproc_per_node=$GPUS --master_port=$PORT \
+torchrun --nproc_per_node=$GPUS --master_port=$PORT \
     $(dirname "$0")/test.py $CONFIG $CHECKPOINT --launcher pytorch ${@:4} --out 'seg_result.pkl'
diff --git a/tools/dist_train.sh b/tools/dist_train.sh
index cd9dd42..35e7be6 100755
--- a/tools/dist_train.sh
+++ b/tools/dist_train.sh
@@ -2,13 +2,14 @@
 #
 CONFIG=$1
 GPUS=$2
+WORK_DIR=$3
 NNODES=${NNODES:-1}
 NODE_RANK=${NODE_RANK:-0}
 PORT=${PORT:-29500}
 MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch \
+torchrun \
     --nnodes=$NNODES \
     --node_rank=$NODE_RANK \
     --master_addr=$MASTER_ADDR \
@@ -17,4 +18,5 @@ python -m torch.distributed.launch \
     $(dirname "$0")/train.py \
     $CONFIG \
     --seed 0 \
-    --launcher pytorch ${@:3} --deterministic 2>&1 | tee output.log
\ No newline at end of file
+    --work-dir ${WORK_DIR} \
+    --launcher pytorch ${@:4} --deterministic
\ No newline at end of file
diff --git a/tools/model_converters/convert_votenet_checkpoints.py b/tools/model_converters/convert_votenet_checkpoints.py
deleted file mode 100755
index 33792b0..0000000
--- a/tools/model_converters/convert_votenet_checkpoints.py
+++ /dev/null
@@ -1,152 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import tempfile
-import torch
-from mmcv import Config
-from mmcv.runner import load_state_dict
-
-from mmdet3d.models import build_detector
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(
-        description='MMDet3D upgrade model version(before v0.6.0) of VoteNet')
-    parser.add_argument('checkpoint', help='checkpoint file')
-    parser.add_argument('--out', help='path of the output checkpoint file')
-    args = parser.parse_args()
-    return args
-
-
-def parse_config(config_strings):
-    """Parse config from strings.
-
-    Args:
-        config_strings (string): strings of model config.
-
-    Returns:
-        Config: model config
-    """
-    temp_file = tempfile.NamedTemporaryFile()
-    config_path = f'{temp_file.name}.py'
-    with open(config_path, 'w') as f:
-        f.write(config_strings)
-
-    config = Config.fromfile(config_path)
-
-    # Update backbone config
-    if 'pool_mod' in config.model.backbone:
-        config.model.backbone.pop('pool_mod')
-
-    if 'sa_cfg' not in config.model.backbone:
-        config.model.backbone['sa_cfg'] = dict(
-            type='PointSAModule',
-            pool_mod='max',
-            use_xyz=True,
-            normalize_xyz=True)
-
-    if 'type' not in config.model.bbox_head.vote_aggregation_cfg:
-        config.model.bbox_head.vote_aggregation_cfg['type'] = 'PointSAModule'
-
-    # Update bbox_head config
-    if 'pred_layer_cfg' not in config.model.bbox_head:
-        config.model.bbox_head['pred_layer_cfg'] = dict(
-            in_channels=128, shared_conv_channels=(128, 128), bias=True)
-
-    if 'feat_channels' in config.model.bbox_head:
-        config.model.bbox_head.pop('feat_channels')
-
-    if 'vote_moudule_cfg' in config.model.bbox_head:
-        config.model.bbox_head['vote_module_cfg'] = config.model.bbox_head.pop(
-            'vote_moudule_cfg')
-
-    if config.model.bbox_head.vote_aggregation_cfg.use_xyz:
-        config.model.bbox_head.vote_aggregation_cfg.mlp_channels[0] -= 3
-
-    temp_file.close()
-
-    return config
-
-
-def main():
-    """Convert keys in checkpoints for VoteNet.
-
-    There can be some breaking changes during the development of mmdetection3d,
-    and this tool is used for upgrading checkpoints trained with old versions
-    (before v0.6.0) to the latest one.
-    """
-    args = parse_args()
-    checkpoint = torch.load(args.checkpoint)
-    cfg = parse_config(checkpoint['meta']['config'])
-    # Build the model and load checkpoint
-    model = build_detector(
-        cfg.model,
-        train_cfg=cfg.get('train_cfg'),
-        test_cfg=cfg.get('test_cfg'))
-    orig_ckpt = checkpoint['state_dict']
-    converted_ckpt = orig_ckpt.copy()
-
-    if cfg['dataset_type'] == 'ScanNetDataset':
-        NUM_CLASSES = 18
-    elif cfg['dataset_type'] == 'SUNRGBDDataset':
-        NUM_CLASSES = 10
-    else:
-        raise NotImplementedError
-
-    RENAME_PREFIX = {
-        'bbox_head.conv_pred.0': 'bbox_head.conv_pred.shared_convs.layer0',
-        'bbox_head.conv_pred.1': 'bbox_head.conv_pred.shared_convs.layer1'
-    }
-
-    DEL_KEYS = [
-        'bbox_head.conv_pred.0.bn.num_batches_tracked',
-        'bbox_head.conv_pred.1.bn.num_batches_tracked'
-    ]
-
-    EXTRACT_KEYS = {
-        'bbox_head.conv_pred.conv_cls.weight':
-        ('bbox_head.conv_pred.conv_out.weight', [(0, 2), (-NUM_CLASSES, -1)]),
-        'bbox_head.conv_pred.conv_cls.bias':
-        ('bbox_head.conv_pred.conv_out.bias', [(0, 2), (-NUM_CLASSES, -1)]),
-        'bbox_head.conv_pred.conv_reg.weight':
-        ('bbox_head.conv_pred.conv_out.weight', [(2, -NUM_CLASSES)]),
-        'bbox_head.conv_pred.conv_reg.bias':
-        ('bbox_head.conv_pred.conv_out.bias', [(2, -NUM_CLASSES)])
-    }
-
-    # Delete some useless keys
-    for key in DEL_KEYS:
-        converted_ckpt.pop(key)
-
-    # Rename keys with specific prefix
-    RENAME_KEYS = dict()
-    for old_key in converted_ckpt.keys():
-        for rename_prefix in RENAME_PREFIX.keys():
-            if rename_prefix in old_key:
-                new_key = old_key.replace(rename_prefix,
-                                          RENAME_PREFIX[rename_prefix])
-                RENAME_KEYS[new_key] = old_key
-    for new_key, old_key in RENAME_KEYS.items():
-        converted_ckpt[new_key] = converted_ckpt.pop(old_key)
-
-    # Extract weights and rename the keys
-    for new_key, (old_key, indices) in EXTRACT_KEYS.items():
-        cur_layers = orig_ckpt[old_key]
-        converted_layers = []
-        for (start, end) in indices:
-            if end != -1:
-                converted_layers.append(cur_layers[start:end])
-            else:
-                converted_layers.append(cur_layers[start:])
-        converted_layers = torch.cat(converted_layers, 0)
-        converted_ckpt[new_key] = converted_layers
-        if old_key in converted_ckpt.keys():
-            converted_ckpt.pop(old_key)
-
-    # Check the converted checkpoint by loading to the model
-    load_state_dict(model, converted_ckpt, strict=True)
-    checkpoint['state_dict'] = converted_ckpt
-    torch.save(checkpoint, args.out)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/model_converters/publish_model.py b/tools/model_converters/publish_model.py
deleted file mode 100755
index 318fd46..0000000
--- a/tools/model_converters/publish_model.py
+++ /dev/null
@@ -1,35 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import subprocess
-import torch
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Process a checkpoint to be published')
-    parser.add_argument('in_file', help='input checkpoint filename')
-    parser.add_argument('out_file', help='output checkpoint filename')
-    args = parser.parse_args()
-    return args
-
-
-def process_checkpoint(in_file, out_file):
-    checkpoint = torch.load(in_file, map_location='cpu')
-    # remove optimizer for smaller file size
-    if 'optimizer' in checkpoint:
-        del checkpoint['optimizer']
-    # if it is necessary to remove some sensitive data in checkpoint['meta'],
-    # add the code here.
-    torch.save(checkpoint, out_file)
-    sha = subprocess.check_output(['sha256sum', out_file]).decode()
-    final_file = out_file.rstrip('.pth') + '-{}.pth'.format(sha[:8])
-    subprocess.Popen(['mv', out_file, final_file])
-
-
-def main():
-    args = parse_args()
-    process_checkpoint(args.in_file, args.out_file)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/model_converters/regnet2mmdet.py b/tools/model_converters/regnet2mmdet.py
deleted file mode 100755
index 9dee3c8..0000000
--- a/tools/model_converters/regnet2mmdet.py
+++ /dev/null
@@ -1,89 +0,0 @@
-# Copyright (c) OpenMMLab. All rights reserved.
-import argparse
-import torch
-from collections import OrderedDict
-
-
-def convert_stem(model_key, model_weight, state_dict, converted_names):
-    new_key = model_key.replace('stem.conv', 'conv1')
-    new_key = new_key.replace('stem.bn', 'bn1')
-    state_dict[new_key] = model_weight
-    converted_names.add(model_key)
-    print(f'Convert {model_key} to {new_key}')
-
-
-def convert_head(model_key, model_weight, state_dict, converted_names):
-    new_key = model_key.replace('head.fc', 'fc')
-    state_dict[new_key] = model_weight
-    converted_names.add(model_key)
-    print(f'Convert {model_key} to {new_key}')
-
-
-def convert_reslayer(model_key, model_weight, state_dict, converted_names):
-    split_keys = model_key.split('.')
-    layer, block, module = split_keys[:3]
-    block_id = int(block[1:])
-    layer_name = f'layer{int(layer[1:])}'
-    block_name = f'{block_id - 1}'
-
-    if block_id == 1 and module == 'bn':
-        new_key = f'{layer_name}.{block_name}.downsample.1.{split_keys[-1]}'
-    elif block_id == 1 and module == 'proj':
-        new_key = f'{layer_name}.{block_name}.downsample.0.{split_keys[-1]}'
-    elif module == 'f':
-        if split_keys[3] == 'a_bn':
-            module_name = 'bn1'
-        elif split_keys[3] == 'b_bn':
-            module_name = 'bn2'
-        elif split_keys[3] == 'c_bn':
-            module_name = 'bn3'
-        elif split_keys[3] == 'a':
-            module_name = 'conv1'
-        elif split_keys[3] == 'b':
-            module_name = 'conv2'
-        elif split_keys[3] == 'c':
-            module_name = 'conv3'
-        new_key = f'{layer_name}.{block_name}.{module_name}.{split_keys[-1]}'
-    else:
-        raise ValueError(f'Unsupported conversion of key {model_key}')
-    print(f'Convert {model_key} to {new_key}')
-    state_dict[new_key] = model_weight
-    converted_names.add(model_key)
-
-
-def convert(src, dst):
-    """Convert keys in pycls pretrained RegNet models to mmdet style."""
-    # load caffe model
-    regnet_model = torch.load(src)
-    blobs = regnet_model['model_state']
-    # convert to pytorch style
-    state_dict = OrderedDict()
-    converted_names = set()
-    for key, weight in blobs.items():
-        if 'stem' in key:
-            convert_stem(key, weight, state_dict, converted_names)
-        elif 'head' in key:
-            convert_head(key, weight, state_dict, converted_names)
-        elif key.startswith('s'):
-            convert_reslayer(key, weight, state_dict, converted_names)
-
-    # check if all layers are converted
-    for key in blobs:
-        if key not in converted_names:
-            print(f'not converted: {key}')
-    # save checkpoint
-    checkpoint = dict()
-    checkpoint['state_dict'] = state_dict
-    torch.save(checkpoint, dst)
-
-
-def main():
-    parser = argparse.ArgumentParser(description='Convert model keys')
-    parser.add_argument('src', help='src detectron model path')
-    parser.add_argument('dst', help='save path')
-    args = parser.parse_args()
-    convert(args.src, args.dst)
-
-
-if __name__ == '__main__':
-    main()
diff --git a/tools/test.py b/tools/test.py
index b7de8a6..ff93666 100755
--- a/tools/test.py
+++ b/tools/test.py
@@ -1,10 +1,15 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 import argparse
 import mmcv
 import os
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import warnings
 from mmcv import Config, DictAction
 from mmcv.cnn import fuse_conv_bn
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
 from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
                          wrap_fp16_model)
@@ -20,6 +25,9 @@ import time
 import os.path as osp
 from tools.eval_metrics.lidar_seg import *
 
+torch.npu.config.allow_internal_format = False
+
+
 def parse_args():
     parser = argparse.ArgumentParser(
         description='MMDet test (and eval) a model')
@@ -225,11 +233,11 @@ def main():
         model.PALETTE = dataset.PALETTE
 
     if not distributed:
-        # assert False
-        model = MMDataParallel(model, device_ids=[0])
-        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)
+        assert False
+        # model = MMDataParallel(model, device_ids=[0])
+        # outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)
     else:
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False)
diff --git a/tools/train.py b/tools/train.py
index 390d37d..af6e918 100755
--- a/tools/train.py
+++ b/tools/train.py
@@ -6,6 +6,8 @@ import mmcv
 import os
 import time
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import warnings
 from mmcv import Config, DictAction
 from mmcv.runner import get_dist_info, init_dist
@@ -22,6 +24,8 @@ from mmseg import __version__ as mmseg_version
 
 from mmcv.utils import TORCH_VERSION, digit_version
 
+torch.npu.config.allow_internal_format = False
+
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Train a detector')
@@ -131,10 +135,6 @@ def main():
     # set cudnn_benchmark
     if cfg.get('cudnn_benchmark', False):
         torch.backends.cudnn.benchmark = True
-    # set tf32
-    if cfg.get('close_tf32', False):
-        torch.backends.cuda.matmul.allow_tf32 = False
-        torch.backends.cudnn.allow_tf32 = False
 
     # work_dir is determined in this priority: CLI > segment in file > filename
     if args.work_dir is not None:
