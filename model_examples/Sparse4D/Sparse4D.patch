diff --git a/local_test.sh b/local_test.sh
index e63bc91..51cc99b 100644
--- a/local_test.sh
+++ b/local_test.sh
@@ -1,12 +1,10 @@
 export PYTHONPATH=$PYTHONPATH:./
-export CUDA_VISIBLE_DEVICES=3
 export PORT=29532
 
-gpus=(${CUDA_VISIBLE_DEVICES//,/ })
-gpu_num=${#gpus[@]}
+gpu_num=$2
 
 config=projects/configs/$1.py
-checkpoint=$2
+checkpoint=$3
 
 echo "number of gpus: "${gpu_num}
 echo "config file: "${config}
@@ -26,4 +24,4 @@ else
         ${checkpoint} \
         --eval bbox \
         $@
-fi
+fi
\ No newline at end of file
diff --git a/projects/configs/sparse4dv3_temporal_r50_1x8_bs6_256x704.py b/projects/configs/sparse4dv3_temporal_r50_1x8_bs6_256x704.py
index fdf075b..95ab3da 100644
--- a/projects/configs/sparse4dv3_temporal_r50_1x8_bs6_256x704.py
+++ b/projects/configs/sparse4dv3_temporal_r50_1x8_bs6_256x704.py
@@ -71,7 +71,7 @@ checkpoint_config = dict(
     interval=num_iters_per_epoch * checkpoint_epoch_interval
 )
 log_config = dict(
-    interval=51,
+    interval=1,
     hooks=[
         dict(type="TextLoggerHook", by_epoch=False),
         dict(type="TensorboardLoggerHook"),
@@ -451,7 +451,7 @@ vis_pipeline = [
     ),
 ]
 evaluation = dict(
-    interval=num_iters_per_epoch * checkpoint_epoch_interval,
+    interval=num_iters_per_epoch * num_epochs + 1,
     pipeline=vis_pipeline,
     # out_dir="./vis",  # for visualization
 )
diff --git a/projects/mmdet3d_plugin/apis/mmdet_train.py b/projects/mmdet3d_plugin/apis/mmdet_train.py
index ad6dc60..2648116 100644
--- a/projects/mmdet3d_plugin/apis/mmdet_train.py
+++ b/projects/mmdet3d_plugin/apis/mmdet_train.py
@@ -85,6 +85,8 @@ def custom_train_detector(
                 type="DistributedSampler"
             ),  # dict(type='DistributedSampler'),
             runner_type=runner_type,
+            persistent_workers=True,
+            pin_memory=True,
         )
         for ds in dataset
     ]
@@ -194,7 +196,7 @@ def custom_train_detector(
             time.ctime().replace(" ", "_").replace(":", "_"),
         )
         eval_hook = CustomDistEvalHook if distributed else EvalHook
-        runner.register_hook(eval_hook(val_dataloader, **eval_cfg))
+        runner.register_hook(eval_hook(val_dataloader, **eval_cfg), priority="LOW")
 
     # user-defined hooks
     if cfg.get("custom_hooks", None):
diff --git a/projects/mmdet3d_plugin/datasets/builder.py b/projects/mmdet3d_plugin/datasets/builder.py
index ab30f9d..3dff066 100644
--- a/projects/mmdet3d_plugin/datasets/builder.py
+++ b/projects/mmdet3d_plugin/datasets/builder.py
@@ -115,7 +115,7 @@ def build_dataloader(
         batch_sampler=batch_sampler,
         num_workers=num_workers,
         collate_fn=partial(collate, samples_per_gpu=samples_per_gpu),
-        pin_memory=False,
+        pin_memory=kwargs.pop("pin_memory", False),
         worker_init_fn=init_fn,
         **kwargs
     )
diff --git a/projects/mmdet3d_plugin/models/blocks.py b/projects/mmdet3d_plugin/models/blocks.py
index 6523bbc..7833204 100644
--- a/projects/mmdet3d_plugin/models/blocks.py
+++ b/projects/mmdet3d_plugin/models/blocks.py
@@ -19,7 +19,7 @@ from mmcv.cnn.bricks.registry import (
 )
 
 try:
-    from ..ops import deformable_aggregation_function as DAF
+    from mx_driving.fused import npu_deformable_aggregation as DAF
 except:
     DAF = None
 
diff --git a/projects/mmdet3d_plugin/ops/__init__.py b/projects/mmdet3d_plugin/ops/__init__.py
index cf23848..65d8407 100644
--- a/projects/mmdet3d_plugin/ops/__init__.py
+++ b/projects/mmdet3d_plugin/ops/__init__.py
@@ -1,6 +1,6 @@
 import torch
 
-from .deformable_aggregation import DeformableAggregationFunction
+from mx_driving.fused import npu_deformable_aggregation
 
 
 def deformable_aggregation_function(
@@ -10,7 +10,8 @@ def deformable_aggregation_function(
     sampling_location,
     weights,
 ):
-    return DeformableAggregationFunction.apply(
+    return npu_deformable_aggregation(
+
         feature_maps,
         spatial_shape,
         scale_start_index,
diff --git a/tools/dist_test.sh b/tools/dist_test.sh
index 033365e..e0329c2 100644
--- a/tools/dist_test.sh
+++ b/tools/dist_test.sh
@@ -6,5 +6,5 @@ GPUS=$3
 PORT=${PORT:-29610}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python3 -m torch.distributed.launch --nproc_per_node=$GPUS --master_port=$PORT \
-    $(dirname "$0")/test.py $CONFIG $CHECKPOINT --launcher pytorch ${@:4}
+torchrun --nproc_per_node=$GPUS --master_port=$PORT \
+    $(dirname "$0")/test.py $CONFIG $CHECKPOINT --launcher pytorch ${@:4}
\ No newline at end of file
diff --git a/tools/dist_train.sh b/tools/dist_train.sh
index 565905e..2d4a5e1 100644
--- a/tools/dist_train.sh
+++ b/tools/dist_train.sh
@@ -5,5 +5,5 @@ GPUS=$2
 PORT=${PORT:-28650}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python3 -m torch.distributed.launch --nproc_per_node=$GPUS --master_port=$PORT \
-    $(dirname "$0")/train.py $CONFIG --launcher pytorch ${@:3}
+torchrun --nproc_per_node=$GPUS --master_port=$PORT \
+    $(dirname "$0")/train.py $CONFIG --launcher pytorch ${@:3}
\ No newline at end of file
diff --git a/tools/test.py b/tools/test.py
index 963e668..d304d4f 100644
--- a/tools/test.py
+++ b/tools/test.py
@@ -3,6 +3,8 @@ import argparse
 import mmcv
 import os
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import warnings
 from mmcv import Config, DictAction
 from mmcv.cnn import fuse_conv_bn
@@ -310,4 +312,8 @@ if __name__ == "__main__":
     torch.multiprocessing.set_start_method(
         "fork"
     )  # use fork workers_per_gpu can be > 1
-    main()
+    from patch import generate_patcher_builder
+
+    sparse4d_patcher_builder = generate_patcher_builder()
+    with sparse4d_patcher_builder.build():
+        main()
diff --git a/tools/train.py b/tools/train.py
index bca3f6b..ee86a2f 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -26,8 +26,13 @@ from datetime import timedelta
 
 import cv2
 
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
 cv2.setNumThreads(8)
 
+torch.npu.config.allow_internal_format = True
+torch.npu.set_compile_mode(jit_compile=False)
 
 def parse_args():
     parser = argparse.ArgumentParser(description="Train a detector")
@@ -313,6 +318,11 @@ def main():
 
 if __name__ == "__main__":
     torch.multiprocessing.set_start_method(
-        "fork"
+        "fork",
+        force=True
     )  # use fork workers_per_gpu can be > 1
-    main()
+    from patch import generate_patcher_builder
+
+    sparse4d_patcher_builder = generate_patcher_builder()
+    with sparse4d_patcher_builder.build():
+        main()
