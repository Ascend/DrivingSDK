diff --git a/projects/bevformer/modules/decoder.py b/projects/bevformer/modules/decoder.py
index 33024f8..cb61b12 100644
--- a/projects/bevformer/modules/decoder.py
+++ b/projects/bevformer/modules/decoder.py
@@ -26,7 +26,7 @@ from mmcv.utils import (ConfigDict, build_from_cfg, deprecated_api_warning,
 from mmcv.utils import ext_loader
 from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32, \
     MultiScaleDeformableAttnFunction_fp16
-
+import mx_driving
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
 
@@ -290,7 +290,7 @@ class CustomMSDeformableAttention(BaseModule):
 
         bs, num_query, _ = query.shape
         bs, num_value, _ = value.shape
-        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        # assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
 
         value = self.value_proj(value)
         if key_padding_mask is not None:
@@ -323,15 +323,8 @@ class CustomMSDeformableAttention(BaseModule):
                 f'Last dim of reference_points must be'
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = mx_driving.multi_scale_deformable_attn(value, spatial_shapes, level_start_index,
+                                                            sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
diff --git a/projects/bevformer/modules/spatial_cross_attention.py b/projects/bevformer/modules/spatial_cross_attention.py
index 77dfa91..b8a0520 100644
--- a/projects/bevformer/modules/spatial_cross_attention.py
+++ b/projects/bevformer/modules/spatial_cross_attention.py
@@ -25,7 +25,13 @@ from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFuncti
     MultiScaleDeformableAttnFunction_fp16
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+import mx_driving
 
+bev_mask_global = torch.tensor([]).npu()
+indexes_global = None
+max_len_global = None
+bev_mask_id_global = -1
+count_global = None
 
 @ATTENTION.register_module()
 class SpatialCrossAttention(BaseModule):
@@ -134,10 +140,28 @@ class SpatialCrossAttention(BaseModule):
 
         D = reference_points_cam.size(3)
         indexes = []
-        for i, mask_per_img in enumerate(bev_mask):
-            index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
-            indexes.append(index_query_per_img)
-        max_len = max([len(each) for each in indexes])
+        global bev_mask_global, indexes_global, max_len_global, bev_mask_id_global, count_global
+        bev_mask_id = id(bev_mask)
+        if bev_mask_id == bev_mask_id_global:
+            indexes = indexes_global
+            max_len = max_len_global
+            count = count_global
+        else:
+            count = torch.any(bev_mask, 3)
+            bev_mask_ = count.squeeze()
+            for i, mask_per_img in enumerate(bev_mask_):
+                index_query_per_img = mask_per_img.nonzero().squeeze(-1)
+                indexes.append(index_query_per_img)
+
+            max_len = max([len(each) for each in indexes])
+            count = count.permute(1, 2, 0).sum(-1)
+            count = torch.clamp(count, min=1.0)
+            count = count[..., None]
+            count_global = count
+            bev_mask_global = bev_mask.clone()
+            indexes_global = indexes
+            max_len_global = max_len
+            bev_mask_id_global = bev_mask_id
 
         # each camera only interacts with its corresponding BEV queries. This step can  greatly save GPU memory.
         queries_rebatch = query.new_zeros(
@@ -145,9 +169,9 @@ class SpatialCrossAttention(BaseModule):
         reference_points_rebatch = reference_points_cam.new_zeros(
             [bs, self.num_cams, max_len, D, 2])
         
-        for j in range(bs):
-            for i, reference_points_per_img in enumerate(reference_points_cam):   
-                index_query_per_img = indexes[i]
+        for i, reference_points_per_img in enumerate(reference_points_cam):   
+            index_query_per_img = indexes[i]
+            for j in range(bs):
                 queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
                 reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
 
@@ -155,20 +179,18 @@ class SpatialCrossAttention(BaseModule):
 
         key = key.permute(2, 0, 1, 3).reshape(
             bs * self.num_cams, l, self.embed_dims)
-        value = value.permute(2, 0, 1, 3).reshape(
+        value = value.permute(2, 0, 1, 3).reshape( 
             bs * self.num_cams, l, self.embed_dims)
 
-        queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
-                                            reference_points=reference_points_rebatch.view(bs*self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
+        queries = self.deformable_attention(query=queries_rebatch.view(bs * self.num_cams, max_len, self.embed_dims), key=key, value=value,
+                                            reference_points=reference_points_rebatch.view(bs * self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
                                             level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims)
         for j in range(bs):
             for i, index_query_per_img in enumerate(indexes):
                 slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
 
-        count = bev_mask.sum(-1) > 0
-        count = count.permute(1, 2, 0).sum(-1)
-        count = torch.clamp(count, min=1.0)
-        slots = slots / count[..., None]
+
+        slots = slots / count
         slots = self.output_proj(slots)
 
         return self.dropout(slots) + inp_residual
@@ -328,7 +350,7 @@ class MSDeformableAttention3D(BaseModule):
 
         bs, num_query, _ = query.shape
         bs, num_value, _ = value.shape
-        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        # assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
 
         value = self.value_proj(value)
         if key_padding_mask is not None:
@@ -382,13 +404,8 @@ class MSDeformableAttention3D(BaseModule):
         #
 
         if torch.cuda.is_available() and value.is_cuda:
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = mx_driving.multi_scale_deformable_attn(value, spatial_shapes, level_start_index,
+                                                            sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
diff --git a/projects/bevformer/modules/temporal_self_attention.py b/projects/bevformer/modules/temporal_self_attention.py
index b52a9aa..437051a 100644
--- a/projects/bevformer/modules/temporal_self_attention.py
+++ b/projects/bevformer/modules/temporal_self_attention.py
@@ -19,6 +19,7 @@ from mmcv.utils import (ConfigDict, build_from_cfg, deprecated_api_warning,
 from mmcv.utils import ext_loader
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+import mx_driving
 
 
 @ATTENTION.register_module()
@@ -190,7 +191,7 @@ class TemporalSelfAttention(BaseModule):
             value = value.permute(1, 0, 2)
         bs,  num_query, embed_dims = query.shape
         _, num_value, _ = value.shape
-        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        # assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
         assert self.num_bev_queue == 2
 
         query = torch.cat([value[:bs], query], -1)
@@ -237,15 +238,8 @@ class TemporalSelfAttention(BaseModule):
                 f'Last dim of reference_points must be'
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = mx_driving.multi_scale_deformable_attn(value, spatial_shapes, level_start_index,
+                                                            sampling_locations, attention_weights)
         else:
 
             output = multi_scale_deformable_attn_pytorch(
diff --git a/projects/configs/lanesegnet_r50_8x1_24e_olv2_subset_A.py b/projects/configs/lanesegnet_r50_8x1_24e_olv2_subset_A.py
index 9900f37..ff6e803 100644
--- a/projects/configs/lanesegnet_r50_8x1_24e_olv2_subset_A.py
+++ b/projects/configs/lanesegnet_r50_8x1_24e_olv2_subset_A.py
@@ -265,7 +265,7 @@ data = dict(
 )
 
 optimizer = dict(
-    type='AdamW',
+    type='NpuFusedAdamW',
     lr=2e-4,
     paramwise_cfg=dict(
         custom_keys={
diff --git a/projects/lanesegnet/models/dense_heads/laneseg_head.py b/projects/lanesegnet/models/dense_heads/laneseg_head.py
index 267e036..910864a 100644
--- a/projects/lanesegnet/models/dense_heads/laneseg_head.py
+++ b/projects/lanesegnet/models/dense_heads/laneseg_head.py
@@ -151,6 +151,9 @@ class LaneSegHead(AnchorFreeHead):
         self.num_lane_type_classes = num_lane_type_classes
         self._init_layers()
 
+        self.pc_range_diff_tensor = torch.tensor(list(map(lambda x, y: x - y, pc_range[self.pts_dim:], pc_range[:self.pts_dim]))).cuda()
+        self.pc_range_head_tensor = torch.tensor(pc_range[:self.pts_dim]).cuda()
+
     def _init_layers(self):
         cls_branch = []
         for _ in range(self.num_reg_fcs):
@@ -229,7 +232,10 @@ class LaneSegHead(AnchorFreeHead):
         bev_feats = bev_feats.view([bev_feats.shape[0], self.bev_h, self.bev_w, self.embed_dims])
         bev_feats = bev_feats.permute(0, 3, 1, 2).contiguous()
         mask_embed = self.mask_embed[lvl](output)
-        outputs_mask = torch.einsum("bqc,bchw->bqhw", mask_embed, bev_feats)
+        bev_feats = bev_feats.reshape(bev_feats.shape[0], bev_feats.shape[1], -1)
+        outputs_mask = torch.bmm(mask_embed, bev_feats)
+        outputs_mask = outputs_mask.reshape(outputs_mask.shape[0], outputs_mask.shape[1], self.bev_h, self.bev_w)
+        # outputs_mask = torch.einsum("bqc,bchw->bqhw", mask_embed, bev_feats)
         return outputs_mask
 
     @auto_fp16(apply_to=('mlvl_feats'))
@@ -284,10 +290,13 @@ class LaneSegHead(AnchorFreeHead):
             tmp = tmp.sigmoid()
 
             coord = tmp.clone()
-            coord[..., 0] = coord[..., 0] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0]
-            coord[..., 1] = coord[..., 1] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1]
+
             if self.pts_dim == 3:
-                coord[..., 2] = coord[..., 2] * (self.pc_range[5] - self.pc_range[2]) + self.pc_range[2]
+                coord = coord * self.pc_range_diff_tensor + self.pc_range_head_tensor
+            else:
+                coord[..., 0] = coord[..., 0] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0]
+                coord[..., 1] = coord[..., 1] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1]
+
             centerline = coord.view(bs, num_query, -1).contiguous()
 
             offset = self.reg_branches_offset[-1](hs[-1])
@@ -338,10 +347,13 @@ class LaneSegHead(AnchorFreeHead):
             tmp = tmp.sigmoid()
 
             coord = tmp.clone()
-            coord[..., 0] = coord[..., 0] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0]
-            coord[..., 1] = coord[..., 1] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1]
+
             if self.pts_dim == 3:
-                coord[..., 2] = coord[..., 2] * (self.pc_range[5] - self.pc_range[2]) + self.pc_range[2]
+                coord = coord * self.pc_range_diff_tensor + self.pc_range_head_tensor
+            else:
+                coord[..., 0] = coord[..., 0] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0]
+                coord[..., 1] = coord[..., 1] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1]
+
             centerline = coord.view(bs, num_query, -1).contiguous()
 
             offset = self.reg_branches_offset[lvl](hs[lvl])
@@ -387,11 +399,13 @@ class LaneSegHead(AnchorFreeHead):
                            gt_lanes_right_type,
                            gt_bboxes_ignore=None):
 
+
         num_bboxes = lanes_pred.size(0)
         # assigner and sampler
         assign_result = self.assigner.assign(lanes_pred, masks_pred, cls_score, gt_lanes, 
                                              gt_instance_masks, gt_labels)
-
+        gt_labels = gt_labels.float()
+        gt_instance_masks = gt_instance_masks.float()
         sampling_result = self.sampler.sample(assign_result, lanes_pred, gt_lanes)
 
         pos_inds = sampling_result.pos_inds
@@ -524,26 +538,28 @@ class LaneSegHead(AnchorFreeHead):
         # Compute the average number of gt boxes accross all gpus, for
         # normalization purposes
         num_total_pos = loss_cls.new_tensor([num_total_pos])
-        num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()
+        # num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1).item()
+        num_total_pos = torch.clamp(reduce_mean(num_total_pos), min=1)
 
         # regression L1 loss
         lanes_preds = lanes_preds.reshape(-1, lanes_preds.size(-1))
 
         isnotnan = torch.isfinite(bbox_targets).all(dim=-1)
         bbox_weights = bbox_weights * self.code_weights
-
-        loss_bbox = self.loss_bbox(
-            lanes_preds[isnotnan, :self.code_size], 
-            bbox_targets[isnotnan, :self.code_size],
-            bbox_weights[isnotnan, :self.code_size],
-            avg_factor=num_total_pos)
+        if self.code_size == lanes_preds.size(1):
+            loss_bbox = self.loss_bbox(
+                lanes_preds[isnotnan], 
+                bbox_targets[isnotnan],
+                bbox_weights[isnotnan],
+                avg_factor=num_total_pos)
+        else:
+            loss_bbox = self.loss_bbox(
+                lanes_preds[isnotnan, :self.code_size], 
+                bbox_targets[isnotnan, :self.code_size],
+                bbox_weights[isnotnan, :self.code_size],
+                avg_factor=num_total_pos)
 
         # segmentation part
-        cls_scores = cls_scores.flatten(0,1)
-        num_total_masks = reduce_mean(cls_scores.new_tensor([num_total_pos]))
-        num_total_masks = max(num_total_masks, 1)
-
-        # extract positive ones, shape (batch_size, num_queries, h, w) -> (num_total_gts, h, w)
         mask_preds = masks_preds[mask_weights > 0]
 
         if mask_targets.shape[0] == 0:
@@ -553,7 +569,7 @@ class LaneSegHead(AnchorFreeHead):
 
         # dice loss
         loss_dice = self.loss_dice(
-            mask_preds, mask_targets, avg_factor=num_total_masks
+            mask_preds, mask_targets, avg_factor=num_total_pos
         )
 
         # mask loss (point based, deprecated)
@@ -569,7 +585,7 @@ class LaneSegHead(AnchorFreeHead):
             mask_targets = mask_targets.reshape(mask_preds.shape).bool()
 
         loss_mask = self.loss_mask(
-            mask_preds, mask_targets, avg_factor=num_total_masks * h * w
+            mask_preds, mask_targets, avg_factor=num_total_pos * h * w
         )
 
         if digit_version(TORCH_VERSION) >= digit_version('1.8'):
diff --git a/projects/lanesegnet/models/modules/lane_attention.py b/projects/lanesegnet/models/modules/lane_attention.py
index 0996561..2f63c3c 100644
--- a/projects/lanesegnet/models/modules/lane_attention.py
+++ b/projects/lanesegnet/models/modules/lane_attention.py
@@ -16,6 +16,7 @@ from mmcv.cnn.bricks.registry import ATTENTION
 from mmcv.runner.base_module import BaseModule
 from mmcv.ops.multi_scale_deform_attn import multi_scale_deformable_attn_pytorch
 from projects.bevformer.modules.multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32
+import mx_driving
 
 
 @ATTENTION.register_module()
@@ -116,7 +117,7 @@ class LaneAttention(BaseModule):
 
         bs, num_query, _ = query.shape
         bs, num_value, _ = value.shape
-        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        # assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
 
         value = self.value_proj(value)
         if key_padding_mask is not None:
@@ -145,15 +146,8 @@ class LaneAttention(BaseModule):
                 f'Last dim of reference_points must be'
                 f' 2, but get {reference_points.shape[-1]} instead.')
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = mx_driving.multi_scale_deformable_attn(value, spatial_shapes, level_start_index,
+                                                            sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
diff --git a/projects/lanesegnet/models/modules/laneseg_decoder.py b/projects/lanesegnet/models/modules/laneseg_decoder.py
index b7c7351..2e11a16 100644
--- a/projects/lanesegnet/models/modules/laneseg_decoder.py
+++ b/projects/lanesegnet/models/modules/laneseg_decoder.py
@@ -10,6 +10,7 @@ from mmcv.cnn.bricks.transformer import TransformerLayerSequence, BaseTransforme
 from mmdet.models.utils.transformer import inverse_sigmoid
 
 
+# TRANSFORMER_LAYER_SEQUENCE > 识别为mmcv的一个模块
 @TRANSFORMER_LAYER_SEQUENCE.register_module()
 class LaneSegNetDecoder(TransformerLayerSequence):
 
@@ -26,6 +27,8 @@ class LaneSegNetDecoder(TransformerLayerSequence):
         self.pc_range = pc_range
         self.sample_idx = sample_idx
         self.pts_dim = pts_dim
+        self.pc_range_diff_tensor = torch.tensor(list(map(lambda x, y: x - y, pc_range[self.pts_dim:], pc_range[:self.pts_dim]))).cuda()
+        self.pc_range_head_tensor = torch.tensor(pc_range[:self.pts_dim]).cuda()
 
     def forward(self,
                 query,
@@ -40,6 +43,7 @@ class LaneSegNetDecoder(TransformerLayerSequence):
         intermediate_reference_points = []
         lane_ref_points = reference_points[:, :, self.sample_idx * 2, :]
         for lid, layer in enumerate(self.layers):
+            # print('layer'*10, layer)
             # BS NUM_QUERY NUM_LEVEL NUM_REFPTS 3
             reference_points_input = lane_ref_points[..., :2].unsqueeze(2)
             output = layer(
@@ -65,10 +69,12 @@ class LaneSegNetDecoder(TransformerLayerSequence):
                 reference_points = tmp.detach()
 
                 coord = tmp.clone()
-                coord[..., 0] = coord[..., 0] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0]
-                coord[..., 1] = coord[..., 1] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1]
+
                 if self.pts_dim == 3:
-                    coord[..., 2] = coord[..., 2] * (self.pc_range[5] - self.pc_range[2]) + self.pc_range[2]
+                    coord = coord * self.pc_range_diff_tensor + self.pc_range_head_tensor
+                else:
+                    coord[..., 0] = coord[..., 0] * (self.pc_range[3] - self.pc_range[0]) + self.pc_range[0]
+                    coord[..., 1] = coord[..., 1] * (self.pc_range[4] - self.pc_range[1]) + self.pc_range[1]
                 centerline = coord.view(bs, num_query, -1).contiguous()
 
                 offset = reg_offset[lid](output)
@@ -79,10 +85,11 @@ class LaneSegNetDecoder(TransformerLayerSequence):
 
                 lane_ref_points = torch.cat([left_laneline, right_laneline], axis=-2).contiguous().detach()
 
-                lane_ref_points[..., 0] = (lane_ref_points[..., 0] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])
-                lane_ref_points[..., 1] = (lane_ref_points[..., 1] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])
                 if self.pts_dim == 3:
-                    lane_ref_points[..., 2] = (lane_ref_points[..., 2] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])
+                    lane_ref_points = (lane_ref_points -  self.pc_range_head_tensor) / self.pc_range_diff_tensor
+                else:
+                    lane_ref_points[..., 0] = (lane_ref_points[..., 0] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])
+                    lane_ref_points[..., 1] = (lane_ref_points[..., 1] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])
 
             output = output.permute(1, 0, 2)
             if self.return_intermediate:
diff --git a/tools/train.py b/tools/train.py
index 18684fb..784b715 100755
--- a/tools/train.py
+++ b/tools/train.py
@@ -13,6 +13,8 @@ from os import path as osp
 
 import mmcv
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import torch.distributed as dist
 from mmcv import Config, DictAction
 from mmcv.runner import get_dist_info, init_dist
